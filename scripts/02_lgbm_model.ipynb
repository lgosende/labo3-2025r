{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd556e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be140ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODELO LGBM PARA PREDICCI√ìN DE TONELADAS ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91a457f723147abab261900c3e15ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos originales: (17021654, 38)\n",
      "Periodos √∫nicos: [201701, 201702, 201703, 201704, 201705, 201706, 201707, 201708, 201709, 201710, 201711, 201712, 201801, 201802, 201803, 201804, 201805, 201806, 201807, 201808, 201809, 201810, 201811, 201812, 201901, 201902, 201903, 201904, 201905, 201906, 201907, 201908, 201909, 201910, 201911, 201912]\n",
      "Limpiando datos num√©ricos...\n",
      "  Convertido plan_precios_cuidados de bool a int\n",
      "  Limpiadas 36 columnas num√©ricas\n",
      "  Convertidos float64 -> float32\n",
      "  Reemplazados valores infinitos por NaN\n",
      "\n",
      "Datos de entrenamiento: (17021654, 39)\n",
      "Datos despu√©s de remover NaN en target: (15628837, 39)\n",
      "Preparando features (versi√≥n CORREGIDA)...\n",
      "‚úÖ VERIFICACI√ìN PASADA\n",
      "Variables categ√≥ricas (3): ['cat1', 'cat2', 'brand']\n",
      "Variables num√©ricas (30): ['is_max_tn_12m', 'tendencia_6m', 'tn_lag_3m', 'tn_mismo_mes_anio_anterior', 'is_min_tn_6m']...\n",
      "Variables excluidas (6): ['product_id', 'customer_id', 'periodo', 'target_tn', 'tn', 'registro_sintetico']\n",
      "Codificando variables categ√≥ricas...\n",
      "  cat1: 4 categor√≠as √∫nicas\n",
      "  cat2: 15 categor√≠as √∫nicas\n",
      "  brand: 36 categor√≠as √∫nicas\n",
      "\n",
      "Features finales: 33\n",
      "Shape final - X: (15628837, 33), y: (15628837,)\n",
      "Train: (10002455, 33), Val: (2500614, 33), Test: (3125768, 33)\n",
      "Estandarizando variables num√©ricas...\n",
      "Estandarizadas 30 variables num√©ricas\n",
      "Estandarizando variables num√©ricas...\n",
      "Estandarizadas 30 variables num√©ricas\n",
      "Entrenando modelo LightGBM...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttrain's rmse: 0.910105\tval's rmse: 1.02815\n",
      "[200]\ttrain's rmse: 0.86374\tval's rmse: 1.01893\n",
      "[300]\ttrain's rmse: 0.831852\tval's rmse: 1.01585\n",
      "[400]\ttrain's rmse: 0.8049\tval's rmse: 1.01595\n",
      "Early stopping, best iteration is:\n",
      "[358]\ttrain's rmse: 0.815788\tval's rmse: 1.01524\n",
      "\n",
      "=== AN√ÅLISIS DEL MODELO ===\n",
      "Top 10 features m√°s importantes:\n",
      "                                 feature    importance\n",
      "21                             ma_tn_12m  1.902882e+07\n",
      "13                     coef_variacion_6m  1.174474e+07\n",
      "29                              ma_tn_6m  8.752325e+06\n",
      "11  desviacion_vs_promedio_historico_mes  7.108065e+06\n",
      "5                       cust_request_qty  6.950550e+06\n",
      "16                           delta_tn_1m  6.726928e+06\n",
      "28                           delta_tn_3m  4.940555e+06\n",
      "32                                 brand  4.389301e+06\n",
      "25                promedio_historico_mes  3.945912e+06\n",
      "18                              ma_tn_3m  3.659212e+06\n",
      "\n",
      "An√°lisis de residuos:\n",
      "  Media de residuos: -0.0006 (debe estar cerca de 0)\n",
      "  Std de residuos: 1.0152\n",
      "  % predicciones negativas: 0.00%\n",
      "  MAE validaci√≥n: 0.0729\n",
      "  R¬≤ validaci√≥n: 0.4430\n",
      "Evaluando modelo...\n",
      "\n",
      "M√©tricas del modelo:\n",
      "MAE: 0.0726\n",
      "RMSE: 0.9897\n",
      "R¬≤: 0.4303\n",
      "\n",
      "Top 15 features m√°s importantes:\n",
      "                                 feature    importance\n",
      "21                             ma_tn_12m  1.902882e+07\n",
      "13                     coef_variacion_6m  1.174474e+07\n",
      "29                              ma_tn_6m  8.752325e+06\n",
      "11  desviacion_vs_promedio_historico_mes  7.108065e+06\n",
      "5                       cust_request_qty  6.950550e+06\n",
      "16                           delta_tn_1m  6.726928e+06\n",
      "28                           delta_tn_3m  4.940555e+06\n",
      "32                                 brand  4.389301e+06\n",
      "25                promedio_historico_mes  3.945912e+06\n",
      "18                              ma_tn_3m  3.659212e+06\n",
      "14                    antiguedad_cliente  3.042886e+06\n",
      "26                   antiguedad_producto  3.014562e+06\n",
      "1                           tendencia_6m  3.008778e+06\n",
      "7                           delta_tn_12m  2.439241e+06\n",
      "17                       cust_request_tn  2.329083e+06\n",
      "\n",
      "Guardando modelo...\n",
      "Modelo guardado como '02_lgbm_model.txt'\n",
      "Objetos auxiliares guardados como 'model_objects.pkl'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuraci√≥n\n",
    "PERIODO_CORTE = 201912  # Entrenar hasta este periodo\n",
    "TARGET_LAG = 2  # Predecir periodo +2\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"\n",
    "    Carga y prepara los datos para el modelo\n",
    "    \"\"\"\n",
    "    # Conectar a la base de datos DuckDB\n",
    "    con = duckdb.connect(database='../input/db/labo3.duckdb')\n",
    "    \n",
    "    # Cargar datos de la tabla ventas_features_final\n",
    "    query = \"\"\"SELECT \n",
    "        antiguedad_cliente,\n",
    "        antiguedad_producto,\n",
    "        cust_request_qty,\n",
    "        cust_request_tn,\n",
    "        customer_id,\n",
    "        periodo,\n",
    "        plan_precios_cuidados,\n",
    "        product_id,\n",
    "        registro_sintetico,\n",
    "        tn,\n",
    "        coef_variacion_6m,\n",
    "        delta_tn_12m,\n",
    "        delta_tn_1m,\n",
    "        delta_tn_3m,\n",
    "        delta_tn_6m,\n",
    "        is_max_tn_12m,\n",
    "        is_max_tn_3m,\n",
    "        is_max_tn_6m,\n",
    "        is_min_tn_12m,\n",
    "        is_min_tn_3m,\n",
    "        is_min_tn_6m,\n",
    "        ma_tn_12m,\n",
    "        ma_tn_3m,\n",
    "        ma_tn_6m,\n",
    "        tendencia_6m,\n",
    "        tn_lag_12m,\n",
    "        tn_lag_1m,\n",
    "        tn_lag_3m,\n",
    "        tn_lag_6m,\n",
    "        tn_mismo_mes_anio_anterior,\n",
    "        trimestre,\n",
    "        desviacion_vs_promedio_historico_mes,\n",
    "        meses_desde_ultima_compra,\n",
    "        promedio_historico_mes,\n",
    "        stddev_historico_mes,\n",
    "        cat1,\n",
    "        cat2,\n",
    "        -- cat3,\n",
    "        brand\n",
    "        -- sku_size \n",
    "    FROM ventas_features_final\"\"\"\n",
    "    df = con.execute(query).df()\n",
    "    \n",
    "    # Cerrar conexi√≥n\n",
    "    con.close()\n",
    "    print(f\"Datos originales: {df.shape}\")\n",
    "    print(f\"Periodos √∫nicos: {sorted(df['periodo'].unique())}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_target_variable_correct(df):\n",
    "    # Ordenar por cliente, producto, periodo\n",
    "    df = df.sort_values(['customer_id', 'product_id', 'periodo'])\n",
    "    \n",
    "    # Target = tn individual en periodo +2\n",
    "    df['target_tn'] = df.groupby(['customer_id', 'product_id'])['tn'].shift(-TARGET_LAG)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_target_variable_v_old(df):\n",
    "    \"\"\"\n",
    "    Crea la variable objetivo (tn en periodo +2) agrupada por product_id\n",
    "    \"\"\"\n",
    "    print(\"Creando variable objetivo...\")\n",
    "    \n",
    "    # Agregar tn por product_id y periodo\n",
    "    df_agg = df.groupby(['product_id', 'periodo']).agg({\n",
    "        'tn': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Ordenar por product_id y periodo\n",
    "    df_agg = df_agg.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    # Crear target variable (tn en periodo +2)\n",
    "    df_agg['target_tn'] = df_agg.groupby('product_id')['tn'].shift(-TARGET_LAG)\n",
    "    \n",
    "    # Merge de vuelta con el dataset original\n",
    "    df = df.merge(df_agg[['product_id', 'periodo', 'target_tn']], \n",
    "                  on=['product_id', 'periodo'], how='left')\n",
    "    \n",
    "    print(f\"Registros con target v√°lido: {df['target_tn'].notna().sum()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_features(df):\n",
    "    \"\"\"\n",
    "    Prepara las features excluyendo variables key_*, fechas y separando categ√≥ricas\n",
    "    \"\"\"\n",
    "    print(\"Preparando features...\")\n",
    "    \n",
    "    # Variables a excluir\n",
    "    exclude_vars = [col for col in df.columns if col.startswith('key_')]\n",
    "    exclude_vars.extend(['target_tn', 'tn'])  # Excluir tambi√©n el target y la variable original\n",
    "    \n",
    "    # Excluir variables de fecha que no son √∫tiles como features num√©ricas\n",
    "    date_vars = ['PVC', 'PVP', 'UVC', 'UVP', 'periodo_fecha']\n",
    "    exclude_vars.extend([col for col in date_vars if col in df.columns])\n",
    "    \n",
    "    # Variables categ√≥ricas\n",
    "    categorical_vars = ['cat1', 'cat2', 'cat3', 'brand']\n",
    "    \n",
    "    # Variables num√©ricas (todas las dem√°s excepto las excluidas)\n",
    "    all_vars = set(df.columns)\n",
    "    numeric_vars = list(all_vars - set(exclude_vars) - set(categorical_vars))\n",
    "    \n",
    "    print(f\"Variables categ√≥ricas ({len(categorical_vars)}): {categorical_vars}\")\n",
    "    print(f\"Variables num√©ricas ({len(numeric_vars)}): {numeric_vars[:10]}...\")  # Mostrar solo las primeras 10\n",
    "    print(f\"Variables excluidas ({len(exclude_vars)}): {exclude_vars}\")\n",
    "    \n",
    "    return numeric_vars, categorical_vars, exclude_vars\n",
    "\n",
    "def prepare_features_fixed(df):\n",
    "    \"\"\"\n",
    "    Funci√≥n CORREGIDA que FUERZA la exclusi√≥n de variables problem√°ticas\n",
    "    \"\"\"\n",
    "    print(\"Preparando features (versi√≥n CORREGIDA)...\")\n",
    "    \n",
    "    # 1. EXCLUSI√ìN FORZADA - ESTAS VARIABLES NO PUEDEN SER FEATURES\n",
    "    forced_excludes = [\n",
    "        'product_id',           # ID del producto - NO es feature\n",
    "        'customer_id',          # ID del cliente - NO es feature  \n",
    "        'periodo',              # Periodo - NO es feature\n",
    "        'target_tn',            # Variable objetivo\n",
    "        'tn',                   # Variable original\n",
    "        'registro_sintetico'    # Variable sint√©tica\n",
    "    ]\n",
    "    \n",
    "    # 2. Variables categ√≥ricas\n",
    "    categorical_vars = ['cat1', 'cat2', 'brand']  # Solo las que existen\n",
    "    \n",
    "    # 3. Calcular variables num√©ricas = TODO lo dem√°s\n",
    "    all_columns = set(df.columns)\n",
    "    excluded_set = set(forced_excludes)\n",
    "    categorical_set = set(categorical_vars)\n",
    "    \n",
    "    numeric_vars = list(all_columns - excluded_set - categorical_set)\n",
    "    \n",
    "    # 4. VERIFICACI√ìN CR√çTICA\n",
    "    contaminated = []\n",
    "    for var in numeric_vars:\n",
    "        if var in ['product_id', 'customer_id', 'periodo']:\n",
    "            contaminated.append(var)\n",
    "    \n",
    "    if contaminated:\n",
    "        print(f\"üö® ERROR CR√çTICO: Variables prohibidas detectadas: {contaminated}\")\n",
    "        print(\"üõë DETENIENDO EJECUCI√ìN\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(f\"‚úÖ VERIFICACI√ìN PASADA\")\n",
    "    print(f\"Variables categ√≥ricas ({len(categorical_vars)}): {categorical_vars}\")\n",
    "    print(f\"Variables num√©ricas ({len(numeric_vars)}): {numeric_vars[:5]}...\")\n",
    "    print(f\"Variables excluidas ({len(forced_excludes)}): {forced_excludes}\")\n",
    "    \n",
    "    return numeric_vars, categorical_vars, forced_excludes\n",
    "\n",
    "\n",
    "def encode_categorical_variables(df, categorical_vars):\n",
    "    \"\"\"\n",
    "    Codifica variables categ√≥ricas usando LabelEncoder\n",
    "    \"\"\"\n",
    "    print(\"Codificando variables categ√≥ricas...\")\n",
    "    \n",
    "    label_encoders = {}\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    for col in categorical_vars:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            # Manejar valores nulos\n",
    "            df_encoded[col] = df_encoded[col].fillna('UNKNOWN')\n",
    "            df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "            print(f\"  {col}: {len(le.classes_)} categor√≠as √∫nicas\")\n",
    "    \n",
    "    return df_encoded, label_encoders\n",
    "\n",
    "def clean_numeric_data(df):\n",
    "    \"\"\"\n",
    "    Limpia y convierte datos num√©ricos para evitar problemas\n",
    "    \"\"\"\n",
    "    print(\"Limpiando datos num√©ricos...\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convertir plan_precios_cuidados de bool a int\n",
    "    if 'plan_precios_cuidados' in df_clean.columns:\n",
    "        df_clean['plan_precios_cuidados'] = df_clean['plan_precios_cuidados'].astype(int)\n",
    "        print(\"  Convertido plan_precios_cuidados de bool a int\")\n",
    "    \n",
    "    # Identificar columnas num√©ricas\n",
    "    numeric_columns = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        # Reemplazar infinitos por NaN\n",
    "        df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Convertir float64 a float32 para ahorrar memoria y evitar problemas de precisi√≥n\n",
    "        if df_clean[col].dtype == 'float64':\n",
    "            df_clean[col] = df_clean[col].astype('float32')\n",
    "    \n",
    "    print(f\"  Limpiadas {len(numeric_columns)} columnas num√©ricas\")\n",
    "    print(\"  Convertidos float64 -> float32\")\n",
    "    print(\"  Reemplazados valores infinitos por NaN\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def standardize_numeric_features(X_train, X_test, numeric_vars):\n",
    "    \"\"\"\n",
    "    Estandariza variables num√©ricas\n",
    "    \"\"\"\n",
    "    print(\"Estandarizando variables num√©ricas...\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Copiar los datasets\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    \n",
    "    # Manejar valores nulos antes de estandarizar\n",
    "    X_train_scaled = X_train_scaled.fillna(0)\n",
    "    X_test_scaled = X_test_scaled.fillna(0)\n",
    "    \n",
    "    # Identificar columnas num√©ricas que existen en los datos\n",
    "    numeric_cols_present = [col for col in numeric_vars if col in X_train.columns]\n",
    "    \n",
    "    # Verificar que las columnas sean realmente num√©ricas y no tengan problemas\n",
    "    numeric_cols_valid = []\n",
    "    for col in numeric_cols_present:\n",
    "        if X_train_scaled[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "            # Verificar que no haya infinitos o valores muy grandes\n",
    "            if not (np.isinf(X_train_scaled[col]).any() or np.isinf(X_test_scaled[col]).any()):\n",
    "                # Verificar que no haya valores extremadamente grandes\n",
    "                max_val = max(X_train_scaled[col].abs().max(), X_test_scaled[col].abs().max())\n",
    "                if max_val < 1e10:  # L√≠mite razonable\n",
    "                    numeric_cols_valid.append(col)\n",
    "                else:\n",
    "                    print(f\"  Saltando {col}: valores muy grandes (max: {max_val:.2e})\")\n",
    "            else:\n",
    "                print(f\"  Saltando {col}: contiene valores infinitos\")\n",
    "        else:\n",
    "            print(f\"  Saltando columna no num√©rica: {col} (tipo: {X_train_scaled[col].dtype})\")\n",
    "    \n",
    "    if numeric_cols_valid:\n",
    "        # Estandarizar solo las columnas num√©ricas v√°lidas\n",
    "        X_train_scaled[numeric_cols_valid] = scaler.fit_transform(X_train_scaled[numeric_cols_valid])\n",
    "        X_test_scaled[numeric_cols_valid] = scaler.transform(X_test_scaled[numeric_cols_valid])\n",
    "        print(f\"Estandarizadas {len(numeric_cols_valid)} variables num√©ricas\")\n",
    "    else:\n",
    "        print(\"No se encontraron variables num√©ricas v√°lidas para estandarizar\")\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "def train_lgbm_model(X_train, y_train, X_val, y_val, categorical_features):\n",
    "    \"\"\"\n",
    "    Entrena modelo LightGBM\n",
    "    \"\"\"\n",
    "    print(\"Entrenando modelo LightGBM...\")\n",
    "    \n",
    "    # Par√°metros del modelo\n",
    "    params = {\n",
    "        #'objective': 'regression',\n",
    "        #'metric': 'rmse',\n",
    "        #'boosting_type': 'gbdt',\n",
    "        #'num_leaves': 50,  # Aumentado\n",
    "        #'learning_rate': 0.03,  # Reducido\n",
    "        #'feature_fraction': 0.8,\n",
    "        #'bagging_fraction': 0.8,\n",
    "        #'bagging_freq': 5,\n",
    "        #'min_data_in_leaf': 100,  # NUEVO - evita overfitting\n",
    "        #'lambda_l1': 0.1,  # NUEVO - regularizaci√≥n\n",
    "        #'lambda_l2': 0.1,  # NUEVO - regularizaci√≥n\n",
    "        #'verbose': -1,\n",
    "        #'random_state': 42,\n",
    "        #'force_col_wise': True\n",
    "\n",
    "        'n_jobs': -1, \n",
    "        'device': 'gpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "\n",
    "        #Mejor corrida\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        'num_leaves': 31, # la corrida en en kaggle mejor tiene 31 leaves\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'random_state': 42,\n",
    "        #'sample_weight': True, \n",
    "        #'linear_tree': True,\n",
    "\n",
    "        #'num_leaves': 63,\n",
    "        #'learning_rate': 0.03738149205005092,\n",
    "        #'feature_fraction': 0.5576004633455672,\n",
    "        #'bagging_fraction': 0.8444560165276432,\n",
    "        #'bagging_freq': 1,\n",
    "        #'min_data_in_leaf': 83,  # NUEVO - evita overfitting\n",
    "        #'lambda_l1': 0.0016447293604905321,\n",
    "        #'lambda_l2': 3.901069568054151e-06,  # NUEVO - regularizaci√≥n\n",
    "        #'min_gain_to_split': 0.8073822295272939,  # NUEVO - evita splits innecesarios\n",
    "        #'max_depth': 11,  # Controla profundidad del √°rbol\n",
    "    }\n",
    "    \n",
    "    # Crear datasets de LightGBM\n",
    "    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=categorical_features, reference=train_data)\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'val'],\n",
    "        num_boost_round=1000,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=100)]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_lgbm_model_improved(X_train, y_train, X_val, y_val, categorical_features):\n",
    "    \"\"\"\n",
    "    Entrena modelo LightGBM con hiperpar√°metros optimizados\n",
    "    REEMPLAZA tu funci√≥n train_lgbm_model() existente con esta\n",
    "    \"\"\"\n",
    "    print(\"Entrenando modelo LightGBM mejorado...\")\n",
    "    \n",
    "    # Par√°metros optimizados para regresi√≥n de ventas\n",
    "    params = {\n",
    "        'device': 'gpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        \n",
    "        # COMPLEJIDAD DEL MODELO\n",
    "        'num_leaves': 80,               # Aumentado de 31 ‚Üí captura m√°s patrones\n",
    "        'max_depth': 7,                 # Controla profundidad del √°rbol\n",
    "        'min_data_in_leaf': 30,         # M√≠nimo de datos por hoja ‚Üí evita overfitting\n",
    "        \n",
    "        # LEARNING RATE\n",
    "        'learning_rate': 0.025,         # Reducido de 0.05 ‚Üí convergencia m√°s lenta pero mejor\n",
    "        \n",
    "        # SAMPLING Y FEATURES\n",
    "        'feature_fraction': 0.75,       # 75% de features por √°rbol ‚Üí m√°s diversidad\n",
    "        'bagging_fraction': 0.75,       # 75% de datos por √°rbol ‚Üí reduce overfitting  \n",
    "        'bagging_freq': 3,              # Cada 3 iteraciones ‚Üí m√°s randomizaci√≥n\n",
    "        \n",
    "        # REGULARIZACI√ìN\n",
    "        'lambda_l1': 0.05,              # Regularizaci√≥n L1 ‚Üí selecci√≥n de features\n",
    "        'lambda_l2': 0.05,              # Regularizaci√≥n L2 ‚Üí suaviza weights\n",
    "        'min_gain_to_split': 0.05,      # Ganancia m√≠nima para split ‚Üí evita ruido\n",
    "        'min_sum_hessian_in_leaf': 20,  # Control adicional de overfitting\n",
    "        \n",
    "        # MANEJO DE CATEGORICAS\n",
    "        'cat_smooth': 10,               # Suavizado para variables categ√≥ricas\n",
    "        'max_cat_threshold': 32,        # M√°ximo threshold para categ√≥ricas\n",
    "        \n",
    "        # PERFORMANCE\n",
    "        'verbose': -1,\n",
    "        'random_state': 42,\n",
    "        'force_col_wise': True,\n",
    "        #'n_jobs': -1,                   # Usar todos los cores\n",
    "        'deterministic': True           # Para reproducibilidad\n",
    "    }\n",
    "    \n",
    "    # Configuraci√≥n de entrenamiento\n",
    "    training_config = {\n",
    "        'num_boost_round': 2500,        # M√°s iteraciones para mejor convergencia\n",
    "        'early_stopping_rounds': 200,   # M√°s paciencia antes de parar\n",
    "        'verbose_eval': 100             # Log cada 100 iteraciones\n",
    "    }\n",
    "    \n",
    "    print(f\"Configuraci√≥n del modelo:\")\n",
    "    print(f\"  Learning rate: {params['learning_rate']}\")\n",
    "    print(f\"  Num leaves: {params['num_leaves']}\")\n",
    "    print(f\"  Max depth: {params['max_depth']}\")\n",
    "    print(f\"  Feature fraction: {params['feature_fraction']}\")\n",
    "    print(f\"  Max iterations: {training_config['num_boost_round']}\")\n",
    "    print(f\"  Early stopping: {training_config['early_stopping_rounds']}\")\n",
    "    \n",
    "    # Crear datasets de LightGBM\n",
    "    train_data = lgb.Dataset(\n",
    "        X_train, \n",
    "        label=y_train, \n",
    "        categorical_feature=categorical_features,\n",
    "        free_raw_data=False  # Mantener datos para debugging\n",
    "    )\n",
    "    \n",
    "    val_data = lgb.Dataset(\n",
    "        X_val, \n",
    "        label=y_val, \n",
    "        categorical_feature=categorical_features, \n",
    "        reference=train_data,\n",
    "        free_raw_data=False\n",
    "    )\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'val'],\n",
    "        num_boost_round=training_config['num_boost_round'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=training_config['early_stopping_rounds']), \n",
    "            lgb.log_evaluation(period=training_config['verbose_eval'])\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Entrenamiento completado:\")\n",
    "    print(f\"  Mejor iteraci√≥n: {model.best_iteration}\")\n",
    "    \n",
    "    # Manejo seguro de best_score (formato puede variar)\n",
    "    try:\n",
    "        train_score = model.best_score['train']['rmse']\n",
    "        val_score = model.best_score['valid_1']['rmse']\n",
    "        print(f\"  Score final train: {train_score:.4f}\")\n",
    "        print(f\"  Score final val: {val_score:.4f}\")\n",
    "    except (KeyError, TypeError):\n",
    "        # Formato alternativo o no disponible\n",
    "        print(f\"  Scores finales disponibles en model.best_score\")\n",
    "        print(f\"  Keys disponibles: {list(model.best_score.keys()) if hasattr(model, 'best_score') else 'No disponible'}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def analyze_model_performance(model, feature_names, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Funci√≥n opcional para analizar el rendimiento del modelo mejorado\n",
    "    \"\"\"\n",
    "    print(\"\\n=== AN√ÅLISIS DEL MODELO ===\")\n",
    "    \n",
    "    # 1. Feature importance\n",
    "    try:\n",
    "        importance = model.feature_importance(importance_type='gain')\n",
    "        feature_imp = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"Top 10 features m√°s importantes:\")\n",
    "        print(feature_imp.head(10)[['feature', 'importance']])\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculando feature importance: {e}\")\n",
    "        feature_imp = None\n",
    "    \n",
    "    # 2. Predicciones de validaci√≥n\n",
    "    try:\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # 3. An√°lisis de residuos\n",
    "        residuals = y_val - y_pred_val\n",
    "        print(f\"\\nAn√°lisis de residuos:\")\n",
    "        print(f\"  Media de residuos: {residuals.mean():.4f} (debe estar cerca de 0)\")\n",
    "        print(f\"  Std de residuos: {residuals.std():.4f}\")\n",
    "        print(f\"  % predicciones negativas: {(y_pred_val < 0).mean()*100:.2f}%\")\n",
    "        \n",
    "        # 4. M√©tricas adicionales\n",
    "        from sklearn.metrics import mean_absolute_error, r2_score\n",
    "        mae = mean_absolute_error(y_val, y_pred_val)\n",
    "        r2 = r2_score(y_val, y_pred_val)\n",
    "        print(f\"  MAE validaci√≥n: {mae:.4f}\")\n",
    "        print(f\"  R¬≤ validaci√≥n: {r2:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en an√°lisis de predicciones: {e}\")\n",
    "    \n",
    "    return feature_imp\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Eval√∫a el modelo\n",
    "    \"\"\"\n",
    "    print(\"Evaluando modelo...\")\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # M√©tricas\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nM√©tricas del modelo:\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R¬≤: {r2:.4f}\")\n",
    "    \n",
    "    return {'mae': mae, 'rmse': rmse, 'r2': r2, 'predictions': y_pred}\n",
    "\n",
    "def get_feature_importance(model, feature_names):\n",
    "    \"\"\"\n",
    "    Obtiene importancia de features\n",
    "    \"\"\"\n",
    "    importance = model.feature_importance(importance_type='gain')\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 features m√°s importantes:\")\n",
    "    print(feature_imp.head(15))\n",
    "    \n",
    "    return feature_imp\n",
    "\n",
    "def verify_model_contamination():\n",
    "    \"\"\"\n",
    "    Verifica si el modelo actual est√° contaminado\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pickle\n",
    "        with open('model_objects.pkl', 'rb') as f:\n",
    "            objects = pickle.load(f)\n",
    "        \n",
    "        feature_cols = objects['feature_cols']\n",
    "        \n",
    "        contaminated = []\n",
    "        for var in ['product_id', 'customer_id', 'periodo']:\n",
    "            if var in feature_cols:\n",
    "                contaminated.append(var)\n",
    "        \n",
    "        if contaminated:\n",
    "            print(f\"üö® MODELO CONTAMINADO: Contiene {contaminated}\")\n",
    "            print(\"üîÑ NECESITAS RE-ENTRENAR con prepare_features_fixed()\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚úÖ Modelo limpio\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo verificar: {e}\")\n",
    "        return True\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Funci√≥n principal\n",
    "    \"\"\"\n",
    "    print(\"=== MODELO LGBM PARA PREDICCI√ìN DE TONELADAS ===\\n\")\n",
    "    \n",
    "    # 0. Verificar si el modelo est√° contaminado\n",
    "    #if verify_model_contamination():\n",
    "    #    print(\"‚ö†Ô∏è El modelo est√° contaminado. Por favor, re-entrena usando prepare_features_fixed().\")\n",
    "    #    return None, None, None\n",
    "\n",
    "    # 1. Cargar datos\n",
    "    df = load_and_prepare_data()\n",
    "    \n",
    "    # 2. Crear variable objetivo\n",
    "    df = create_target_variable_correct(df)\n",
    "    \n",
    "    # 2.5. Limpiar datos num√©ricos (nueva funci√≥n)\n",
    "    df = clean_numeric_data(df)\n",
    "    \n",
    "    # 3. Filtrar datos para entrenamiento (hasta PERIODO_CORTE)\n",
    "    train_data = df[df['periodo'] <= PERIODO_CORTE].copy()\n",
    "    print(f\"\\nDatos de entrenamiento: {train_data.shape}\")\n",
    "    \n",
    "    # 4. Remover registros sin target\n",
    "    train_data = train_data.dropna(subset=['target_tn'])\n",
    "    print(f\"Datos despu√©s de remover NaN en target: {train_data.shape}\")\n",
    "    \n",
    "    # 5. Preparar features\n",
    "    #numeric_vars, categorical_vars, exclude_vars = prepare_features(train_data)\n",
    "    numeric_vars, categorical_vars, exclude_vars = prepare_features_fixed(train_data)\n",
    "\n",
    "    # 6. Codificar variables categ√≥ricas\n",
    "    train_data_encoded, label_encoders = encode_categorical_variables(train_data, categorical_vars)\n",
    "    \n",
    "    # 7. Seleccionar features finales\n",
    "    feature_cols = numeric_vars + categorical_vars\n",
    "    feature_cols = [col for col in feature_cols if col in train_data_encoded.columns]\n",
    "    \n",
    "    print(f\"\\nFeatures finales: {len(feature_cols)}\")\n",
    "    \n",
    "    # 8. Preparar X y y\n",
    "    X = train_data_encoded[feature_cols].copy()\n",
    "    y = train_data_encoded['target_tn'].copy()\n",
    "    \n",
    "    # Manejar valores nulos en X\n",
    "    X = X.fillna(0)\n",
    "    \n",
    "    print(f\"Shape final - X: {X.shape}, y: {y.shape}\")\n",
    "    \n",
    "    # 9. Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=None\n",
    "    )\n",
    "    \n",
    "    # 10. Split train/validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    # 11. Estandarizar variables num√©ricas\n",
    "    X_train_scaled, X_val_scaled, scaler = standardize_numeric_features(X_train, X_val, numeric_vars)\n",
    "    X_test_scaled, _, _ = standardize_numeric_features(X_test, X_test, numeric_vars)\n",
    "    \n",
    "    # 12. Identificar features categ√≥ricas para LightGBM\n",
    "    categorical_indices = [i for i, col in enumerate(feature_cols) if col in categorical_vars]\n",
    "    \n",
    "    # 13. Entrenar modelo\n",
    "    model = train_lgbm_model(\n",
    "        X_train_scaled, y_train, \n",
    "        X_val_scaled, y_val, \n",
    "        categorical_indices\n",
    "    )\n",
    "\n",
    "    feature_analysis = analyze_model_performance(model, feature_cols, X_val_scaled, y_val)\n",
    "    \n",
    "    # 14. Evaluar modelo\n",
    "    results = evaluate_model(model, X_test_scaled, y_test)\n",
    "    \n",
    "    # 15. Feature importance\n",
    "    feature_importance = get_feature_importance(model, feature_cols)\n",
    "    \n",
    "    # 16. Guardar modelo y objetos necesarios\n",
    "    print(\"\\nGuardando modelo...\")\n",
    "    model.save_model('../output/lgbm/02_lgbm_model.txt')\n",
    "    \n",
    "    # Guardar escalador y encoders\n",
    "    import pickle\n",
    "    with open('model_objects.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'scaler': scaler,\n",
    "            'label_encoders': label_encoders,\n",
    "            'feature_cols': feature_cols,\n",
    "            'categorical_vars': categorical_vars,\n",
    "            'numeric_vars': numeric_vars\n",
    "        }, f)\n",
    "    \n",
    "    print(\"Modelo guardado como '02_lgbm_model.txt'\")\n",
    "    print(\"Objetos auxiliares guardados como 'model_objects.pkl'\")\n",
    "    \n",
    "    return model, results, feature_importance\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, results, feature_importance = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0ef9539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREDICCI√ìN PARA PRODUCTOS ESPEC√çFICOS ===\\n\n",
      "Cargando productos a predecir...\n",
      "Productos a predecir: 780\n",
      "Product_ids √∫nicos: 780\n",
      "Cargando modelo y objetos auxiliares...\n",
      "Cargando datos m√°s recientes para predicci√≥n...\n",
      "Datos del periodo 201912: (553419, 98)\n",
      "Preparando datos para predicci√≥n...\n",
      "Productos a predecir: 780\n",
      "Registros filtrados: (465660, 98)\n",
      "Limpiando datos num√©ricos para predicci√≥n...\n",
      "  Limpiadas 85 columnas num√©ricas\n",
      "Generando predicciones individuales...\n",
      "Predicciones individuales generadas: 465660\n",
      "Rango de predicciones: -1.10 - 102.70\n",
      "Promedio predicci√≥n individual: 0.06\n",
      "Agrupando predicciones por product_id...\n",
      "Resultados finales:\n",
      "Productos con predicci√≥n: 780\n",
      "Total tn predichas: 26873.18\n",
      "Promedio tn por producto: 34.45\n",
      "M√°ximo tn por producto: 1292.36\n",
      "Top 10 productos con mayor predicci√≥n:\n",
      "    product_id           tn\n",
      "0        20001  1292.356422\n",
      "2        20003   628.009509\n",
      "3        20004   586.016164\n",
      "1        20002   558.464380\n",
      "4        20005   489.382897\n",
      "8        20009   452.149353\n",
      "31       20032   420.153880\n",
      "6        20007   395.831839\n",
      "10       20011   366.970870\n",
      "13       20014   365.270262\n",
      "Resultado final: 780 productos\n",
      "Predicciones guardadas en: ../output/lgbm/02_lgbm_predicciones.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CHUNK DE PREDICCI√ìN CORREGIDO PARA EL NUEVO ENFOQUE\n",
    "# =============================================================================\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_productos_a_predecir():\n",
    "    \"\"\"\n",
    "    Carga la lista de productos a predecir desde DuckDB\n",
    "    \"\"\"\n",
    "    print(\"Cargando productos a predecir...\")\n",
    "    \n",
    "    # Conectar a la base de datos DuckDB\n",
    "    con = duckdb.connect(database='../input/db/labo3.duckdb')\n",
    "    \n",
    "    # Cargar productos a predecir\n",
    "    query = \"SELECT * FROM tb_productos_a_predecir\"\n",
    "    productos_df = con.execute(query).df()\n",
    "    \n",
    "    con.close()\n",
    "    \n",
    "    print(f\"Productos a predecir: {len(productos_df)}\")\n",
    "    print(f\"Product_ids √∫nicos: {productos_df['product_id'].nunique()}\")\n",
    "    \n",
    "    return productos_df\n",
    "\n",
    "def load_latest_data_for_prediction():\n",
    "    \"\"\"\n",
    "    Carga los datos m√°s recientes para hacer predicciones\n",
    "    \"\"\"\n",
    "    print(\"Cargando datos m√°s recientes para predicci√≥n...\")\n",
    "    \n",
    "    # Conectar a la base de datos DuckDB\n",
    "    con = duckdb.connect(database='../input/db/labo3.duckdb')\n",
    "    \n",
    "    # Cargar datos del periodo 201912 - TODOS los campos como en entrenamiento\n",
    "    query = \"\"\"SELECT \n",
    "        *\n",
    "    FROM ventas_features_final \n",
    "    WHERE periodo = 201912\"\"\"\n",
    "    \n",
    "    df = con.execute(query).df()\n",
    "    con.close()\n",
    "    \n",
    "    print(f\"Datos del periodo 201912: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_model_and_objects():\n",
    "    \"\"\"\n",
    "    Carga el modelo entrenado y objetos auxiliares\n",
    "    \"\"\"\n",
    "    print(\"Cargando modelo y objetos auxiliares...\")\n",
    "    \n",
    "    # Cargar modelo LightGBM\n",
    "    model = lgb.Booster(model_file='../output/lgbm/02_lgbm_model.txt')\n",
    "    \n",
    "    # Cargar objetos auxiliares\n",
    "    with open('model_objects.pkl', 'rb') as f:\n",
    "        objects = pickle.load(f)\n",
    "    \n",
    "    return model, objects\n",
    "\n",
    "def clean_numeric_data_prediction(df):\n",
    "    \"\"\"\n",
    "    Aplica la misma limpieza de datos num√©ricos que en entrenamiento\n",
    "    \"\"\"\n",
    "    print(\"Limpiando datos num√©ricos para predicci√≥n...\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convertir plan_precios_cuidados de bool a int\n",
    "    if 'plan_precios_cuidados' in df_clean.columns:\n",
    "        df_clean['plan_precios_cuidados'] = df_clean['plan_precios_cuidados'].astype(int)\n",
    "    \n",
    "    # Identificar columnas num√©ricas\n",
    "    numeric_columns = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        # Reemplazar infinitos por NaN\n",
    "        df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Convertir float64 a float32\n",
    "        if df_clean[col].dtype == 'float64':\n",
    "            df_clean[col] = df_clean[col].astype('float32')\n",
    "    \n",
    "    print(f\"  Limpiadas {len(numeric_columns)} columnas num√©ricas\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def prepare_prediction_data(df, objects, productos_a_predecir):\n",
    "    \"\"\"\n",
    "    Prepara datos para predicci√≥n - SOLO productos solicitados\n",
    "    \"\"\"\n",
    "    print(\"Preparando datos para predicci√≥n...\")\n",
    "    \n",
    "    # FILTRAR solo productos que necesitamos predecir\n",
    "    product_ids_to_predict = productos_a_predecir['product_id'].unique()\n",
    "    df_filtered = df[df['product_id'].isin(product_ids_to_predict)].copy()\n",
    "    \n",
    "    print(f\"Productos a predecir: {len(product_ids_to_predict)}\")\n",
    "    print(f\"Registros filtrados: {df_filtered.shape}\")\n",
    "    \n",
    "    if df_filtered.empty:\n",
    "        print(\"¬°ADVERTENCIA! No se encontraron datos para los productos a predecir.\")\n",
    "        return None\n",
    "    \n",
    "    # Aplicar limpieza de datos\n",
    "    df_filtered = clean_numeric_data_prediction(df_filtered)\n",
    "    \n",
    "    scaler = objects['scaler']\n",
    "    label_encoders = objects['label_encoders']\n",
    "    feature_cols = objects['feature_cols']\n",
    "    categorical_vars = objects['categorical_vars']\n",
    "    numeric_vars = objects['numeric_vars']\n",
    "    \n",
    "    # Codificar variables categ√≥ricas\n",
    "    for col in categorical_vars:\n",
    "        if col in df_filtered.columns and col in label_encoders:\n",
    "            le = label_encoders[col]\n",
    "            df_filtered[col] = df_filtered[col].fillna('UNKNOWN')\n",
    "            \n",
    "            # Manejar categor√≠as no vistas durante el entrenamiento\n",
    "            unknown_mask = ~df_filtered[col].astype(str).isin(le.classes_)\n",
    "            df_filtered[col] = df_filtered[col].astype(str)\n",
    "            df_filtered.loc[unknown_mask, col] = 'UNKNOWN'\n",
    "            \n",
    "            # Si 'UNKNOWN' no est√° en las clases, usar la primera clase\n",
    "            if 'UNKNOWN' not in le.classes_:\n",
    "                df_filtered.loc[unknown_mask, col] = le.classes_[0]\n",
    "            \n",
    "            df_filtered[col] = le.transform(df_filtered[col])\n",
    "    \n",
    "    # Seleccionar features\n",
    "    missing_features = [col for col in feature_cols if col not in df_filtered.columns]\n",
    "    if missing_features:\n",
    "        print(f\"¬°ADVERTENCIA! Features faltantes: {missing_features}\")\n",
    "        # Crear columnas faltantes con valor 0\n",
    "        for col in missing_features:\n",
    "            df_filtered[col] = 0\n",
    "    \n",
    "    X_pred = df_filtered[feature_cols].copy()\n",
    "    \n",
    "    # Manejar valores nulos\n",
    "    X_pred = X_pred.fillna(0)\n",
    "    \n",
    "    # Estandarizar variables num√©ricas\n",
    "    numeric_cols_present = [col for col in numeric_vars if col in X_pred.columns]\n",
    "    numeric_cols_valid = []\n",
    "    \n",
    "    for col in numeric_cols_present:\n",
    "        if X_pred[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "            if not np.isinf(X_pred[col]).any():\n",
    "                max_val = X_pred[col].abs().max()\n",
    "                if max_val < 1e10:\n",
    "                    numeric_cols_valid.append(col)\n",
    "    \n",
    "    if numeric_cols_valid:\n",
    "        X_pred[numeric_cols_valid] = scaler.transform(X_pred[numeric_cols_valid])\n",
    "    \n",
    "    # Conservar informaci√≥n para el resultado final\n",
    "    result_info = df_filtered[['customer_id', 'product_id']].copy()\n",
    "    \n",
    "    return X_pred, result_info\n",
    "\n",
    "\n",
    "\n",
    "def make_predictions_for_products():\n",
    "    \"\"\"\n",
    "    Funci√≥n principal para hacer predicciones para productos espec√≠ficos\n",
    "    \"\"\"\n",
    "    print(\"=== PREDICCI√ìN PARA PRODUCTOS ESPEC√çFICOS ===\\\\n\")\n",
    "    \n",
    "    # 1. Cargar productos a predecir\n",
    "    productos_a_predecir = load_productos_a_predecir()\n",
    "    \n",
    "    # 2. Cargar modelo y objetos\n",
    "    model, objects = load_model_and_objects()\n",
    "    \n",
    "    # 3. Cargar datos m√°s recientes\n",
    "    df_latest = load_latest_data_for_prediction()\n",
    "    \n",
    "    # 4. Preparar datos para predicci√≥n (solo productos solicitados)\n",
    "    prediction_result = prepare_prediction_data(df_latest, objects, productos_a_predecir)\n",
    "    \n",
    "    if prediction_result is None:\n",
    "        print(\"No se pudieron preparar los datos para predicci√≥n.\")\n",
    "        return None\n",
    "    \n",
    "    X_pred, result_info = prediction_result\n",
    "    \n",
    "    # 5. Hacer predicciones individuales (cliente-producto)\n",
    "    print(\"Generando predicciones individuales...\")\n",
    "    predictions = model.predict(X_pred)\n",
    "    \n",
    "    # 6. Agregar predicciones al resultado\n",
    "    result_info['predicted_tn'] = predictions\n",
    "    \n",
    "    print(f\"Predicciones individuales generadas: {len(predictions)}\")\n",
    "    print(f\"Rango de predicciones: {predictions.min():.2f} - {predictions.max():.2f}\")\n",
    "    print(f\"Promedio predicci√≥n individual: {predictions.mean():.2f}\")\n",
    "    \n",
    "    # 7. Agrupar por product_id y SUMAR las predicciones\n",
    "    print(\"Agrupando predicciones por product_id...\")\n",
    "    final_predictions = result_info.groupby('product_id').agg({\n",
    "        'predicted_tn': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Renombrar columna para claridad\n",
    "    final_predictions = final_predictions.rename(columns={'predicted_tn': 'tn'})\n",
    "    \n",
    "    # Ordenar por tn descendente\n",
    "    final_predictions = final_predictions.sort_values('tn', ascending=False)\n",
    "    \n",
    "    print(f\"Resultados finales:\")\n",
    "    print(f\"Productos con predicci√≥n: {len(final_predictions)}\")\n",
    "    print(f\"Total tn predichas: {final_predictions['tn'].sum():.2f}\")\n",
    "    print(f\"Promedio tn por producto: {final_predictions['tn'].mean():.2f}\")\n",
    "    print(f\"M√°ximo tn por producto: {final_predictions['tn'].max():.2f}\")\n",
    "    \n",
    "    # Mostrar top 10\n",
    "    print(\"Top 10 productos con mayor predicci√≥n:\")\n",
    "    print(final_predictions.head(10))\n",
    "    \n",
    "    # 8. Verificar que tenemos todos los productos solicitados\n",
    "    productos_solicitados = set(productos_a_predecir['product_id'].unique())\n",
    "    productos_predichos = set(final_predictions['product_id'].unique())\n",
    "    productos_faltantes = productos_solicitados - productos_predichos\n",
    "    \n",
    "    if productos_faltantes:\n",
    "        print(f\"¬°ADVERTENCIA! Productos sin datos en 201912: {len(productos_faltantes)}\")\n",
    "        print(f\"Product_ids faltantes: {list(productos_faltantes)[:10]}...\")\n",
    "        \n",
    "        # Agregar productos faltantes con tn = 0\n",
    "        for product_id in productos_faltantes:\n",
    "            final_predictions = pd.concat([\n",
    "                final_predictions,\n",
    "                pd.DataFrame({'product_id': [product_id], 'tn': [0.0]})\n",
    "            ], ignore_index=True)\n",
    "        \n",
    "        print(f\"Productos faltantes agregados con tn=0\")\n",
    "    \n",
    "    print(f\"Resultado final: {len(final_predictions)} productos\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 9. Guardar resultados\n",
    "    output_file = '../output/lgbm/02_lgbm_predicciones.csv'\n",
    "    final_predictions.to_csv(output_file, index=False)\n",
    "    print(f\"Predicciones guardadas en: {output_file}\")\n",
    "    \n",
    "    #guardar el contenido del archivo actual en un txt\n",
    "\n",
    "\n",
    "\n",
    "    return final_predictions\n",
    "    \n",
    "\n",
    "# Ejecutar predicciones\n",
    "if __name__ == \"__main__\":\n",
    "    predicciones = make_predictions_for_products()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311 (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
