{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd556e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be140ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODELO LGBM PARA PREDICCIÓN DE TONELADAS ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91a457f723147abab261900c3e15ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos originales: (17021654, 38)\n",
      "Periodos únicos: [201701, 201702, 201703, 201704, 201705, 201706, 201707, 201708, 201709, 201710, 201711, 201712, 201801, 201802, 201803, 201804, 201805, 201806, 201807, 201808, 201809, 201810, 201811, 201812, 201901, 201902, 201903, 201904, 201905, 201906, 201907, 201908, 201909, 201910, 201911, 201912]\n",
      "Limpiando datos numéricos...\n",
      "  Convertido plan_precios_cuidados de bool a int\n",
      "  Limpiadas 36 columnas numéricas\n",
      "  Convertidos float64 -> float32\n",
      "  Reemplazados valores infinitos por NaN\n",
      "\n",
      "Datos de entrenamiento: (17021654, 39)\n",
      "Datos después de remover NaN en target: (15628837, 39)\n",
      "Preparando features (versión CORREGIDA)...\n",
      "✅ VERIFICACIÓN PASADA\n",
      "Variables categóricas (3): ['cat1', 'cat2', 'brand']\n",
      "Variables numéricas (30): ['is_max_tn_12m', 'tendencia_6m', 'tn_lag_3m', 'tn_mismo_mes_anio_anterior', 'is_min_tn_6m']...\n",
      "Variables excluidas (6): ['product_id', 'customer_id', 'periodo', 'target_tn', 'tn', 'registro_sintetico']\n",
      "Codificando variables categóricas...\n",
      "  cat1: 4 categorías únicas\n",
      "  cat2: 15 categorías únicas\n",
      "  brand: 36 categorías únicas\n",
      "\n",
      "Features finales: 33\n",
      "Shape final - X: (15628837, 33), y: (15628837,)\n",
      "Train: (10002455, 33), Val: (2500614, 33), Test: (3125768, 33)\n",
      "Estandarizando variables numéricas...\n",
      "Estandarizadas 30 variables numéricas\n",
      "Estandarizando variables numéricas...\n",
      "Estandarizadas 30 variables numéricas\n",
      "Entrenando modelo LightGBM...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttrain's rmse: 0.910105\tval's rmse: 1.02815\n",
      "[200]\ttrain's rmse: 0.86374\tval's rmse: 1.01893\n",
      "[300]\ttrain's rmse: 0.831852\tval's rmse: 1.01585\n",
      "[400]\ttrain's rmse: 0.8049\tval's rmse: 1.01595\n",
      "Early stopping, best iteration is:\n",
      "[358]\ttrain's rmse: 0.815788\tval's rmse: 1.01524\n",
      "\n",
      "=== ANÁLISIS DEL MODELO ===\n",
      "Top 10 features más importantes:\n",
      "                                 feature    importance\n",
      "21                             ma_tn_12m  1.902882e+07\n",
      "13                     coef_variacion_6m  1.174474e+07\n",
      "29                              ma_tn_6m  8.752325e+06\n",
      "11  desviacion_vs_promedio_historico_mes  7.108065e+06\n",
      "5                       cust_request_qty  6.950550e+06\n",
      "16                           delta_tn_1m  6.726928e+06\n",
      "28                           delta_tn_3m  4.940555e+06\n",
      "32                                 brand  4.389301e+06\n",
      "25                promedio_historico_mes  3.945912e+06\n",
      "18                              ma_tn_3m  3.659212e+06\n",
      "\n",
      "Análisis de residuos:\n",
      "  Media de residuos: -0.0006 (debe estar cerca de 0)\n",
      "  Std de residuos: 1.0152\n",
      "  % predicciones negativas: 0.00%\n",
      "  MAE validación: 0.0729\n",
      "  R² validación: 0.4430\n",
      "Evaluando modelo...\n",
      "\n",
      "Métricas del modelo:\n",
      "MAE: 0.0726\n",
      "RMSE: 0.9897\n",
      "R²: 0.4303\n",
      "\n",
      "Top 15 features más importantes:\n",
      "                                 feature    importance\n",
      "21                             ma_tn_12m  1.902882e+07\n",
      "13                     coef_variacion_6m  1.174474e+07\n",
      "29                              ma_tn_6m  8.752325e+06\n",
      "11  desviacion_vs_promedio_historico_mes  7.108065e+06\n",
      "5                       cust_request_qty  6.950550e+06\n",
      "16                           delta_tn_1m  6.726928e+06\n",
      "28                           delta_tn_3m  4.940555e+06\n",
      "32                                 brand  4.389301e+06\n",
      "25                promedio_historico_mes  3.945912e+06\n",
      "18                              ma_tn_3m  3.659212e+06\n",
      "14                    antiguedad_cliente  3.042886e+06\n",
      "26                   antiguedad_producto  3.014562e+06\n",
      "1                           tendencia_6m  3.008778e+06\n",
      "7                           delta_tn_12m  2.439241e+06\n",
      "17                       cust_request_tn  2.329083e+06\n",
      "\n",
      "Guardando modelo...\n",
      "Modelo guardado como '02_lgbm_model.txt'\n",
      "Objetos auxiliares guardados como 'model_objects.pkl'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuración\n",
    "PERIODO_CORTE = 201912  # Entrenar hasta este periodo\n",
    "TARGET_LAG = 2  # Predecir periodo +2\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"\n",
    "    Carga y prepara los datos para el modelo\n",
    "    \"\"\"\n",
    "    # Conectar a la base de datos DuckDB\n",
    "    con = duckdb.connect(database='../input/db/labo3.duckdb')\n",
    "    \n",
    "    # Cargar datos de la tabla ventas_features_final\n",
    "    query = \"\"\"SELECT \n",
    "        antiguedad_cliente,\n",
    "        antiguedad_producto,\n",
    "        cust_request_qty,\n",
    "        cust_request_tn,\n",
    "        customer_id,\n",
    "        periodo,\n",
    "        plan_precios_cuidados,\n",
    "        product_id,\n",
    "        registro_sintetico,\n",
    "        tn,\n",
    "        coef_variacion_6m,\n",
    "        delta_tn_12m,\n",
    "        delta_tn_1m,\n",
    "        delta_tn_3m,\n",
    "        delta_tn_6m,\n",
    "        is_max_tn_12m,\n",
    "        is_max_tn_3m,\n",
    "        is_max_tn_6m,\n",
    "        is_min_tn_12m,\n",
    "        is_min_tn_3m,\n",
    "        is_min_tn_6m,\n",
    "        ma_tn_12m,\n",
    "        ma_tn_3m,\n",
    "        ma_tn_6m,\n",
    "        tendencia_6m,\n",
    "        tn_lag_12m,\n",
    "        tn_lag_1m,\n",
    "        tn_lag_3m,\n",
    "        tn_lag_6m,\n",
    "        tn_mismo_mes_anio_anterior,\n",
    "        trimestre,\n",
    "        desviacion_vs_promedio_historico_mes,\n",
    "        meses_desde_ultima_compra,\n",
    "        promedio_historico_mes,\n",
    "        stddev_historico_mes,\n",
    "        cat1,\n",
    "        cat2,\n",
    "        -- cat3,\n",
    "        brand\n",
    "        -- sku_size \n",
    "    FROM ventas_features_final\"\"\"\n",
    "    df = con.execute(query).df()\n",
    "    \n",
    "    # Cerrar conexión\n",
    "    con.close()\n",
    "    print(f\"Datos originales: {df.shape}\")\n",
    "    print(f\"Periodos únicos: {sorted(df['periodo'].unique())}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_target_variable_correct(df):\n",
    "    # Ordenar por cliente, producto, periodo\n",
    "    df = df.sort_values(['customer_id', 'product_id', 'periodo'])\n",
    "    \n",
    "    # Target = tn individual en periodo +2\n",
    "    df['target_tn'] = df.groupby(['customer_id', 'product_id'])['tn'].shift(-TARGET_LAG)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_target_variable_v_old(df):\n",
    "    \"\"\"\n",
    "    Crea la variable objetivo (tn en periodo +2) agrupada por product_id\n",
    "    \"\"\"\n",
    "    print(\"Creando variable objetivo...\")\n",
    "    \n",
    "    # Agregar tn por product_id y periodo\n",
    "    df_agg = df.groupby(['product_id', 'periodo']).agg({\n",
    "        'tn': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Ordenar por product_id y periodo\n",
    "    df_agg = df_agg.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    # Crear target variable (tn en periodo +2)\n",
    "    df_agg['target_tn'] = df_agg.groupby('product_id')['tn'].shift(-TARGET_LAG)\n",
    "    \n",
    "    # Merge de vuelta con el dataset original\n",
    "    df = df.merge(df_agg[['product_id', 'periodo', 'target_tn']], \n",
    "                  on=['product_id', 'periodo'], how='left')\n",
    "    \n",
    "    print(f\"Registros con target válido: {df['target_tn'].notna().sum()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_features(df):\n",
    "    \"\"\"\n",
    "    Prepara las features excluyendo variables key_*, fechas y separando categóricas\n",
    "    \"\"\"\n",
    "    print(\"Preparando features...\")\n",
    "    \n",
    "    # Variables a excluir\n",
    "    exclude_vars = [col for col in df.columns if col.startswith('key_')]\n",
    "    exclude_vars.extend(['target_tn', 'tn'])  # Excluir también el target y la variable original\n",
    "    \n",
    "    # Excluir variables de fecha que no son útiles como features numéricas\n",
    "    date_vars = ['PVC', 'PVP', 'UVC', 'UVP', 'periodo_fecha']\n",
    "    exclude_vars.extend([col for col in date_vars if col in df.columns])\n",
    "    \n",
    "    # Variables categóricas\n",
    "    categorical_vars = ['cat1', 'cat2', 'cat3', 'brand']\n",
    "    \n",
    "    # Variables numéricas (todas las demás excepto las excluidas)\n",
    "    all_vars = set(df.columns)\n",
    "    numeric_vars = list(all_vars - set(exclude_vars) - set(categorical_vars))\n",
    "    \n",
    "    print(f\"Variables categóricas ({len(categorical_vars)}): {categorical_vars}\")\n",
    "    print(f\"Variables numéricas ({len(numeric_vars)}): {numeric_vars[:10]}...\")  # Mostrar solo las primeras 10\n",
    "    print(f\"Variables excluidas ({len(exclude_vars)}): {exclude_vars}\")\n",
    "    \n",
    "    return numeric_vars, categorical_vars, exclude_vars\n",
    "\n",
    "def prepare_features_fixed(df):\n",
    "    \"\"\"\n",
    "    Función CORREGIDA que FUERZA la exclusión de variables problemáticas\n",
    "    \"\"\"\n",
    "    print(\"Preparando features (versión CORREGIDA)...\")\n",
    "    \n",
    "    # 1. EXCLUSIÓN FORZADA - ESTAS VARIABLES NO PUEDEN SER FEATURES\n",
    "    forced_excludes = [\n",
    "        'product_id',           # ID del producto - NO es feature\n",
    "        'customer_id',          # ID del cliente - NO es feature  \n",
    "        'periodo',              # Periodo - NO es feature\n",
    "        'target_tn',            # Variable objetivo\n",
    "        'tn',                   # Variable original\n",
    "        'registro_sintetico'    # Variable sintética\n",
    "    ]\n",
    "    \n",
    "    # 2. Variables categóricas\n",
    "    categorical_vars = ['cat1', 'cat2', 'brand']  # Solo las que existen\n",
    "    \n",
    "    # 3. Calcular variables numéricas = TODO lo demás\n",
    "    all_columns = set(df.columns)\n",
    "    excluded_set = set(forced_excludes)\n",
    "    categorical_set = set(categorical_vars)\n",
    "    \n",
    "    numeric_vars = list(all_columns - excluded_set - categorical_set)\n",
    "    \n",
    "    # 4. VERIFICACIÓN CRÍTICA\n",
    "    contaminated = []\n",
    "    for var in numeric_vars:\n",
    "        if var in ['product_id', 'customer_id', 'periodo']:\n",
    "            contaminated.append(var)\n",
    "    \n",
    "    if contaminated:\n",
    "        print(f\"🚨 ERROR CRÍTICO: Variables prohibidas detectadas: {contaminated}\")\n",
    "        print(\"🛑 DETENIENDO EJECUCIÓN\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(f\"✅ VERIFICACIÓN PASADA\")\n",
    "    print(f\"Variables categóricas ({len(categorical_vars)}): {categorical_vars}\")\n",
    "    print(f\"Variables numéricas ({len(numeric_vars)}): {numeric_vars[:5]}...\")\n",
    "    print(f\"Variables excluidas ({len(forced_excludes)}): {forced_excludes}\")\n",
    "    \n",
    "    return numeric_vars, categorical_vars, forced_excludes\n",
    "\n",
    "\n",
    "def encode_categorical_variables(df, categorical_vars):\n",
    "    \"\"\"\n",
    "    Codifica variables categóricas usando LabelEncoder\n",
    "    \"\"\"\n",
    "    print(\"Codificando variables categóricas...\")\n",
    "    \n",
    "    label_encoders = {}\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    for col in categorical_vars:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            # Manejar valores nulos\n",
    "            df_encoded[col] = df_encoded[col].fillna('UNKNOWN')\n",
    "            df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "            print(f\"  {col}: {len(le.classes_)} categorías únicas\")\n",
    "    \n",
    "    return df_encoded, label_encoders\n",
    "\n",
    "def clean_numeric_data(df):\n",
    "    \"\"\"\n",
    "    Limpia y convierte datos numéricos para evitar problemas\n",
    "    \"\"\"\n",
    "    print(\"Limpiando datos numéricos...\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convertir plan_precios_cuidados de bool a int\n",
    "    if 'plan_precios_cuidados' in df_clean.columns:\n",
    "        df_clean['plan_precios_cuidados'] = df_clean['plan_precios_cuidados'].astype(int)\n",
    "        print(\"  Convertido plan_precios_cuidados de bool a int\")\n",
    "    \n",
    "    # Identificar columnas numéricas\n",
    "    numeric_columns = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        # Reemplazar infinitos por NaN\n",
    "        df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Convertir float64 a float32 para ahorrar memoria y evitar problemas de precisión\n",
    "        if df_clean[col].dtype == 'float64':\n",
    "            df_clean[col] = df_clean[col].astype('float32')\n",
    "    \n",
    "    print(f\"  Limpiadas {len(numeric_columns)} columnas numéricas\")\n",
    "    print(\"  Convertidos float64 -> float32\")\n",
    "    print(\"  Reemplazados valores infinitos por NaN\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def standardize_numeric_features(X_train, X_test, numeric_vars):\n",
    "    \"\"\"\n",
    "    Estandariza variables numéricas\n",
    "    \"\"\"\n",
    "    print(\"Estandarizando variables numéricas...\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Copiar los datasets\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    \n",
    "    # Manejar valores nulos antes de estandarizar\n",
    "    X_train_scaled = X_train_scaled.fillna(0)\n",
    "    X_test_scaled = X_test_scaled.fillna(0)\n",
    "    \n",
    "    # Identificar columnas numéricas que existen en los datos\n",
    "    numeric_cols_present = [col for col in numeric_vars if col in X_train.columns]\n",
    "    \n",
    "    # Verificar que las columnas sean realmente numéricas y no tengan problemas\n",
    "    numeric_cols_valid = []\n",
    "    for col in numeric_cols_present:\n",
    "        if X_train_scaled[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "            # Verificar que no haya infinitos o valores muy grandes\n",
    "            if not (np.isinf(X_train_scaled[col]).any() or np.isinf(X_test_scaled[col]).any()):\n",
    "                # Verificar que no haya valores extremadamente grandes\n",
    "                max_val = max(X_train_scaled[col].abs().max(), X_test_scaled[col].abs().max())\n",
    "                if max_val < 1e10:  # Límite razonable\n",
    "                    numeric_cols_valid.append(col)\n",
    "                else:\n",
    "                    print(f\"  Saltando {col}: valores muy grandes (max: {max_val:.2e})\")\n",
    "            else:\n",
    "                print(f\"  Saltando {col}: contiene valores infinitos\")\n",
    "        else:\n",
    "            print(f\"  Saltando columna no numérica: {col} (tipo: {X_train_scaled[col].dtype})\")\n",
    "    \n",
    "    if numeric_cols_valid:\n",
    "        # Estandarizar solo las columnas numéricas válidas\n",
    "        X_train_scaled[numeric_cols_valid] = scaler.fit_transform(X_train_scaled[numeric_cols_valid])\n",
    "        X_test_scaled[numeric_cols_valid] = scaler.transform(X_test_scaled[numeric_cols_valid])\n",
    "        print(f\"Estandarizadas {len(numeric_cols_valid)} variables numéricas\")\n",
    "    else:\n",
    "        print(\"No se encontraron variables numéricas válidas para estandarizar\")\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "def train_lgbm_model(X_train, y_train, X_val, y_val, categorical_features):\n",
    "    \"\"\"\n",
    "    Entrena modelo LightGBM\n",
    "    \"\"\"\n",
    "    print(\"Entrenando modelo LightGBM...\")\n",
    "    \n",
    "    # Parámetros del modelo\n",
    "    params = {\n",
    "        #'objective': 'regression',\n",
    "        #'metric': 'rmse',\n",
    "        #'boosting_type': 'gbdt',\n",
    "        #'num_leaves': 50,  # Aumentado\n",
    "        #'learning_rate': 0.03,  # Reducido\n",
    "        #'feature_fraction': 0.8,\n",
    "        #'bagging_fraction': 0.8,\n",
    "        #'bagging_freq': 5,\n",
    "        #'min_data_in_leaf': 100,  # NUEVO - evita overfitting\n",
    "        #'lambda_l1': 0.1,  # NUEVO - regularización\n",
    "        #'lambda_l2': 0.1,  # NUEVO - regularización\n",
    "        #'verbose': -1,\n",
    "        #'random_state': 42,\n",
    "        #'force_col_wise': True\n",
    "\n",
    "        'n_jobs': -1, \n",
    "        'device': 'gpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "\n",
    "        #Mejor corrida\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        'num_leaves': 31, # la corrida en en kaggle mejor tiene 31 leaves\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'random_state': 42,\n",
    "        #'sample_weight': True, \n",
    "        #'linear_tree': True,\n",
    "\n",
    "        #'num_leaves': 63,\n",
    "        #'learning_rate': 0.03738149205005092,\n",
    "        #'feature_fraction': 0.5576004633455672,\n",
    "        #'bagging_fraction': 0.8444560165276432,\n",
    "        #'bagging_freq': 1,\n",
    "        #'min_data_in_leaf': 83,  # NUEVO - evita overfitting\n",
    "        #'lambda_l1': 0.0016447293604905321,\n",
    "        #'lambda_l2': 3.901069568054151e-06,  # NUEVO - regularización\n",
    "        #'min_gain_to_split': 0.8073822295272939,  # NUEVO - evita splits innecesarios\n",
    "        #'max_depth': 11,  # Controla profundidad del árbol\n",
    "    }\n",
    "    \n",
    "    # Crear datasets de LightGBM\n",
    "    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=categorical_features, reference=train_data)\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'val'],\n",
    "        num_boost_round=1000,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=100)]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_lgbm_model_improved(X_train, y_train, X_val, y_val, categorical_features):\n",
    "    \"\"\"\n",
    "    Entrena modelo LightGBM con hiperparámetros optimizados\n",
    "    REEMPLAZA tu función train_lgbm_model() existente con esta\n",
    "    \"\"\"\n",
    "    print(\"Entrenando modelo LightGBM mejorado...\")\n",
    "    \n",
    "    # Parámetros optimizados para regresión de ventas\n",
    "    params = {\n",
    "        'device': 'gpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        \n",
    "        # COMPLEJIDAD DEL MODELO\n",
    "        'num_leaves': 80,               # Aumentado de 31 → captura más patrones\n",
    "        'max_depth': 7,                 # Controla profundidad del árbol\n",
    "        'min_data_in_leaf': 30,         # Mínimo de datos por hoja → evita overfitting\n",
    "        \n",
    "        # LEARNING RATE\n",
    "        'learning_rate': 0.025,         # Reducido de 0.05 → convergencia más lenta pero mejor\n",
    "        \n",
    "        # SAMPLING Y FEATURES\n",
    "        'feature_fraction': 0.75,       # 75% de features por árbol → más diversidad\n",
    "        'bagging_fraction': 0.75,       # 75% de datos por árbol → reduce overfitting  \n",
    "        'bagging_freq': 3,              # Cada 3 iteraciones → más randomización\n",
    "        \n",
    "        # REGULARIZACIÓN\n",
    "        'lambda_l1': 0.05,              # Regularización L1 → selección de features\n",
    "        'lambda_l2': 0.05,              # Regularización L2 → suaviza weights\n",
    "        'min_gain_to_split': 0.05,      # Ganancia mínima para split → evita ruido\n",
    "        'min_sum_hessian_in_leaf': 20,  # Control adicional de overfitting\n",
    "        \n",
    "        # MANEJO DE CATEGORICAS\n",
    "        'cat_smooth': 10,               # Suavizado para variables categóricas\n",
    "        'max_cat_threshold': 32,        # Máximo threshold para categóricas\n",
    "        \n",
    "        # PERFORMANCE\n",
    "        'verbose': -1,\n",
    "        'random_state': 42,\n",
    "        'force_col_wise': True,\n",
    "        #'n_jobs': -1,                   # Usar todos los cores\n",
    "        'deterministic': True           # Para reproducibilidad\n",
    "    }\n",
    "    \n",
    "    # Configuración de entrenamiento\n",
    "    training_config = {\n",
    "        'num_boost_round': 2500,        # Más iteraciones para mejor convergencia\n",
    "        'early_stopping_rounds': 200,   # Más paciencia antes de parar\n",
    "        'verbose_eval': 100             # Log cada 100 iteraciones\n",
    "    }\n",
    "    \n",
    "    print(f\"Configuración del modelo:\")\n",
    "    print(f\"  Learning rate: {params['learning_rate']}\")\n",
    "    print(f\"  Num leaves: {params['num_leaves']}\")\n",
    "    print(f\"  Max depth: {params['max_depth']}\")\n",
    "    print(f\"  Feature fraction: {params['feature_fraction']}\")\n",
    "    print(f\"  Max iterations: {training_config['num_boost_round']}\")\n",
    "    print(f\"  Early stopping: {training_config['early_stopping_rounds']}\")\n",
    "    \n",
    "    # Crear datasets de LightGBM\n",
    "    train_data = lgb.Dataset(\n",
    "        X_train, \n",
    "        label=y_train, \n",
    "        categorical_feature=categorical_features,\n",
    "        free_raw_data=False  # Mantener datos para debugging\n",
    "    )\n",
    "    \n",
    "    val_data = lgb.Dataset(\n",
    "        X_val, \n",
    "        label=y_val, \n",
    "        categorical_feature=categorical_features, \n",
    "        reference=train_data,\n",
    "        free_raw_data=False\n",
    "    )\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'val'],\n",
    "        num_boost_round=training_config['num_boost_round'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=training_config['early_stopping_rounds']), \n",
    "            lgb.log_evaluation(period=training_config['verbose_eval'])\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Entrenamiento completado:\")\n",
    "    print(f\"  Mejor iteración: {model.best_iteration}\")\n",
    "    \n",
    "    # Manejo seguro de best_score (formato puede variar)\n",
    "    try:\n",
    "        train_score = model.best_score['train']['rmse']\n",
    "        val_score = model.best_score['valid_1']['rmse']\n",
    "        print(f\"  Score final train: {train_score:.4f}\")\n",
    "        print(f\"  Score final val: {val_score:.4f}\")\n",
    "    except (KeyError, TypeError):\n",
    "        # Formato alternativo o no disponible\n",
    "        print(f\"  Scores finales disponibles en model.best_score\")\n",
    "        print(f\"  Keys disponibles: {list(model.best_score.keys()) if hasattr(model, 'best_score') else 'No disponible'}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def analyze_model_performance(model, feature_names, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Función opcional para analizar el rendimiento del modelo mejorado\n",
    "    \"\"\"\n",
    "    print(\"\\n=== ANÁLISIS DEL MODELO ===\")\n",
    "    \n",
    "    # 1. Feature importance\n",
    "    try:\n",
    "        importance = model.feature_importance(importance_type='gain')\n",
    "        feature_imp = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"Top 10 features más importantes:\")\n",
    "        print(feature_imp.head(10)[['feature', 'importance']])\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculando feature importance: {e}\")\n",
    "        feature_imp = None\n",
    "    \n",
    "    # 2. Predicciones de validación\n",
    "    try:\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # 3. Análisis de residuos\n",
    "        residuals = y_val - y_pred_val\n",
    "        print(f\"\\nAnálisis de residuos:\")\n",
    "        print(f\"  Media de residuos: {residuals.mean():.4f} (debe estar cerca de 0)\")\n",
    "        print(f\"  Std de residuos: {residuals.std():.4f}\")\n",
    "        print(f\"  % predicciones negativas: {(y_pred_val < 0).mean()*100:.2f}%\")\n",
    "        \n",
    "        # 4. Métricas adicionales\n",
    "        from sklearn.metrics import mean_absolute_error, r2_score\n",
    "        mae = mean_absolute_error(y_val, y_pred_val)\n",
    "        r2 = r2_score(y_val, y_pred_val)\n",
    "        print(f\"  MAE validación: {mae:.4f}\")\n",
    "        print(f\"  R² validación: {r2:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en análisis de predicciones: {e}\")\n",
    "    \n",
    "    return feature_imp\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo\n",
    "    \"\"\"\n",
    "    print(\"Evaluando modelo...\")\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Métricas\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nMétricas del modelo:\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    \n",
    "    return {'mae': mae, 'rmse': rmse, 'r2': r2, 'predictions': y_pred}\n",
    "\n",
    "def get_feature_importance(model, feature_names):\n",
    "    \"\"\"\n",
    "    Obtiene importancia de features\n",
    "    \"\"\"\n",
    "    importance = model.feature_importance(importance_type='gain')\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 features más importantes:\")\n",
    "    print(feature_imp.head(15))\n",
    "    \n",
    "    return feature_imp\n",
    "\n",
    "def verify_model_contamination():\n",
    "    \"\"\"\n",
    "    Verifica si el modelo actual está contaminado\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pickle\n",
    "        with open('model_objects.pkl', 'rb') as f:\n",
    "            objects = pickle.load(f)\n",
    "        \n",
    "        feature_cols = objects['feature_cols']\n",
    "        \n",
    "        contaminated = []\n",
    "        for var in ['product_id', 'customer_id', 'periodo']:\n",
    "            if var in feature_cols:\n",
    "                contaminated.append(var)\n",
    "        \n",
    "        if contaminated:\n",
    "            print(f\"🚨 MODELO CONTAMINADO: Contiene {contaminated}\")\n",
    "            print(\"🔄 NECESITAS RE-ENTRENAR con prepare_features_fixed()\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"✅ Modelo limpio\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo verificar: {e}\")\n",
    "        return True\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal\n",
    "    \"\"\"\n",
    "    print(\"=== MODELO LGBM PARA PREDICCIÓN DE TONELADAS ===\\n\")\n",
    "    \n",
    "    # 0. Verificar si el modelo está contaminado\n",
    "    #if verify_model_contamination():\n",
    "    #    print(\"⚠️ El modelo está contaminado. Por favor, re-entrena usando prepare_features_fixed().\")\n",
    "    #    return None, None, None\n",
    "\n",
    "    # 1. Cargar datos\n",
    "    df = load_and_prepare_data()\n",
    "    \n",
    "    # 2. Crear variable objetivo\n",
    "    df = create_target_variable_correct(df)\n",
    "    \n",
    "    # 2.5. Limpiar datos numéricos (nueva función)\n",
    "    df = clean_numeric_data(df)\n",
    "    \n",
    "    # 3. Filtrar datos para entrenamiento (hasta PERIODO_CORTE)\n",
    "    train_data = df[df['periodo'] <= PERIODO_CORTE].copy()\n",
    "    print(f\"\\nDatos de entrenamiento: {train_data.shape}\")\n",
    "    \n",
    "    # 4. Remover registros sin target\n",
    "    train_data = train_data.dropna(subset=['target_tn'])\n",
    "    print(f\"Datos después de remover NaN en target: {train_data.shape}\")\n",
    "    \n",
    "    # 5. Preparar features\n",
    "    #numeric_vars, categorical_vars, exclude_vars = prepare_features(train_data)\n",
    "    numeric_vars, categorical_vars, exclude_vars = prepare_features_fixed(train_data)\n",
    "\n",
    "    # 6. Codificar variables categóricas\n",
    "    train_data_encoded, label_encoders = encode_categorical_variables(train_data, categorical_vars)\n",
    "    \n",
    "    # 7. Seleccionar features finales\n",
    "    feature_cols = numeric_vars + categorical_vars\n",
    "    feature_cols = [col for col in feature_cols if col in train_data_encoded.columns]\n",
    "    \n",
    "    print(f\"\\nFeatures finales: {len(feature_cols)}\")\n",
    "    \n",
    "    # 8. Preparar X y y\n",
    "    X = train_data_encoded[feature_cols].copy()\n",
    "    y = train_data_encoded['target_tn'].copy()\n",
    "    \n",
    "    # Manejar valores nulos en X\n",
    "    X = X.fillna(0)\n",
    "    \n",
    "    print(f\"Shape final - X: {X.shape}, y: {y.shape}\")\n",
    "    \n",
    "    # 9. Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=None\n",
    "    )\n",
    "    \n",
    "    # 10. Split train/validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    # 11. Estandarizar variables numéricas\n",
    "    X_train_scaled, X_val_scaled, scaler = standardize_numeric_features(X_train, X_val, numeric_vars)\n",
    "    X_test_scaled, _, _ = standardize_numeric_features(X_test, X_test, numeric_vars)\n",
    "    \n",
    "    # 12. Identificar features categóricas para LightGBM\n",
    "    categorical_indices = [i for i, col in enumerate(feature_cols) if col in categorical_vars]\n",
    "    \n",
    "    # 13. Entrenar modelo\n",
    "    model = train_lgbm_model(\n",
    "        X_train_scaled, y_train, \n",
    "        X_val_scaled, y_val, \n",
    "        categorical_indices\n",
    "    )\n",
    "\n",
    "    feature_analysis = analyze_model_performance(model, feature_cols, X_val_scaled, y_val)\n",
    "    \n",
    "    # 14. Evaluar modelo\n",
    "    results = evaluate_model(model, X_test_scaled, y_test)\n",
    "    \n",
    "    # 15. Feature importance\n",
    "    feature_importance = get_feature_importance(model, feature_cols)\n",
    "    \n",
    "    # 16. Guardar modelo y objetos necesarios\n",
    "    print(\"\\nGuardando modelo...\")\n",
    "    model.save_model('../output/lgbm/02_lgbm_model.txt')\n",
    "    \n",
    "    # Guardar escalador y encoders\n",
    "    import pickle\n",
    "    with open('model_objects.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'scaler': scaler,\n",
    "            'label_encoders': label_encoders,\n",
    "            'feature_cols': feature_cols,\n",
    "            'categorical_vars': categorical_vars,\n",
    "            'numeric_vars': numeric_vars\n",
    "        }, f)\n",
    "    \n",
    "    print(\"Modelo guardado como '02_lgbm_model.txt'\")\n",
    "    print(\"Objetos auxiliares guardados como 'model_objects.pkl'\")\n",
    "    \n",
    "    return model, results, feature_importance\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, results, feature_importance = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0ef9539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREDICCIÓN PARA PRODUCTOS ESPECÍFICOS ===\\n\n",
      "Cargando productos a predecir...\n",
      "Productos a predecir: 780\n",
      "Product_ids únicos: 780\n",
      "Cargando modelo y objetos auxiliares...\n",
      "Cargando datos más recientes para predicción...\n",
      "Datos del periodo 201912: (553419, 98)\n",
      "Preparando datos para predicción...\n",
      "Productos a predecir: 780\n",
      "Registros filtrados: (465660, 98)\n",
      "Limpiando datos numéricos para predicción...\n",
      "  Limpiadas 85 columnas numéricas\n",
      "Generando predicciones individuales...\n",
      "Predicciones individuales generadas: 465660\n",
      "Rango de predicciones: -1.10 - 102.70\n",
      "Promedio predicción individual: 0.06\n",
      "Agrupando predicciones por product_id...\n",
      "Resultados finales:\n",
      "Productos con predicción: 780\n",
      "Total tn predichas: 26873.18\n",
      "Promedio tn por producto: 34.45\n",
      "Máximo tn por producto: 1292.36\n",
      "Top 10 productos con mayor predicción:\n",
      "    product_id           tn\n",
      "0        20001  1292.356422\n",
      "2        20003   628.009509\n",
      "3        20004   586.016164\n",
      "1        20002   558.464380\n",
      "4        20005   489.382897\n",
      "8        20009   452.149353\n",
      "31       20032   420.153880\n",
      "6        20007   395.831839\n",
      "10       20011   366.970870\n",
      "13       20014   365.270262\n",
      "Resultado final: 780 productos\n",
      "Predicciones guardadas en: ../output/lgbm/02_lgbm_predicciones.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CHUNK DE PREDICCIÓN CORREGIDO PARA EL NUEVO ENFOQUE\n",
    "# =============================================================================\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_productos_a_predecir():\n",
    "    \"\"\"\n",
    "    Carga la lista de productos a predecir desde DuckDB\n",
    "    \"\"\"\n",
    "    print(\"Cargando productos a predecir...\")\n",
    "    \n",
    "    # Conectar a la base de datos DuckDB\n",
    "    con = duckdb.connect(database='../input/db/labo3.duckdb')\n",
    "    \n",
    "    # Cargar productos a predecir\n",
    "    query = \"SELECT * FROM tb_productos_a_predecir\"\n",
    "    productos_df = con.execute(query).df()\n",
    "    \n",
    "    con.close()\n",
    "    \n",
    "    print(f\"Productos a predecir: {len(productos_df)}\")\n",
    "    print(f\"Product_ids únicos: {productos_df['product_id'].nunique()}\")\n",
    "    \n",
    "    return productos_df\n",
    "\n",
    "def load_latest_data_for_prediction():\n",
    "    \"\"\"\n",
    "    Carga los datos más recientes para hacer predicciones\n",
    "    \"\"\"\n",
    "    print(\"Cargando datos más recientes para predicción...\")\n",
    "    \n",
    "    # Conectar a la base de datos DuckDB\n",
    "    con = duckdb.connect(database='../input/db/labo3.duckdb')\n",
    "    \n",
    "    # Cargar datos del periodo 201912 - TODOS los campos como en entrenamiento\n",
    "    query = \"\"\"SELECT \n",
    "        *\n",
    "    FROM ventas_features_final \n",
    "    WHERE periodo = 201912\"\"\"\n",
    "    \n",
    "    df = con.execute(query).df()\n",
    "    con.close()\n",
    "    \n",
    "    print(f\"Datos del periodo 201912: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_model_and_objects():\n",
    "    \"\"\"\n",
    "    Carga el modelo entrenado y objetos auxiliares\n",
    "    \"\"\"\n",
    "    print(\"Cargando modelo y objetos auxiliares...\")\n",
    "    \n",
    "    # Cargar modelo LightGBM\n",
    "    model = lgb.Booster(model_file='../output/lgbm/02_lgbm_model.txt')\n",
    "    \n",
    "    # Cargar objetos auxiliares\n",
    "    with open('model_objects.pkl', 'rb') as f:\n",
    "        objects = pickle.load(f)\n",
    "    \n",
    "    return model, objects\n",
    "\n",
    "def clean_numeric_data_prediction(df):\n",
    "    \"\"\"\n",
    "    Aplica la misma limpieza de datos numéricos que en entrenamiento\n",
    "    \"\"\"\n",
    "    print(\"Limpiando datos numéricos para predicción...\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convertir plan_precios_cuidados de bool a int\n",
    "    if 'plan_precios_cuidados' in df_clean.columns:\n",
    "        df_clean['plan_precios_cuidados'] = df_clean['plan_precios_cuidados'].astype(int)\n",
    "    \n",
    "    # Identificar columnas numéricas\n",
    "    numeric_columns = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        # Reemplazar infinitos por NaN\n",
    "        df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Convertir float64 a float32\n",
    "        if df_clean[col].dtype == 'float64':\n",
    "            df_clean[col] = df_clean[col].astype('float32')\n",
    "    \n",
    "    print(f\"  Limpiadas {len(numeric_columns)} columnas numéricas\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def prepare_prediction_data(df, objects, productos_a_predecir):\n",
    "    \"\"\"\n",
    "    Prepara datos para predicción - SOLO productos solicitados\n",
    "    \"\"\"\n",
    "    print(\"Preparando datos para predicción...\")\n",
    "    \n",
    "    # FILTRAR solo productos que necesitamos predecir\n",
    "    product_ids_to_predict = productos_a_predecir['product_id'].unique()\n",
    "    df_filtered = df[df['product_id'].isin(product_ids_to_predict)].copy()\n",
    "    \n",
    "    print(f\"Productos a predecir: {len(product_ids_to_predict)}\")\n",
    "    print(f\"Registros filtrados: {df_filtered.shape}\")\n",
    "    \n",
    "    if df_filtered.empty:\n",
    "        print(\"¡ADVERTENCIA! No se encontraron datos para los productos a predecir.\")\n",
    "        return None\n",
    "    \n",
    "    # Aplicar limpieza de datos\n",
    "    df_filtered = clean_numeric_data_prediction(df_filtered)\n",
    "    \n",
    "    scaler = objects['scaler']\n",
    "    label_encoders = objects['label_encoders']\n",
    "    feature_cols = objects['feature_cols']\n",
    "    categorical_vars = objects['categorical_vars']\n",
    "    numeric_vars = objects['numeric_vars']\n",
    "    \n",
    "    # Codificar variables categóricas\n",
    "    for col in categorical_vars:\n",
    "        if col in df_filtered.columns and col in label_encoders:\n",
    "            le = label_encoders[col]\n",
    "            df_filtered[col] = df_filtered[col].fillna('UNKNOWN')\n",
    "            \n",
    "            # Manejar categorías no vistas durante el entrenamiento\n",
    "            unknown_mask = ~df_filtered[col].astype(str).isin(le.classes_)\n",
    "            df_filtered[col] = df_filtered[col].astype(str)\n",
    "            df_filtered.loc[unknown_mask, col] = 'UNKNOWN'\n",
    "            \n",
    "            # Si 'UNKNOWN' no está en las clases, usar la primera clase\n",
    "            if 'UNKNOWN' not in le.classes_:\n",
    "                df_filtered.loc[unknown_mask, col] = le.classes_[0]\n",
    "            \n",
    "            df_filtered[col] = le.transform(df_filtered[col])\n",
    "    \n",
    "    # Seleccionar features\n",
    "    missing_features = [col for col in feature_cols if col not in df_filtered.columns]\n",
    "    if missing_features:\n",
    "        print(f\"¡ADVERTENCIA! Features faltantes: {missing_features}\")\n",
    "        # Crear columnas faltantes con valor 0\n",
    "        for col in missing_features:\n",
    "            df_filtered[col] = 0\n",
    "    \n",
    "    X_pred = df_filtered[feature_cols].copy()\n",
    "    \n",
    "    # Manejar valores nulos\n",
    "    X_pred = X_pred.fillna(0)\n",
    "    \n",
    "    # Estandarizar variables numéricas\n",
    "    numeric_cols_present = [col for col in numeric_vars if col in X_pred.columns]\n",
    "    numeric_cols_valid = []\n",
    "    \n",
    "    for col in numeric_cols_present:\n",
    "        if X_pred[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "            if not np.isinf(X_pred[col]).any():\n",
    "                max_val = X_pred[col].abs().max()\n",
    "                if max_val < 1e10:\n",
    "                    numeric_cols_valid.append(col)\n",
    "    \n",
    "    if numeric_cols_valid:\n",
    "        X_pred[numeric_cols_valid] = scaler.transform(X_pred[numeric_cols_valid])\n",
    "    \n",
    "    # Conservar información para el resultado final\n",
    "    result_info = df_filtered[['customer_id', 'product_id']].copy()\n",
    "    \n",
    "    return X_pred, result_info\n",
    "\n",
    "\n",
    "\n",
    "def make_predictions_for_products():\n",
    "    \"\"\"\n",
    "    Función principal para hacer predicciones para productos específicos\n",
    "    \"\"\"\n",
    "    print(\"=== PREDICCIÓN PARA PRODUCTOS ESPECÍFICOS ===\\\\n\")\n",
    "    \n",
    "    # 1. Cargar productos a predecir\n",
    "    productos_a_predecir = load_productos_a_predecir()\n",
    "    \n",
    "    # 2. Cargar modelo y objetos\n",
    "    model, objects = load_model_and_objects()\n",
    "    \n",
    "    # 3. Cargar datos más recientes\n",
    "    df_latest = load_latest_data_for_prediction()\n",
    "    \n",
    "    # 4. Preparar datos para predicción (solo productos solicitados)\n",
    "    prediction_result = prepare_prediction_data(df_latest, objects, productos_a_predecir)\n",
    "    \n",
    "    if prediction_result is None:\n",
    "        print(\"No se pudieron preparar los datos para predicción.\")\n",
    "        return None\n",
    "    \n",
    "    X_pred, result_info = prediction_result\n",
    "    \n",
    "    # 5. Hacer predicciones individuales (cliente-producto)\n",
    "    print(\"Generando predicciones individuales...\")\n",
    "    predictions = model.predict(X_pred)\n",
    "    \n",
    "    # 6. Agregar predicciones al resultado\n",
    "    result_info['predicted_tn'] = predictions\n",
    "    \n",
    "    print(f\"Predicciones individuales generadas: {len(predictions)}\")\n",
    "    print(f\"Rango de predicciones: {predictions.min():.2f} - {predictions.max():.2f}\")\n",
    "    print(f\"Promedio predicción individual: {predictions.mean():.2f}\")\n",
    "    \n",
    "    # 7. Agrupar por product_id y SUMAR las predicciones\n",
    "    print(\"Agrupando predicciones por product_id...\")\n",
    "    final_predictions = result_info.groupby('product_id').agg({\n",
    "        'predicted_tn': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Renombrar columna para claridad\n",
    "    final_predictions = final_predictions.rename(columns={'predicted_tn': 'tn'})\n",
    "    \n",
    "    # Ordenar por tn descendente\n",
    "    final_predictions = final_predictions.sort_values('tn', ascending=False)\n",
    "    \n",
    "    print(f\"Resultados finales:\")\n",
    "    print(f\"Productos con predicción: {len(final_predictions)}\")\n",
    "    print(f\"Total tn predichas: {final_predictions['tn'].sum():.2f}\")\n",
    "    print(f\"Promedio tn por producto: {final_predictions['tn'].mean():.2f}\")\n",
    "    print(f\"Máximo tn por producto: {final_predictions['tn'].max():.2f}\")\n",
    "    \n",
    "    # Mostrar top 10\n",
    "    print(\"Top 10 productos con mayor predicción:\")\n",
    "    print(final_predictions.head(10))\n",
    "    \n",
    "    # 8. Verificar que tenemos todos los productos solicitados\n",
    "    productos_solicitados = set(productos_a_predecir['product_id'].unique())\n",
    "    productos_predichos = set(final_predictions['product_id'].unique())\n",
    "    productos_faltantes = productos_solicitados - productos_predichos\n",
    "    \n",
    "    if productos_faltantes:\n",
    "        print(f\"¡ADVERTENCIA! Productos sin datos en 201912: {len(productos_faltantes)}\")\n",
    "        print(f\"Product_ids faltantes: {list(productos_faltantes)[:10]}...\")\n",
    "        \n",
    "        # Agregar productos faltantes con tn = 0\n",
    "        for product_id in productos_faltantes:\n",
    "            final_predictions = pd.concat([\n",
    "                final_predictions,\n",
    "                pd.DataFrame({'product_id': [product_id], 'tn': [0.0]})\n",
    "            ], ignore_index=True)\n",
    "        \n",
    "        print(f\"Productos faltantes agregados con tn=0\")\n",
    "    \n",
    "    print(f\"Resultado final: {len(final_predictions)} productos\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 9. Guardar resultados\n",
    "    output_file = '../output/lgbm/02_lgbm_predicciones.csv'\n",
    "    final_predictions.to_csv(output_file, index=False)\n",
    "    print(f\"Predicciones guardadas en: {output_file}\")\n",
    "    \n",
    "    #guardar el contenido del archivo actual en un txt\n",
    "\n",
    "\n",
    "\n",
    "    return final_predictions\n",
    "    \n",
    "\n",
    "# Ejecutar predicciones\n",
    "if __name__ == \"__main__\":\n",
    "    predicciones = make_predictions_for_products()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311 (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
