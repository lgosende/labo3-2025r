{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd556e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler, PowerTransformer\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be140ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODELO LGBM PARA PREDICCI√ìN DE TONELADAS ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29366cb203904d7b857120d6de438a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos originales: (17021654, 38)\n",
      "Periodos √∫nicos: [201701, 201702, 201703, 201704, 201705, 201706, 201707, 201708, 201709, 201710, 201711, 201712, 201801, 201802, 201803, 201804, 201805, 201806, 201807, 201808, 201809, 201810, 201811, 201812, 201901, 201902, 201903, 201904, 201905, 201906, 201907, 201908, 201909, 201910, 201911, 201912]\n",
      "Creando variable objetivo...\n",
      "Registros con target v√°lido: 15628837\n",
      "Limpiando datos num√©ricos...\n",
      "  Convertido plan_precios_cuidados de bool a int\n",
      "  Limpiadas 36 columnas num√©ricas\n",
      "  Convertidos float64 -> float32\n",
      "  Reemplazados valores infinitos por NaN\n",
      "\n",
      "Datos de entrenamiento: (17021654, 39)\n",
      "Datos despu√©s de remover NaN en target: (15628837, 39)\n",
      "=== AN√ÅLISIS DEL TARGET ===\n",
      "Target stats:\n",
      "  Count: 15,628,837\n",
      "  Min: 0.0000\n",
      "  Max: 2295.1982\n",
      "  Mean: 42.6592\n",
      "  Median: 9.6803\n",
      "  Std: 107.7060\n",
      "\n",
      "Percentiles:\n",
      "  1%: 0.0064\n",
      "  5%: 0.1873\n",
      "  10%: 0.6104\n",
      "  25%: 2.2491\n",
      "  50%: 9.6803\n",
      "  75%: 30.0179\n",
      "  90%: 106.0346\n",
      "  95%: 191.1998\n",
      "  99%: 547.6251\n",
      "\n",
      "Distribuci√≥n de valores:\n",
      "  Negativos: 0 (0.0%)\n",
      "  Ceros: 114,013 (0.7%)\n",
      "  Positivos: 15,514,824 (99.3%)\n",
      "Preparando features (versi√≥n corregida)...\n",
      "Variables categ√≥ricas (4): ['cat1', 'cat2', 'cat3', 'brand']\n",
      "Variables num√©ricas (31)\n",
      "Variables excluidas (17)\n",
      "VERIFICACI√ìN: product_id excluido = True\n",
      "VERIFICACI√ìN: customer_id excluido = True\n",
      "VERIFICACI√ìN: periodo excluido = True\n",
      "Codificando variables categ√≥ricas...\n",
      "  cat1: 4 categor√≠as √∫nicas\n",
      "  cat2: 15 categor√≠as √∫nicas\n",
      "  brand: 36 categor√≠as √∫nicas\n",
      "\n",
      "Features finales: 34\n",
      "=== DEBUG DE SELECCI√ìN DE FEATURES ===\n",
      "‚ö†Ô∏è  Variables sospechosas en features:\n",
      "  - plan_precios_cuidados\n",
      "\n",
      "Total features seleccionadas: 34\n",
      "Primeras 10 features: ['delta_tn_12m', 'delta_tn_3m', 'meses_desde_ultima_compra', 'ma_tn_6m', 'is_max_tn_6m', 'tn_lag_1m', 'ma_tn_3m', 'delta_tn_1m', 'is_max_tn_3m', 'plan_precios_cuidados']\n",
      "Shape final - X: (15628837, 34), y: (15628837,)\n",
      "Train: (10002455, 34), Val: (2500614, 34), Test: (3125768, 34)\n",
      "Estandarizando variables num√©ricas...\n",
      "Estandarizadas 31 variables num√©ricas\n",
      "Estandarizando variables num√©ricas...\n",
      "Estandarizadas 31 variables num√©ricas\n",
      "Entrenando modelo LightGBM mejorado...\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttrain's rmse: 88.6133\tval's rmse: 88.8716\n",
      "[200]\ttrain's rmse: 86.655\tval's rmse: 86.9984\n",
      "[300]\ttrain's rmse: 85.8466\tval's rmse: 86.1876\n",
      "[400]\ttrain's rmse: 85.2547\tval's rmse: 85.6309\n",
      "[500]\ttrain's rmse: 84.803\tval's rmse: 85.2165\n",
      "[600]\ttrain's rmse: 84.4419\tval's rmse: 84.8915\n",
      "[700]\ttrain's rmse: 84.102\tval's rmse: 84.5863\n",
      "[800]\ttrain's rmse: 83.7748\tval's rmse: 84.2927\n",
      "[900]\ttrain's rmse: 83.4092\tval's rmse: 83.964\n",
      "[1000]\ttrain's rmse: 83.1161\tval's rmse: 83.7137\n",
      "[1100]\ttrain's rmse: 82.8797\tval's rmse: 83.5131\n",
      "[1200]\ttrain's rmse: 82.6603\tval's rmse: 83.3354\n",
      "[1300]\ttrain's rmse: 82.4492\tval's rmse: 83.171\n",
      "[1400]\ttrain's rmse: 82.2307\tval's rmse: 82.9858\n",
      "[1500]\ttrain's rmse: 82.0361\tval's rmse: 82.8299\n",
      "[1600]\ttrain's rmse: 81.8514\tval's rmse: 82.6835\n",
      "[1700]\ttrain's rmse: 81.7023\tval's rmse: 82.5625\n",
      "[1800]\ttrain's rmse: 81.5408\tval's rmse: 82.4391\n",
      "[1900]\ttrain's rmse: 81.3929\tval's rmse: 82.332\n",
      "[2000]\ttrain's rmse: 81.2423\tval's rmse: 82.2236\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\ttrain's rmse: 81.2423\tval's rmse: 82.2236\n",
      "Evaluando modelo...\n",
      "Aplicando restricciones post-predicci√≥n...\n",
      "Predicciones originales - Min: -162.9596, Max: 1936.1211\n",
      "Predicciones negativas: 258393 (8.3%)\n",
      "Despu√©s del clip - Min: 0.0000, Max: 1936.1211\n",
      "\n",
      "M√©tricas del modelo:\n",
      "MAE: 30.2539\n",
      "RMSE: 89.8599\n",
      "R¬≤: 0.3487\n",
      "\n",
      "Top 15 features m√°s importantes:\n",
      "                   feature    importance\n",
      "33                   brand  3.312039e+11\n",
      "32                    cat2  1.092025e+11\n",
      "23               ma_tn_12m  9.661057e+10\n",
      "15     antiguedad_producto  4.140457e+10\n",
      "14      antiguedad_cliente  3.396043e+10\n",
      "11  promedio_historico_mes  2.375417e+10\n",
      "3                 ma_tn_6m  2.323515e+10\n",
      "6                 ma_tn_3m  1.680455e+10\n",
      "31                    cat1  1.650691e+10\n",
      "9    plan_precios_cuidados  1.424582e+10\n",
      "26        cust_request_qty  1.056862e+10\n",
      "20    stddev_historico_mes  1.029396e+10\n",
      "29               trimestre  1.005042e+10\n",
      "22         cust_request_tn  7.645818e+09\n",
      "7              delta_tn_1m  6.497705e+09\n",
      "\n",
      "Guardando modelo...\n",
      "Modelo guardado como '02_lgbm_model_v2.txt'\n",
      "Objetos auxiliares guardados como '02_lgbm_model_v2.pkl'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuraci√≥n\n",
    "PERIODO_CORTE = 201912  # Entrenar hasta este periodo\n",
    "TARGET_LAG = 2  # Predecir periodo +2\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"\n",
    "    Carga y prepara los datos para el modelo\n",
    "    \"\"\"\n",
    "    # Conectar a la base de datos DuckDB\n",
    "    con = duckdb.connect(database='../input/db/labo3.duckdb')\n",
    "    \n",
    "    # Cargar datos de la tabla ventas_features_final\n",
    "    query = \"\"\"SELECT \n",
    "        antiguedad_cliente,\n",
    "        antiguedad_producto,\n",
    "        cust_request_qty,\n",
    "        cust_request_tn,\n",
    "        customer_id,\n",
    "        periodo,\n",
    "        plan_precios_cuidados,\n",
    "        product_id,\n",
    "        registro_sintetico,\n",
    "        -- stock_final,\n",
    "        tn,\n",
    "        anio,\n",
    "        cliente_recurrente,\n",
    "        coef_variacion_6m,\n",
    "        crecimiento_consistente,\n",
    "        delta_request_1m,\n",
    "        delta_request_3m,\n",
    "        delta_request_6m,\n",
    "        delta_stock_1m,\n",
    "        delta_stock_3m,\n",
    "        delta_tn_12m,\n",
    "        delta_tn_1m,\n",
    "        delta_tn_3m,\n",
    "        delta_tn_6m,\n",
    "        es_invierno,\n",
    "        es_verano,\n",
    "        fill_rate,\n",
    "        is_max_tn_12m,\n",
    "        is_max_tn_3m,\n",
    "        is_max_tn_6m,\n",
    "        is_min_tn_12m,\n",
    "        is_min_tn_3m,\n",
    "        is_min_tn_6m,\n",
    "        ma_request_3m,\n",
    "        ma_request_6m,\n",
    "        ma_stock_3m,\n",
    "        ma_tn_12m,\n",
    "        ma_tn_3m,\n",
    "        ma_tn_6m,\n",
    "        mes,\n",
    "        pct_change_tn_1m,\n",
    "        pct_change_tn_3m,\n",
    "        pct_change_vs_mismo_mes_anio_anterior,\n",
    "        producto_maduro,\n",
    "        ratio_tn_vs_2m_ahead,\n",
    "        ratio_vs_ma_6m,\n",
    "        request_tn_lag_1m,\n",
    "        request_tn_lag_3m,\n",
    "        request_tn_lag_6m,\n",
    "        stock_lag_1m,\n",
    "        stock_lag_3m,\n",
    "        stock_turnover_ratio,\n",
    "        tendencia_6m,\n",
    "        tn_lag_12m,\n",
    "        tn_lag_1m,\n",
    "        tn_lag_3m,\n",
    "        tn_lag_6m,\n",
    "        tn_mismo_mes_anio_anterior,\n",
    "        trimestre,\n",
    "        volatilidad_tn_6m,\n",
    "        -- clientes_total_producto,\n",
    "        coef_variacion_estacional,\n",
    "        desviacion_vs_promedio_historico_mes,\n",
    "        es_cliente_principal_producto,\n",
    "        es_producto_principal_cliente,\n",
    "        indice_concentracion_producto,\n",
    "        indice_estacionalidad,\n",
    "        -- meses_desde_ultima_compra,\n",
    "        -- participacion_cliente_en_producto,\n",
    "        -- participacion_producto_en_cliente,\n",
    "        patron_ciclico_3m,\n",
    "        patron_ciclico_6m,\n",
    "        periodicidad_promedio_compras,\n",
    "        productos_total_cliente_periodo,\n",
    "        promedio_anual,\n",
    "        promedio_historico_mes,\n",
    "        ranking_cliente_en_producto,\n",
    "        ranking_producto_en_cliente,\n",
    "        ratio_vs_promedio_otros_productos_cliente,\n",
    "        stddev_historico_mes,\n",
    "        -- tn_total_cliente_periodo,\n",
    "        -- tn_total_producto_periodo,\n",
    "        cat1,\n",
    "        cat2,\n",
    "        -- cat3,\n",
    "        brand\n",
    "        -- sku_size\n",
    "    FROM ventas_features_final\"\"\"\n",
    "    df = con.execute(query).df()\n",
    "    \n",
    "    # Cerrar conexi√≥n\n",
    "    con.close()\n",
    "    print(f\"Datos originales: {df.shape}\")\n",
    "    print(f\"Periodos √∫nicos: {sorted(df['periodo'].unique())}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_target_distribution(df):\n",
    "    \"\"\"\n",
    "    Analiza la distribuci√≥n del target para entender el problema\n",
    "    \"\"\"\n",
    "    print(\"=== AN√ÅLISIS DEL TARGET ===\")\n",
    "    target = df['target_tn'].dropna()\n",
    "    \n",
    "    print(f\"Target stats:\")\n",
    "    print(f\"  Count: {len(target):,}\")\n",
    "    print(f\"  Min: {target.min():.4f}\")\n",
    "    print(f\"  Max: {target.max():.4f}\")\n",
    "    print(f\"  Mean: {target.mean():.4f}\")\n",
    "    print(f\"  Median: {target.median():.4f}\")\n",
    "    print(f\"  Std: {target.std():.4f}\")\n",
    "    \n",
    "    # Percentiles\n",
    "    percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]\n",
    "    print(f\"\\nPercentiles:\")\n",
    "    for p in percentiles:\n",
    "        print(f\"  {p}%: {np.percentile(target, p):.4f}\")\n",
    "    \n",
    "    # Valores negativos y ceros\n",
    "    negative_count = (target < 0).sum()\n",
    "    zero_count = (target == 0).sum()\n",
    "    positive_count = (target > 0).sum()\n",
    "    \n",
    "    print(f\"\\nDistribuci√≥n de valores:\")\n",
    "    print(f\"  Negativos: {negative_count:,} ({negative_count/len(target)*100:.1f}%)\")\n",
    "    print(f\"  Ceros: {zero_count:,} ({zero_count/len(target)*100:.1f}%)\")\n",
    "    print(f\"  Positivos: {positive_count:,} ({positive_count/len(target)*100:.1f}%)\")\n",
    "    \n",
    "    return target\n",
    "def identify_problematic_features(train_data, target_col='target_tn'):\n",
    "    \"\"\"\n",
    "    Identifica variables problem√°ticas que pueden causar overfitting\n",
    "    \"\"\"\n",
    "    print(\"=== IDENTIFICACI√ìN DE VARIABLES PROBLEM√ÅTICAS ===\")\n",
    "    \n",
    "    # 1. Variables que pueden ser data leakage\n",
    "    potential_leaks = []\n",
    "    suspicious_keywords = ['total', 'sum', 'global', 'all', 'periodo_total']\n",
    "    \n",
    "    for col in train_data.columns:\n",
    "        for keyword in suspicious_keywords:\n",
    "            if keyword in col.lower():\n",
    "                potential_leaks.append(col)\n",
    "                break\n",
    "    \n",
    "    print(f\"Posibles data leaks ({len(potential_leaks)}): {potential_leaks}\")\n",
    "    \n",
    "    # 2. Variables con valores extremos\n",
    "    numeric_cols = train_data.select_dtypes(include=[np.number]).columns\n",
    "    extreme_vars = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col != target_col and col in train_data.columns:\n",
    "            q99 = train_data[col].quantile(0.99)\n",
    "            q01 = train_data[col].quantile(0.01)\n",
    "            range_ratio = q99 / (q01 + 1e-8)  # Evitar divisi√≥n por 0\n",
    "            \n",
    "            if range_ratio > 1000:  # Rango muy amplio\n",
    "                extreme_vars.append((col, range_ratio))\n",
    "    \n",
    "    extreme_vars.sort(key=lambda x: x[1], reverse=True)\n",
    "    print(f\"\\nVariables con rangos extremos (top 10):\")\n",
    "    for col, ratio in extreme_vars[:10]:\n",
    "        print(f\"  {col}: ratio={ratio:.1f}\")\n",
    "    \n",
    "    # 3. Variables altamente correlacionadas con el target\n",
    "    target_corr = []\n",
    "    for col in numeric_cols:\n",
    "        if col != target_col and col in train_data.columns:\n",
    "            corr = train_data[col].corr(train_data[target_col])\n",
    "            if not np.isnan(corr) and abs(corr) > 0.8:\n",
    "                target_corr.append((col, corr))\n",
    "    \n",
    "    target_corr.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    print(f\"\\nVariables muy correlacionadas con target (>0.8):\")\n",
    "    for col, corr in target_corr:\n",
    "        print(f\"  {col}: corr={corr:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'potential_leaks': potential_leaks,\n",
    "        'extreme_vars': [x[0] for x in extreme_vars],\n",
    "        'high_corr': [x[0] for x in target_corr]\n",
    "    }\n",
    "\n",
    "def create_feature_exclusion_list(problematic_features):\n",
    "    \"\"\"\n",
    "    Crea lista de variables a excluir\n",
    "    \"\"\"\n",
    "    print(\"=== CREANDO LISTA DE EXCLUSI√ìN ===\")\n",
    "    \n",
    "    # Variables definitivamente problem√°ticas\n",
    "    definite_excludes = [\n",
    "        'tn_total_cliente_periodo',\n",
    "        'tn_total_producto_periodo', \n",
    "        'clientes_total_producto',\n",
    "        'productos_total_cliente_periodo'\n",
    "    ]\n",
    "\n",
    "    # Variables importantes que NUNCA debemos excluir\n",
    "    keep_always = [\n",
    "        'antiguedad_cliente',\n",
    "        'antiguedad_producto',\n",
    "        #'stock_final',\n",
    "        'fill_rate',\n",
    "        'cust_request_qty',\n",
    "        'cust_request_tn'\n",
    "    ]\n",
    "    \n",
    "    auto_excludes = []\n",
    "\n",
    "    # Filtrar potential_leaks excluyendo las importantes\n",
    "    filtered_leaks = [var for var in problematic_features['potential_leaks'] \n",
    "                      if var not in keep_always]\n",
    "    auto_excludes.extend(filtered_leaks)\n",
    "    \n",
    "    # Solo las m√°s extremas de las variables con rangos amplios (pero filtradas)\n",
    "    filtered_extreme = [var for var in problematic_features['extreme_vars'][:5] \n",
    "                        if var not in keep_always]\n",
    "    auto_excludes.extend(filtered_extreme)\n",
    "    \n",
    "    # Variables muy correlacionadas (posible leakage) - pero filtradas\n",
    "    filtered_corr = [var for var in problematic_features['high_corr'] \n",
    "                     if var not in keep_always]\n",
    "    auto_excludes.extend(filtered_corr)\n",
    "    \n",
    "    # Remover duplicados\n",
    "    all_excludes = list(set(definite_excludes + auto_excludes))\n",
    "    \n",
    "    print(f\"Variables a excluir ({len(all_excludes)}):\")\n",
    "    for var in sorted(all_excludes):\n",
    "        print(f\"  - {var}\")\n",
    "    \n",
    "    return all_excludes\n",
    "\n",
    "def robust_preprocessing(X_train, X_test, numeric_vars, extreme_threshold=1000):\n",
    "    \"\"\"\n",
    "    Preprocesamiento robusto usando RobustScaler y transformaciones\n",
    "    \"\"\"\n",
    "    print(\"=== PREPROCESAMIENTO ROBUSTO ===\")\n",
    "    \n",
    "    X_train_processed = X_train.copy()\n",
    "    X_test_processed = X_test.copy()\n",
    "    \n",
    "    # Identificar columnas num√©ricas v√°lidas\n",
    "    numeric_cols_present = [col for col in numeric_vars if col in X_train.columns]\n",
    "    \n",
    "    transformers = {}\n",
    "    \n",
    "    for col in numeric_cols_present:\n",
    "        if X_train_processed[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "            \n",
    "            # 1. Manejar valores extremos\n",
    "            q99 = X_train_processed[col].quantile(0.99)\n",
    "            q01 = X_train_processed[col].quantile(0.01)\n",
    "            \n",
    "            # Winsorizaci√≥n (clip outliers)\n",
    "            X_train_processed[col] = X_train_processed[col].clip(q01, q99)\n",
    "            X_test_processed[col] = X_test_processed[col].clip(q01, q99)\n",
    "            \n",
    "            # 2. Verificar si necesita transformaci√≥n adicional\n",
    "            col_range = q99 - q01\n",
    "            col_std = X_train_processed[col].std()\n",
    "            \n",
    "            if col_range > extreme_threshold or col_std > extreme_threshold:\n",
    "                print(f\"  Aplicando transformaci√≥n robusta a: {col}\")\n",
    "                \n",
    "                # Usar RobustScaler (menos sensible a outliers)\n",
    "                scaler = RobustScaler()\n",
    "                X_train_processed[col] = scaler.fit_transform(X_train_processed[col].values.reshape(-1, 1)).flatten()\n",
    "                X_test_processed[col] = scaler.transform(X_test_processed[col].values.reshape(-1, 1)).flatten()\n",
    "                \n",
    "                transformers[col] = scaler\n",
    "            else:\n",
    "                # StandardScaler normal para variables bien comportadas\n",
    "                from sklearn.preprocessing import StandardScaler\n",
    "                scaler = StandardScaler()\n",
    "                X_train_processed[col] = scaler.fit_transform(X_train_processed[col].values.reshape(-1, 1)).flatten()\n",
    "                X_test_processed[col] = scaler.transform(X_test_processed[col].values.reshape(-1, 1)).flatten()\n",
    "                \n",
    "                transformers[col] = scaler\n",
    "    \n",
    "    print(f\"Procesadas {len(transformers)} variables num√©ricas\")\n",
    "    \n",
    "    return X_train_processed, X_test_processed, transformers\n",
    "\n",
    "def debug_feature_selection(train_data, feature_cols):\n",
    "    \"\"\"\n",
    "    Debuggea qu√© variables se est√°n incluyendo incorrectamente\n",
    "    \"\"\"\n",
    "    print(\"=== DEBUG DE SELECCI√ìN DE FEATURES ===\")\n",
    "    \n",
    "    # Variables que DEFINITIVAMENTE no deber√≠an estar\n",
    "    forbidden_vars = ['product_id', 'customer_id', 'periodo', 'target_tn', 'tn']\n",
    "    forbidden_found = []\n",
    "    \n",
    "    for var in forbidden_vars:\n",
    "        if var in feature_cols:\n",
    "            forbidden_found.append(var)\n",
    "    \n",
    "    if forbidden_found:\n",
    "        print(f\"üö® ERROR: Variables prohibidas encontradas en features:\")\n",
    "        for var in forbidden_found:\n",
    "            print(f\"  - {var}\")\n",
    "    \n",
    "    # Variables con nombres sospechosos\n",
    "    suspicious_vars = []\n",
    "    for var in feature_cols:\n",
    "        if any(keyword in var.lower() for keyword in ['total', 'id', 'key_']):\n",
    "            suspicious_vars.append(var)\n",
    "    \n",
    "    if suspicious_vars:\n",
    "        print(f\"‚ö†Ô∏è  Variables sospechosas en features:\")\n",
    "        for var in suspicious_vars:\n",
    "            print(f\"  - {var}\")\n",
    "    \n",
    "    print(f\"\\nTotal features seleccionadas: {len(feature_cols)}\")\n",
    "    print(f\"Primeras 10 features: {feature_cols[:10]}\")\n",
    "    \n",
    "    return forbidden_found, suspicious_vars\n",
    "def prepare_features_fixed(df):\n",
    "    \"\"\"\n",
    "    Funci√≥n de preparaci√≥n de features con exclusi√≥n FORZADA\n",
    "    \"\"\"\n",
    "    print(\"Preparando features (versi√≥n corregida)...\")\n",
    "    \n",
    "    # 1. EXCLUSI√ìN FORZADA DE IDENTIFICADORES\n",
    "    forced_excludes = [\n",
    "        'product_id', 'customer_id', 'periodo',  # IDs\n",
    "        'target_tn', 'tn',  # Target y variable original\n",
    "        'key_customer_producto_periodo',  # Keys\n",
    "        'key_periodo_customer_producto',\n",
    "        'key_periodo_producto',\n",
    "        'PVC', 'PVP', 'UVC', 'UVP', 'periodo_fecha'  # Fechas\n",
    "    ]\n",
    "    \n",
    "    # 2. EXCLUSI√ìN DE VARIABLES PROBLEM√ÅTICAS\n",
    "    problematic_excludes = [\n",
    "        'tn_total_cliente_periodo',\n",
    "        'tn_total_producto_periodo', \n",
    "        'clientes_total_producto',\n",
    "        'productos_total_cliente_periodo'\n",
    "    ]\n",
    "    \n",
    "    # 3. Variables categ√≥ricas\n",
    "    categorical_vars = ['cat1', 'cat2', 'cat3', 'brand']\n",
    "    \n",
    "    # 4. Combinar todas las exclusiones\n",
    "    all_excludes = forced_excludes + problematic_excludes\n",
    "    \n",
    "    # 5. Variables num√©ricas = todo lo dem√°s\n",
    "    all_vars = set(df.columns)\n",
    "    excluded_set = set(all_excludes)\n",
    "    categorical_set = set(categorical_vars)\n",
    "    \n",
    "    numeric_vars = list(all_vars - excluded_set - categorical_set)\n",
    "    \n",
    "    # 6. Verificar que no hay contaminaci√≥n\n",
    "    contaminated = []\n",
    "    for var in numeric_vars:\n",
    "        if any(forbidden in var.lower() for forbidden in ['product_id', 'customer_id', 'periodo']):\n",
    "            contaminated.append(var)\n",
    "    \n",
    "    if contaminated:\n",
    "        print(f\"üö® ELIMINANDO variables contaminadas: {contaminated}\")\n",
    "        numeric_vars = [var for var in numeric_vars if var not in contaminated]\n",
    "    \n",
    "    print(f\"Variables categ√≥ricas ({len(categorical_vars)}): {categorical_vars}\")\n",
    "    print(f\"Variables num√©ricas ({len(numeric_vars)})\")\n",
    "    print(f\"Variables excluidas ({len(all_excludes)})\")\n",
    "    print(f\"VERIFICACI√ìN: product_id excluido = {'product_id' not in numeric_vars}\")\n",
    "    print(f\"VERIFICACI√ìN: customer_id excluido = {'customer_id' not in numeric_vars}\")\n",
    "    print(f\"VERIFICACI√ìN: periodo excluido = {'periodo' not in numeric_vars}\")\n",
    "    \n",
    "    return numeric_vars, categorical_vars, all_excludes\n",
    "\n",
    "\n",
    "\n",
    "def prepare_features_improved(df):\n",
    "    \"\"\"\n",
    "    Versi√≥n mejorada de prepare_features que excluye variables problem√°ticas\n",
    "    \"\"\"\n",
    "    print(\"Preparando features (versi√≥n mejorada)...\")\n",
    "    \n",
    "    # 1. Identificar variables problem√°ticas\n",
    "    problematic = identify_problematic_features(df)\n",
    "    \n",
    "    # 2. Crear lista de exclusi√≥n\n",
    "    auto_excludes = create_feature_exclusion_list(problematic)\n",
    "    \n",
    "    # 3. Variables a excluir (originales + autom√°ticas)\n",
    "    exclude_vars = [col for col in df.columns if col.startswith('key_')]\n",
    "    exclude_vars.extend(['target_tn', 'tn'])\n",
    "    \n",
    "    # Excluir variables de fecha\n",
    "    date_vars = ['PVC', 'PVP', 'UVC', 'UVP', 'periodo_fecha']\n",
    "    exclude_vars.extend([col for col in date_vars if col in df.columns])\n",
    "\n",
    "    id_vars = ['product_id', 'customer_id', 'periodo']\n",
    "    exclude_vars.extend([col for col in id_vars if col in df.columns])\n",
    "    \n",
    "    # Agregar exclusiones autom√°ticas\n",
    "    exclude_vars.extend(auto_excludes)\n",
    "    \n",
    "    # Remover duplicados\n",
    "    exclude_vars = list(set(exclude_vars))\n",
    "    \n",
    "    # Variables categ√≥ricas\n",
    "    categorical_vars = ['cat1', 'cat2', 'cat3', 'brand']\n",
    "    \n",
    "    # Variables num√©ricas\n",
    "    all_vars = set(df.columns)\n",
    "    numeric_vars = list(all_vars - set(exclude_vars) - set(categorical_vars))\n",
    "    \n",
    "    print(f\"Variables categ√≥ricas ({len(categorical_vars)}): {categorical_vars}\")\n",
    "    print(f\"Variables num√©ricas ({len(numeric_vars)}): {numeric_vars[:5]}...\")\n",
    "    print(f\"Variables excluidas ({len(exclude_vars)}): {auto_excludes[:5]}...\")\n",
    "    \n",
    "    return numeric_vars, categorical_vars, exclude_vars\n",
    "\n",
    "def apply_same_preprocessing(X_new, transformers, numeric_vars):\n",
    "    \"\"\"\n",
    "    Aplica las mismas transformaciones del entrenamiento a nuevos datos\n",
    "    \"\"\"\n",
    "    X_processed = X_new.copy()\n",
    "    \n",
    "    for col in numeric_vars:\n",
    "        if col in transformers and col in X_processed.columns:\n",
    "            if X_processed[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "                # Aplicar la misma transformaci√≥n que se us√≥ en entrenamiento\n",
    "                scaler = transformers[col]\n",
    "                X_processed[col] = scaler.transform(X_processed[col].values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    return X_processed\n",
    "\n",
    "def apply_post_prediction_constraints(predictions):\n",
    "    \"\"\"\n",
    "    Aplica restricciones post-predicci√≥n para asegurar valores v√°lidos\n",
    "    \"\"\"\n",
    "    print(\"Aplicando restricciones post-predicci√≥n...\")\n",
    "    \n",
    "    original_min = predictions.min()\n",
    "    original_max = predictions.max()\n",
    "    negative_count = (predictions < 0).sum()\n",
    "    \n",
    "    print(f\"Predicciones originales - Min: {original_min:.4f}, Max: {original_max:.4f}\")\n",
    "    print(f\"Predicciones negativas: {negative_count} ({negative_count/len(predictions)*100:.1f}%)\")\n",
    "    \n",
    "    # Clip a 0 (evita valores negativos)\n",
    "    predictions_clipped = np.maximum(predictions, 0)\n",
    "    \n",
    "    print(f\"Despu√©s del clip - Min: {predictions_clipped.min():.4f}, Max: {predictions_clipped.max():.4f}\")\n",
    "    \n",
    "    return predictions_clipped\n",
    "\n",
    "\n",
    "\n",
    "def create_target_variable(df):\n",
    "    \"\"\"\n",
    "    Crea la variable objetivo (tn en periodo +2) agrupada por product_id\n",
    "    \"\"\"\n",
    "    print(\"Creando variable objetivo...\")\n",
    "    \n",
    "    # Agregar tn por product_id y periodo\n",
    "    df_agg = df.groupby(['product_id', 'periodo']).agg({\n",
    "        'tn': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Ordenar por product_id y periodo\n",
    "    df_agg = df_agg.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    # Crear target variable (tn en periodo +2)\n",
    "    df_agg['target_tn'] = df_agg.groupby('product_id')['tn'].shift(-TARGET_LAG)\n",
    "    \n",
    "    # Merge de vuelta con el dataset original\n",
    "    df = df.merge(df_agg[['product_id', 'periodo', 'target_tn']], \n",
    "                  on=['product_id', 'periodo'], how='left')\n",
    "    \n",
    "    print(f\"Registros con target v√°lido: {df['target_tn'].notna().sum()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_features(df):\n",
    "    \"\"\"\n",
    "    Prepara las features excluyendo variables key_*, fechas y separando categ√≥ricas\n",
    "    \"\"\"\n",
    "    print(\"Preparando features...\")\n",
    "    \n",
    "    # Variables a excluir\n",
    "    exclude_vars = [col for col in df.columns if col.startswith('key_')]\n",
    "    exclude_vars.extend(['target_tn', 'tn'])  # Excluir tambi√©n el target y la variable original\n",
    "    \n",
    "    # Excluir variables de fecha que no son √∫tiles como features num√©ricas\n",
    "    date_vars = ['PVC', 'PVP', 'UVC', 'UVP', 'periodo_fecha']\n",
    "    exclude_vars.extend([col for col in date_vars if col in df.columns])\n",
    "    \n",
    "    # Variables categ√≥ricas\n",
    "    categorical_vars = ['cat1', 'cat2', 'cat3', 'brand']\n",
    "    \n",
    "    # Variables num√©ricas (todas las dem√°s excepto las excluidas)\n",
    "    all_vars = set(df.columns)\n",
    "    numeric_vars = list(all_vars - set(exclude_vars) - set(categorical_vars))\n",
    "    \n",
    "    print(f\"Variables categ√≥ricas ({len(categorical_vars)}): {categorical_vars}\")\n",
    "    print(f\"Variables num√©ricas ({len(numeric_vars)}): {numeric_vars[:10]}...\")  # Mostrar solo las primeras 10\n",
    "    print(f\"Variables excluidas ({len(exclude_vars)}): {exclude_vars}\")\n",
    "    \n",
    "    return numeric_vars, categorical_vars, exclude_vars\n",
    "\n",
    "def encode_categorical_variables(df, categorical_vars):\n",
    "    \"\"\"\n",
    "    Codifica variables categ√≥ricas usando LabelEncoder\n",
    "    \"\"\"\n",
    "    print(\"Codificando variables categ√≥ricas...\")\n",
    "    \n",
    "    label_encoders = {}\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    for col in categorical_vars:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            # Manejar valores nulos\n",
    "            df_encoded[col] = df_encoded[col].fillna('UNKNOWN')\n",
    "            df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "            print(f\"  {col}: {len(le.classes_)} categor√≠as √∫nicas\")\n",
    "    \n",
    "    return df_encoded, label_encoders\n",
    "\n",
    "def clean_numeric_data(df):\n",
    "    \"\"\"\n",
    "    Limpia y convierte datos num√©ricos para evitar problemas\n",
    "    \"\"\"\n",
    "    print(\"Limpiando datos num√©ricos...\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convertir plan_precios_cuidados de bool a int\n",
    "    if 'plan_precios_cuidados' in df_clean.columns:\n",
    "        df_clean['plan_precios_cuidados'] = df_clean['plan_precios_cuidados'].astype(int)\n",
    "        print(\"  Convertido plan_precios_cuidados de bool a int\")\n",
    "    \n",
    "    # Identificar columnas num√©ricas\n",
    "    numeric_columns = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        # Reemplazar infinitos por NaN\n",
    "        df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Convertir float64 a float32 para ahorrar memoria y evitar problemas de precisi√≥n\n",
    "        if df_clean[col].dtype == 'float64':\n",
    "            df_clean[col] = df_clean[col].astype('float32')\n",
    "    \n",
    "    print(f\"  Limpiadas {len(numeric_columns)} columnas num√©ricas\")\n",
    "    print(\"  Convertidos float64 -> float32\")\n",
    "    print(\"  Reemplazados valores infinitos por NaN\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def standardize_numeric_features(X_train, X_test, numeric_vars):\n",
    "    \"\"\"\n",
    "    Estandariza variables num√©ricas\n",
    "    \"\"\"\n",
    "    print(\"Estandarizando variables num√©ricas...\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Copiar los datasets\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    \n",
    "    # Manejar valores nulos antes de estandarizar\n",
    "    X_train_scaled = X_train_scaled.fillna(0)\n",
    "    X_test_scaled = X_test_scaled.fillna(0)\n",
    "    \n",
    "    # Identificar columnas num√©ricas que existen en los datos\n",
    "    numeric_cols_present = [col for col in numeric_vars if col in X_train.columns]\n",
    "    \n",
    "    # Verificar que las columnas sean realmente num√©ricas y no tengan problemas\n",
    "    numeric_cols_valid = []\n",
    "    for col in numeric_cols_present:\n",
    "        if X_train_scaled[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "            # Verificar que no haya infinitos o valores muy grandes\n",
    "            if not (np.isinf(X_train_scaled[col]).any() or np.isinf(X_test_scaled[col]).any()):\n",
    "                # Verificar que no haya valores extremadamente grandes\n",
    "                max_val = max(X_train_scaled[col].abs().max(), X_test_scaled[col].abs().max())\n",
    "                if max_val < 1e10:  # L√≠mite razonable\n",
    "                    numeric_cols_valid.append(col)\n",
    "                else:\n",
    "                    print(f\"  Saltando {col}: valores muy grandes (max: {max_val:.2e})\")\n",
    "            else:\n",
    "                print(f\"  Saltando {col}: contiene valores infinitos\")\n",
    "        else:\n",
    "            print(f\"  Saltando columna no num√©rica: {col} (tipo: {X_train_scaled[col].dtype})\")\n",
    "    \n",
    "    if numeric_cols_valid:\n",
    "        # Estandarizar solo las columnas num√©ricas v√°lidas\n",
    "        X_train_scaled[numeric_cols_valid] = scaler.fit_transform(X_train_scaled[numeric_cols_valid])\n",
    "        X_test_scaled[numeric_cols_valid] = scaler.transform(X_test_scaled[numeric_cols_valid])\n",
    "        print(f\"Estandarizadas {len(numeric_cols_valid)} variables num√©ricas\")\n",
    "    else:\n",
    "        print(\"No se encontraron variables num√©ricas v√°lidas para estandarizar\")\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "def train_lgbm_model(X_train, y_train, X_val, y_val, categorical_features):\n",
    "    \"\"\"\n",
    "    Entrena modelo LightGBM con par√°metros mejorados\n",
    "    \"\"\"\n",
    "    print(\"Entrenando modelo LightGBM mejorado...\")\n",
    "    \n",
    "    # Par√°metros mejorados\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 50,  # Aumentado\n",
    "        'learning_rate': 0.03,  # Reducido\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'min_data_in_leaf': 100,  # NUEVO - evita overfitting\n",
    "        'lambda_l1': 0.1,  # NUEVO - regularizaci√≥n\n",
    "        'lambda_l2': 0.1,  # NUEVO - regularizaci√≥n\n",
    "        'verbose': -1,\n",
    "        'random_state': 42,\n",
    "        'force_col_wise': True\n",
    "    }\n",
    "    \n",
    "    # Crear datasets de LightGBM\n",
    "    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=categorical_features, reference=train_data)\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'val'],\n",
    "        num_boost_round=2000,  # Aumentado\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=200), lgb.log_evaluation(period=100)]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Eval√∫a el modelo con mejoras\n",
    "    \"\"\"\n",
    "    print(\"Evaluando modelo...\")\n",
    "    \n",
    "    # Predicciones base\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Aplicar restricciones (clip negativo a 0)\n",
    "    y_pred_final = apply_post_prediction_constraints(y_pred)\n",
    "    \n",
    "    # M√©tricas\n",
    "    mae = mean_absolute_error(y_test, y_pred_final)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_final))\n",
    "    r2 = r2_score(y_test, y_pred_final)\n",
    "    \n",
    "    print(f\"\\nM√©tricas del modelo:\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R¬≤: {r2:.4f}\")\n",
    "    \n",
    "    return {'mae': mae, 'rmse': rmse, 'r2': r2, 'predictions': y_pred_final}\n",
    "\n",
    "def get_feature_importance(model, feature_names):\n",
    "    \"\"\"\n",
    "    Obtiene importancia de features\n",
    "    \"\"\"\n",
    "    importance = model.feature_importance(importance_type='gain')\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 features m√°s importantes:\")\n",
    "    print(feature_imp.head(15))\n",
    "    \n",
    "    return feature_imp\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Funci√≥n principal\n",
    "    \"\"\"\n",
    "    print(\"=== MODELO LGBM PARA PREDICCI√ìN DE TONELADAS ===\\n\")\n",
    "    \n",
    "    # 1. Cargar datos\n",
    "    df = load_and_prepare_data()\n",
    "    \n",
    "    # 2. Crear variable objetivo\n",
    "    df = create_target_variable(df)\n",
    "    \n",
    "    # 2.5. Limpiar datos num√©ricos (nueva funci√≥n)\n",
    "    df = clean_numeric_data(df)\n",
    "    \n",
    "    # 3. Filtrar datos para entrenamiento (hasta PERIODO_CORTE)\n",
    "    train_data = df[df['periodo'] <= PERIODO_CORTE].copy()\n",
    "    print(f\"\\nDatos de entrenamiento: {train_data.shape}\")\n",
    "    \n",
    "    # 4. Remover registros sin target\n",
    "    train_data = train_data.dropna(subset=['target_tn'])\n",
    "    print(f\"Datos despu√©s de remover NaN en target: {train_data.shape}\")\n",
    "    \n",
    "    # 4.5. Analizar distribuci√≥n del target ‚Üê NUEVO\n",
    "    target_analysis = analyze_target_distribution(train_data)\n",
    "\n",
    "    # 5. Preparar features\n",
    "    numeric_vars, categorical_vars, exclude_vars = prepare_features_fixed(train_data)\n",
    "    #numeric_vars, categorical_vars, exclude_vars = prepare_features(train_data)\n",
    "    #numeric_vars, categorical_vars, exclude_vars = prepare_features_improved(train_data)\n",
    "    \n",
    "    # 6. Codificar variables categ√≥ricas\n",
    "    train_data_encoded, label_encoders = encode_categorical_variables(train_data, categorical_vars)\n",
    "    \n",
    "    # 7. Seleccionar features finales\n",
    "    feature_cols = numeric_vars + categorical_vars\n",
    "    feature_cols = [col for col in feature_cols if col in train_data_encoded.columns]\n",
    "    \n",
    "    print(f\"\\nFeatures finales: {len(feature_cols)}\")\n",
    "    \n",
    "    # 8. Preparar X y y\n",
    "    forbidden_found, suspicious_vars = debug_feature_selection(train_data, feature_cols)\n",
    "\n",
    "       \n",
    "    if forbidden_found:\n",
    "        print(\"üõë DETENIENDO: Variables prohibidas detectadas\")\n",
    "        return None\n",
    "\n",
    "    X = train_data_encoded[feature_cols].copy()\n",
    "    y = train_data_encoded['target_tn'].copy()\n",
    "    \n",
    "    # Manejar valores nulos en X\n",
    "    X = X.fillna(0)\n",
    "    \n",
    "    print(f\"Shape final - X: {X.shape}, y: {y.shape}\")\n",
    "    \n",
    "    # 9. Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=None\n",
    "    )\n",
    "    \n",
    "    # 10. Split train/validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    # 11. Estandarizar variables num√©ricas\n",
    "    X_train_scaled, X_val_scaled, scaler = standardize_numeric_features(X_train, X_val, numeric_vars)\n",
    "    #X_train_scaled, X_val_scaled, transformers = robust_preprocessing(X_train, X_val, numeric_vars)\n",
    "    X_test_scaled, _, _ = standardize_numeric_features(X_test, X_test, numeric_vars)\n",
    "    #X_test_scaled = apply_same_preprocessing(X_test, transformers, numeric_vars)\n",
    "    \n",
    "    # 12. Identificar features categ√≥ricas para LightGBM\n",
    "    categorical_indices = [i for i, col in enumerate(feature_cols) if col in categorical_vars]\n",
    "    \n",
    "    # 13. Entrenar modelo\n",
    "    model = train_lgbm_model(\n",
    "        X_train_scaled, y_train, \n",
    "        X_val_scaled, y_val, \n",
    "        categorical_indices\n",
    "    )\n",
    "    \n",
    "    # 14. Evaluar modelo\n",
    "    results = evaluate_model(model, X_test_scaled, y_test)\n",
    "    \n",
    "    # 15. Feature importance\n",
    "    feature_importance = get_feature_importance(model, feature_cols)\n",
    "    \n",
    "    # 16. Guardar modelo y objetos necesarios\n",
    "    print(\"\\nGuardando modelo...\")\n",
    "    model.save_model('../output/lgbm/02_lgbm_model_v2.txt')\n",
    "    \n",
    "    # Guardar escalador y encoders\n",
    "    import pickle\n",
    "    with open('../output/lgbm/02_lgbm_model_v2.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            #'transformers': transformers,\n",
    "            'scaler': scaler,\n",
    "            'label_encoders': label_encoders,\n",
    "            'feature_cols': feature_cols,\n",
    "            'categorical_vars': categorical_vars,\n",
    "            'numeric_vars': numeric_vars\n",
    "        }, f)\n",
    "    \n",
    "    print(\"Modelo guardado como '02_lgbm_model_v2.txt'\")\n",
    "    print(\"Objetos auxiliares guardados como '02_lgbm_model_v2.pkl'\")\n",
    "    \n",
    "    return model, results, feature_importance\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, results, feature_importance = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14e3983e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREDICCI√ìN PARA PRODUCTOS ESPEC√çFICOS ===\n",
      "\n",
      "Cargando productos a predecir...\n",
      "Productos a predecir: 780\n",
      "Product_ids √∫nicos: 780\n",
      "Cargando modelo y objetos auxiliares...\n",
      "Cargando datos m√°s recientes para predicci√≥n...\n",
      "Datos del periodo 201912: (553419, 98)\n",
      "Preparando datos para predicci√≥n...\n",
      "Datos para predicci√≥n (todos los productos): (553419, 98)\n",
      "Limpiando datos num√©ricos para predicci√≥n...\n",
      "  Limpiadas 85 columnas num√©ricas\n",
      "Generando predicciones mejoradas para todos los productos...\n",
      "Aplicando restricciones post-predicci√≥n...\n",
      "Predicciones originales - Min: -363.2131, Max: 1435.9539\n",
      "Predicciones negativas: 59310 (10.7%)\n",
      "Despu√©s del clip - Min: 0.0000, Max: 1435.9539\n",
      "Agrupando predicciones por product_id...\n",
      "Predicciones generadas para 927 productos en total\n",
      "\n",
      "Resultados finales (filtrados):\n",
      "Productos en tb_productos_a_predecir: 780\n",
      "Productos con predicci√≥n disponible: 780\n",
      "Total tn predichas (solo productos solicitados): 17512233.59\n",
      "Promedio tn por producto: 22451.58\n",
      "\n",
      "Top 10 productos solicitados con mayor predicci√≥n:\n",
      "    product_id             tn\n",
      "13       20014  543049.798533\n",
      "0        20001  501716.162067\n",
      "3        20004  444297.116065\n",
      "14       20015  414491.139371\n",
      "2        20003  412461.722533\n",
      "4        20005  315909.280767\n",
      "1        20002  313192.421999\n",
      "8        20009  241061.887738\n",
      "6        20007  234385.376849\n",
      "12       20013  217223.857142\n",
      "\n",
      "Predicciones finales enviadas: 780 productos\n",
      "Predicciones guardadas en: ../output/lgbm/02_lgbm_model_v2.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CHUNK DE PREDICCI√ìN PARA PRODUCTOS ESPEC√çFICOS\n",
    "# =============================================================================\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_productos_a_predecir():\n",
    "    \"\"\"\n",
    "    Carga la lista de productos a predecir desde DuckDB\n",
    "    \"\"\"\n",
    "    print(\"Cargando productos a predecir...\")\n",
    "    \n",
    "    # Conectar a la base de datos DuckDB\n",
    "    con = duckdb.connect(database='../input/db/labo3.duckdb')\n",
    "    \n",
    "    # Cargar productos a predecir\n",
    "    query = \"SELECT * FROM tb_productos_a_predecir\"\n",
    "    productos_df = con.execute(query).df()\n",
    "    \n",
    "    con.close()\n",
    "    \n",
    "    print(f\"Productos a predecir: {len(productos_df)}\")\n",
    "    print(f\"Product_ids √∫nicos: {productos_df['product_id'].nunique()}\")\n",
    "    \n",
    "    return productos_df\n",
    "\n",
    "def load_latest_data_for_prediction():\n",
    "    \"\"\"\n",
    "    Carga los datos m√°s recientes para hacer predicciones\n",
    "    \"\"\"\n",
    "    print(\"Cargando datos m√°s recientes para predicci√≥n...\")\n",
    "    \n",
    "    # Conectar a la base de datos DuckDB\n",
    "    con = duckdb.connect(database='../input/db/labo3.duckdb')\n",
    "    \n",
    "    # Cargar datos del periodo m√°s reciente (201912 para predecir 202002)\n",
    "    # Necesitamos datos del periodo 201912 para predecir periodo +2 (202002)\n",
    "    query = \"\"\"\n",
    "    SELECT * \n",
    "    FROM ventas_features_final \n",
    "    WHERE periodo = 201912\n",
    "    \"\"\"\n",
    "    df = con.execute(query).df()\n",
    "    \n",
    "    con.close()\n",
    "    \n",
    "    print(f\"Datos del periodo 201912: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_model_and_objects():\n",
    "    \"\"\"\n",
    "    Carga el modelo entrenado y objetos auxiliares\n",
    "    \"\"\"\n",
    "    print(\"Cargando modelo y objetos auxiliares...\")\n",
    "    \n",
    "    # Cargar modelo LightGBM\n",
    "    model = lgb.Booster(model_file='../output/lgbm/02_lgbm_model_v2.txt')\n",
    "    \n",
    "    # Cargar objetos auxiliares\n",
    "    with open('../output/lgbm/02_lgbm_model_v2.pkl', 'rb') as f:\n",
    "        objects = pickle.load(f)\n",
    "    \n",
    "    return model, objects\n",
    "\n",
    "def clean_numeric_data_prediction(df):\n",
    "    \"\"\"\n",
    "    Aplica la misma limpieza de datos num√©ricos que en entrenamiento\n",
    "    \"\"\"\n",
    "    print(\"Limpiando datos num√©ricos para predicci√≥n...\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convertir plan_precios_cuidados de bool a int\n",
    "    if 'plan_precios_cuidados' in df_clean.columns:\n",
    "        df_clean['plan_precios_cuidados'] = df_clean['plan_precios_cuidados'].astype(int)\n",
    "    \n",
    "    # Identificar columnas num√©ricas\n",
    "    numeric_columns = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        # Reemplazar infinitos por NaN\n",
    "        df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Convertir float64 a float32\n",
    "        if df_clean[col].dtype == 'float64':\n",
    "            df_clean[col] = df_clean[col].astype('float32')\n",
    "    \n",
    "    print(f\"  Limpiadas {len(numeric_columns)} columnas num√©ricas\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def prepare_prediction_data(df, objects):\n",
    "    \"\"\"\n",
    "    Prepara datos para predicci√≥n usando los mismos pasos del entrenamiento\n",
    "    PREDICE PARA TODOS LOS PRODUCTOS (no filtra)\n",
    "    \"\"\"\n",
    "    print(\"Preparando datos para predicci√≥n...\")\n",
    "    \n",
    "    # NO filtrar - usar todos los productos para predicci√≥n\n",
    "    df_filtered = df.copy()\n",
    "    \n",
    "    print(f\"Datos para predicci√≥n (todos los productos): {df_filtered.shape}\")\n",
    "    \n",
    "    if df_filtered.empty:\n",
    "        print(\"¬°ADVERTENCIA! No se encontraron datos para predicci√≥n.\")\n",
    "        return None\n",
    "    \n",
    "    # Aplicar limpieza de datos\n",
    "    df_filtered = clean_numeric_data_prediction(df_filtered)\n",
    "    \n",
    "    scaler = objects['scaler']\n",
    "    label_encoders = objects['label_encoders']\n",
    "    feature_cols = objects['feature_cols']\n",
    "    categorical_vars = objects['categorical_vars']\n",
    "    numeric_vars = objects['numeric_vars']\n",
    "    \n",
    "    # Codificar variables categ√≥ricas\n",
    "    for col in categorical_vars:\n",
    "        if col in df_filtered.columns and col in label_encoders:\n",
    "            le = label_encoders[col]\n",
    "            df_filtered[col] = df_filtered[col].fillna('UNKNOWN')\n",
    "            \n",
    "            # Manejar categor√≠as no vistas durante el entrenamiento\n",
    "            unknown_mask = ~df_filtered[col].astype(str).isin(le.classes_)\n",
    "            df_filtered[col] = df_filtered[col].astype(str)\n",
    "            df_filtered.loc[unknown_mask, col] = 'UNKNOWN'\n",
    "            \n",
    "            # Si 'UNKNOWN' no est√° en las clases, usar la primera clase\n",
    "            if 'UNKNOWN' not in le.classes_:\n",
    "                df_filtered.loc[unknown_mask, col] = le.classes_[0]\n",
    "            \n",
    "            df_filtered[col] = le.transform(df_filtered[col])\n",
    "    \n",
    "    # Seleccionar features\n",
    "    missing_features = [col for col in feature_cols if col not in df_filtered.columns]\n",
    "    if missing_features:\n",
    "        print(f\"¬°ADVERTENCIA! Features faltantes: {missing_features}\")\n",
    "        # Crear columnas faltantes con valor 0\n",
    "        for col in missing_features:\n",
    "            df_filtered[col] = 0\n",
    "    \n",
    "    X_pred = df_filtered[feature_cols].copy()\n",
    "    \n",
    "    # Manejar valores nulos\n",
    "    X_pred = X_pred.fillna(0)\n",
    "    \n",
    "    # Estandarizar variables num√©ricas usando los mismos pasos del entrenamiento\n",
    "    numeric_cols_present = [col for col in numeric_vars if col in X_pred.columns]\n",
    "    numeric_cols_valid = []\n",
    "    \n",
    "    for col in numeric_cols_present:\n",
    "        if X_pred[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "            if not np.isinf(X_pred[col]).any():\n",
    "                max_val = X_pred[col].abs().max()\n",
    "                if max_val < 1e10:\n",
    "                    numeric_cols_valid.append(col)\n",
    "    \n",
    "    if numeric_cols_valid:\n",
    "        X_pred[numeric_cols_valid] = scaler.transform(X_pred[numeric_cols_valid])\n",
    "    \n",
    "    # Conservar informaci√≥n para el resultado final\n",
    "    result_info = df_filtered[['customer_id', 'product_id']].copy()\n",
    "    \n",
    "    return X_pred, result_info\n",
    "\n",
    "def make_predictions_for_products():\n",
    "    \"\"\"\n",
    "    Funci√≥n principal para hacer predicciones para productos espec√≠ficos\n",
    "    \"\"\"\n",
    "    print(\"=== PREDICCI√ìN PARA PRODUCTOS ESPEC√çFICOS ===\\n\")\n",
    "    \n",
    "    # 1. Cargar productos a predecir\n",
    "    productos_a_predecir = load_productos_a_predecir()\n",
    "    \n",
    "    # 2. Cargar modelo y objetos\n",
    "    model, objects = load_model_and_objects()\n",
    "    \n",
    "    # 3. Cargar datos m√°s recientes\n",
    "    df_latest = load_latest_data_for_prediction()\n",
    "    \n",
    "    # 4. Preparar datos para predicci√≥n (TODOS los productos)\n",
    "    prediction_result = prepare_prediction_data(df_latest, objects)\n",
    "    \n",
    "    if prediction_result is None:\n",
    "        print(\"No se pudieron preparar los datos para predicci√≥n.\")\n",
    "        return None\n",
    "    \n",
    "    X_pred, result_info = prediction_result\n",
    "    \n",
    "    # 5. Hacer predicciones PARA TODOS LOS PRODUCTOS\n",
    "    print(\"Generando predicciones mejoradas para todos los productos...\")\n",
    "    predictions_raw = model.predict(X_pred)\n",
    "    predictions = apply_post_prediction_constraints(predictions_raw)  # ‚Üê NUEVO\n",
    "    \n",
    "    # 6. Agregar predicciones al resultado\n",
    "    result_info['predicted_tn'] = predictions\n",
    "    \n",
    "    # 7. Agrupar por product_id y sumar las predicciones (suma por todos los customer_id)\n",
    "    print(\"Agrupando predicciones por product_id...\")\n",
    "    all_predictions = result_info.groupby('product_id').agg({\n",
    "        'predicted_tn': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Renombrar columna para claridad\n",
    "    all_predictions = all_predictions.rename(columns={'predicted_tn': 'tn'})\n",
    "    \n",
    "    print(f\"Predicciones generadas para {len(all_predictions)} productos en total\")\n",
    "    \n",
    "    # 8. FILTRAR RESULTADOS: Solo enviar productos que est√°n en tb_productos_a_predecir\n",
    "    product_ids_to_send = productos_a_predecir['product_id'].unique()\n",
    "    final_predictions = all_predictions[\n",
    "        all_predictions['product_id'].isin(product_ids_to_send)\n",
    "    ].copy()\n",
    "    \n",
    "    # Ordenar por tn descendente\n",
    "    final_predictions = final_predictions.sort_values('tn', ascending=False)\n",
    "    \n",
    "    print(f\"\\nResultados finales (filtrados):\")\n",
    "    print(f\"Productos en tb_productos_a_predecir: {len(product_ids_to_send)}\")\n",
    "    print(f\"Productos con predicci√≥n disponible: {len(final_predictions)}\")\n",
    "    print(f\"Total tn predichas (solo productos solicitados): {final_predictions['tn'].sum():.2f}\")\n",
    "    print(f\"Promedio tn por producto: {final_predictions['tn'].mean():.2f}\")\n",
    "    \n",
    "    # Mostrar top 10\n",
    "    print(\"\\nTop 10 productos solicitados con mayor predicci√≥n:\")\n",
    "    print(final_predictions.head(10))\n",
    "    \n",
    "    # 9. Verificar que tenemos todos los productos solicitados\n",
    "    productos_solicitados = set(product_ids_to_send)\n",
    "    productos_predichos = set(final_predictions['product_id'].unique())\n",
    "    productos_faltantes = productos_solicitados - productos_predichos\n",
    "    \n",
    "    if productos_faltantes:\n",
    "        print(f\"\\n¬°ADVERTENCIA! Productos solicitados sin datos en 201912: {len(productos_faltantes)}\")\n",
    "        print(f\"Product_ids faltantes: {list(productos_faltantes)[:10]}...\")  # Mostrar solo los primeros 10\n",
    "        \n",
    "        # Agregar productos faltantes con tn = 0\n",
    "        for product_id in productos_faltantes:\n",
    "            final_predictions = pd.concat([\n",
    "                final_predictions,\n",
    "                pd.DataFrame({'product_id': [product_id], 'tn': [0.0]})\n",
    "            ], ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nPredicciones finales enviadas: {len(final_predictions)} productos\")\n",
    "    \n",
    "    # 10. Guardar resultados\n",
    "    output_file = '../output/lgbm/02_lgbm_model_v2.csv'\n",
    "    final_predictions.to_csv(output_file, index=False)\n",
    "    print(f\"Predicciones guardadas en: {output_file}\")\n",
    "    \n",
    "    # Opcional: Guardar tambi√©n TODAS las predicciones para an√°lisis\n",
    "    #all_output_file = 'predicciones_todos_los_productos.csv'\n",
    "    #all_predictions.sort_values('tn', ascending=False).to_csv(all_output_file, index=False)\n",
    "    #print(f\"TODAS las predicciones guardadas en: {all_output_file}\")\n",
    "    \n",
    "    return final_predictions\n",
    "\n",
    "# Ejecutar predicciones\n",
    "if __name__ == \"__main__\":\n",
    "    predicciones = make_predictions_for_products()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311 (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
