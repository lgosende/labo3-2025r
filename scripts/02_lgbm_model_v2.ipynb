{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd556e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler, PowerTransformer\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be140ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODELO LGBM PARA PREDICCIÓN DE TONELADAS ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29366cb203904d7b857120d6de438a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos originales: (17021654, 38)\n",
      "Periodos únicos: [201701, 201702, 201703, 201704, 201705, 201706, 201707, 201708, 201709, 201710, 201711, 201712, 201801, 201802, 201803, 201804, 201805, 201806, 201807, 201808, 201809, 201810, 201811, 201812, 201901, 201902, 201903, 201904, 201905, 201906, 201907, 201908, 201909, 201910, 201911, 201912]\n",
      "Creando variable objetivo...\n",
      "Registros con target válido: 15628837\n",
      "Limpiando datos numéricos...\n",
      "  Convertido plan_precios_cuidados de bool a int\n",
      "  Limpiadas 36 columnas numéricas\n",
      "  Convertidos float64 -> float32\n",
      "  Reemplazados valores infinitos por NaN\n",
      "\n",
      "Datos de entrenamiento: (17021654, 39)\n",
      "Datos después de remover NaN en target: (15628837, 39)\n",
      "=== ANÁLISIS DEL TARGET ===\n",
      "Target stats:\n",
      "  Count: 15,628,837\n",
      "  Min: 0.0000\n",
      "  Max: 2295.1982\n",
      "  Mean: 42.6592\n",
      "  Median: 9.6803\n",
      "  Std: 107.7060\n",
      "\n",
      "Percentiles:\n",
      "  1%: 0.0064\n",
      "  5%: 0.1873\n",
      "  10%: 0.6104\n",
      "  25%: 2.2491\n",
      "  50%: 9.6803\n",
      "  75%: 30.0179\n",
      "  90%: 106.0346\n",
      "  95%: 191.1998\n",
      "  99%: 547.6251\n",
      "\n",
      "Distribución de valores:\n",
      "  Negativos: 0 (0.0%)\n",
      "  Ceros: 114,013 (0.7%)\n",
      "  Positivos: 15,514,824 (99.3%)\n",
      "Preparando features (versión corregida)...\n",
      "Variables categóricas (4): ['cat1', 'cat2', 'cat3', 'brand']\n",
      "Variables numéricas (31)\n",
      "Variables excluidas (17)\n",
      "VERIFICACIÓN: product_id excluido = True\n",
      "VERIFICACIÓN: customer_id excluido = True\n",
      "VERIFICACIÓN: periodo excluido = True\n",
      "Codificando variables categóricas...\n",
      "  cat1: 4 categorías únicas\n",
      "  cat2: 15 categorías únicas\n",
      "  brand: 36 categorías únicas\n",
      "\n",
      "Features finales: 34\n",
      "=== DEBUG DE SELECCIÓN DE FEATURES ===\n",
      "⚠️  Variables sospechosas en features:\n",
      "  - plan_precios_cuidados\n",
      "\n",
      "Total features seleccionadas: 34\n",
      "Primeras 10 features: ['delta_tn_12m', 'delta_tn_3m', 'meses_desde_ultima_compra', 'ma_tn_6m', 'is_max_tn_6m', 'tn_lag_1m', 'ma_tn_3m', 'delta_tn_1m', 'is_max_tn_3m', 'plan_precios_cuidados']\n",
      "Shape final - X: (15628837, 34), y: (15628837,)\n",
      "Train: (10002455, 34), Val: (2500614, 34), Test: (3125768, 34)\n",
      "Estandarizando variables numéricas...\n",
      "Estandarizadas 31 variables numéricas\n",
      "Estandarizando variables numéricas...\n",
      "Estandarizadas 31 variables numéricas\n",
      "Entrenando modelo LightGBM mejorado...\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttrain's rmse: 88.6133\tval's rmse: 88.8716\n",
      "[200]\ttrain's rmse: 86.655\tval's rmse: 86.9984\n",
      "[300]\ttrain's rmse: 85.8466\tval's rmse: 86.1876\n",
      "[400]\ttrain's rmse: 85.2547\tval's rmse: 85.6309\n",
      "[500]\ttrain's rmse: 84.803\tval's rmse: 85.2165\n",
      "[600]\ttrain's rmse: 84.4419\tval's rmse: 84.8915\n",
      "[700]\ttrain's rmse: 84.102\tval's rmse: 84.5863\n",
      "[800]\ttrain's rmse: 83.7748\tval's rmse: 84.2927\n",
      "[900]\ttrain's rmse: 83.4092\tval's rmse: 83.964\n",
      "[1000]\ttrain's rmse: 83.1161\tval's rmse: 83.7137\n",
      "[1100]\ttrain's rmse: 82.8797\tval's rmse: 83.5131\n",
      "[1200]\ttrain's rmse: 82.6603\tval's rmse: 83.3354\n",
      "[1300]\ttrain's rmse: 82.4492\tval's rmse: 83.171\n",
      "[1400]\ttrain's rmse: 82.2307\tval's rmse: 82.9858\n",
      "[1500]\ttrain's rmse: 82.0361\tval's rmse: 82.8299\n",
      "[1600]\ttrain's rmse: 81.8514\tval's rmse: 82.6835\n",
      "[1700]\ttrain's rmse: 81.7023\tval's rmse: 82.5625\n",
      "[1800]\ttrain's rmse: 81.5408\tval's rmse: 82.4391\n",
      "[1900]\ttrain's rmse: 81.3929\tval's rmse: 82.332\n",
      "[2000]\ttrain's rmse: 81.2423\tval's rmse: 82.2236\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\ttrain's rmse: 81.2423\tval's rmse: 82.2236\n",
      "Evaluando modelo...\n",
      "Aplicando restricciones post-predicción...\n",
      "Predicciones originales - Min: -162.9596, Max: 1936.1211\n",
      "Predicciones negativas: 258393 (8.3%)\n",
      "Después del clip - Min: 0.0000, Max: 1936.1211\n",
      "\n",
      "Métricas del modelo:\n",
      "MAE: 30.2539\n",
      "RMSE: 89.8599\n",
      "R²: 0.3487\n",
      "\n",
      "Top 15 features más importantes:\n",
      "                   feature    importance\n",
      "33                   brand  3.312039e+11\n",
      "32                    cat2  1.092025e+11\n",
      "23               ma_tn_12m  9.661057e+10\n",
      "15     antiguedad_producto  4.140457e+10\n",
      "14      antiguedad_cliente  3.396043e+10\n",
      "11  promedio_historico_mes  2.375417e+10\n",
      "3                 ma_tn_6m  2.323515e+10\n",
      "6                 ma_tn_3m  1.680455e+10\n",
      "31                    cat1  1.650691e+10\n",
      "9    plan_precios_cuidados  1.424582e+10\n",
      "26        cust_request_qty  1.056862e+10\n",
      "20    stddev_historico_mes  1.029396e+10\n",
      "29               trimestre  1.005042e+10\n",
      "22         cust_request_tn  7.645818e+09\n",
      "7              delta_tn_1m  6.497705e+09\n",
      "\n",
      "Guardando modelo...\n",
      "Modelo guardado como '02_lgbm_model_v2.txt'\n",
      "Objetos auxiliares guardados como '02_lgbm_model_v2.pkl'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuración\n",
    "PERIODO_CORTE = 201912  # Entrenar hasta este periodo\n",
    "TARGET_LAG = 2  # Predecir periodo +2\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"\n",
    "    Carga y prepara los datos para el modelo\n",
    "    \"\"\"\n",
    "    # Conectar a la base de datos DuckDB\n",
    "    con = duckdb.connect(database='../input/db/labo3.duckdb')\n",
    "    \n",
    "    # Cargar datos de la tabla ventas_features_final\n",
    "    query = \"\"\"SELECT \n",
    "        antiguedad_cliente,\n",
    "        antiguedad_producto,\n",
    "        cust_request_qty,\n",
    "        cust_request_tn,\n",
    "        customer_id,\n",
    "        periodo,\n",
    "        plan_precios_cuidados,\n",
    "        product_id,\n",
    "        registro_sintetico,\n",
    "        -- stock_final,\n",
    "        tn,\n",
    "        anio,\n",
    "        cliente_recurrente,\n",
    "        coef_variacion_6m,\n",
    "        crecimiento_consistente,\n",
    "        delta_request_1m,\n",
    "        delta_request_3m,\n",
    "        delta_request_6m,\n",
    "        delta_stock_1m,\n",
    "        delta_stock_3m,\n",
    "        delta_tn_12m,\n",
    "        delta_tn_1m,\n",
    "        delta_tn_3m,\n",
    "        delta_tn_6m,\n",
    "        es_invierno,\n",
    "        es_verano,\n",
    "        fill_rate,\n",
    "        is_max_tn_12m,\n",
    "        is_max_tn_3m,\n",
    "        is_max_tn_6m,\n",
    "        is_min_tn_12m,\n",
    "        is_min_tn_3m,\n",
    "        is_min_tn_6m,\n",
    "        ma_request_3m,\n",
    "        ma_request_6m,\n",
    "        ma_stock_3m,\n",
    "        ma_tn_12m,\n",
    "        ma_tn_3m,\n",
    "        ma_tn_6m,\n",
    "        mes,\n",
    "        pct_change_tn_1m,\n",
    "        pct_change_tn_3m,\n",
    "        pct_change_vs_mismo_mes_anio_anterior,\n",
    "        producto_maduro,\n",
    "        ratio_tn_vs_2m_ahead,\n",
    "        ratio_vs_ma_6m,\n",
    "        request_tn_lag_1m,\n",
    "        request_tn_lag_3m,\n",
    "        request_tn_lag_6m,\n",
    "        stock_lag_1m,\n",
    "        stock_lag_3m,\n",
    "        stock_turnover_ratio,\n",
    "        tendencia_6m,\n",
    "        tn_lag_12m,\n",
    "        tn_lag_1m,\n",
    "        tn_lag_3m,\n",
    "        tn_lag_6m,\n",
    "        tn_mismo_mes_anio_anterior,\n",
    "        trimestre,\n",
    "        volatilidad_tn_6m,\n",
    "        -- clientes_total_producto,\n",
    "        coef_variacion_estacional,\n",
    "        desviacion_vs_promedio_historico_mes,\n",
    "        es_cliente_principal_producto,\n",
    "        es_producto_principal_cliente,\n",
    "        indice_concentracion_producto,\n",
    "        indice_estacionalidad,\n",
    "        -- meses_desde_ultima_compra,\n",
    "        -- participacion_cliente_en_producto,\n",
    "        -- participacion_producto_en_cliente,\n",
    "        patron_ciclico_3m,\n",
    "        patron_ciclico_6m,\n",
    "        periodicidad_promedio_compras,\n",
    "        productos_total_cliente_periodo,\n",
    "        promedio_anual,\n",
    "        promedio_historico_mes,\n",
    "        ranking_cliente_en_producto,\n",
    "        ranking_producto_en_cliente,\n",
    "        ratio_vs_promedio_otros_productos_cliente,\n",
    "        stddev_historico_mes,\n",
    "        -- tn_total_cliente_periodo,\n",
    "        -- tn_total_producto_periodo,\n",
    "        cat1,\n",
    "        cat2,\n",
    "        -- cat3,\n",
    "        brand\n",
    "        -- sku_size\n",
    "    FROM ventas_features_final\"\"\"\n",
    "    df = con.execute(query).df()\n",
    "    \n",
    "    # Cerrar conexión\n",
    "    con.close()\n",
    "    print(f\"Datos originales: {df.shape}\")\n",
    "    print(f\"Periodos únicos: {sorted(df['periodo'].unique())}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_target_distribution(df):\n",
    "    \"\"\"\n",
    "    Analiza la distribución del target para entender el problema\n",
    "    \"\"\"\n",
    "    print(\"=== ANÁLISIS DEL TARGET ===\")\n",
    "    target = df['target_tn'].dropna()\n",
    "    \n",
    "    print(f\"Target stats:\")\n",
    "    print(f\"  Count: {len(target):,}\")\n",
    "    print(f\"  Min: {target.min():.4f}\")\n",
    "    print(f\"  Max: {target.max():.4f}\")\n",
    "    print(f\"  Mean: {target.mean():.4f}\")\n",
    "    print(f\"  Median: {target.median():.4f}\")\n",
    "    print(f\"  Std: {target.std():.4f}\")\n",
    "    \n",
    "    # Percentiles\n",
    "    percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]\n",
    "    print(f\"\\nPercentiles:\")\n",
    "    for p in percentiles:\n",
    "        print(f\"  {p}%: {np.percentile(target, p):.4f}\")\n",
    "    \n",
    "    # Valores negativos y ceros\n",
    "    negative_count = (target < 0).sum()\n",
    "    zero_count = (target == 0).sum()\n",
    "    positive_count = (target > 0).sum()\n",
    "    \n",
    "    print(f\"\\nDistribución de valores:\")\n",
    "    print(f\"  Negativos: {negative_count:,} ({negative_count/len(target)*100:.1f}%)\")\n",
    "    print(f\"  Ceros: {zero_count:,} ({zero_count/len(target)*100:.1f}%)\")\n",
    "    print(f\"  Positivos: {positive_count:,} ({positive_count/len(target)*100:.1f}%)\")\n",
    "    \n",
    "    return target\n",
    "def identify_problematic_features(train_data, target_col='target_tn'):\n",
    "    \"\"\"\n",
    "    Identifica variables problemáticas que pueden causar overfitting\n",
    "    \"\"\"\n",
    "    print(\"=== IDENTIFICACIÓN DE VARIABLES PROBLEMÁTICAS ===\")\n",
    "    \n",
    "    # 1. Variables que pueden ser data leakage\n",
    "    potential_leaks = []\n",
    "    suspicious_keywords = ['total', 'sum', 'global', 'all', 'periodo_total']\n",
    "    \n",
    "    for col in train_data.columns:\n",
    "        for keyword in suspicious_keywords:\n",
    "            if keyword in col.lower():\n",
    "                potential_leaks.append(col)\n",
    "                break\n",
    "    \n",
    "    print(f\"Posibles data leaks ({len(potential_leaks)}): {potential_leaks}\")\n",
    "    \n",
    "    # 2. Variables con valores extremos\n",
    "    numeric_cols = train_data.select_dtypes(include=[np.number]).columns\n",
    "    extreme_vars = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col != target_col and col in train_data.columns:\n",
    "            q99 = train_data[col].quantile(0.99)\n",
    "            q01 = train_data[col].quantile(0.01)\n",
    "            range_ratio = q99 / (q01 + 1e-8)  # Evitar división por 0\n",
    "            \n",
    "            if range_ratio > 1000:  # Rango muy amplio\n",
    "                extreme_vars.append((col, range_ratio))\n",
    "    \n",
    "    extreme_vars.sort(key=lambda x: x[1], reverse=True)\n",
    "    print(f\"\\nVariables con rangos extremos (top 10):\")\n",
    "    for col, ratio in extreme_vars[:10]:\n",
    "        print(f\"  {col}: ratio={ratio:.1f}\")\n",
    "    \n",
    "    # 3. Variables altamente correlacionadas con el target\n",
    "    target_corr = []\n",
    "    for col in numeric_cols:\n",
    "        if col != target_col and col in train_data.columns:\n",
    "            corr = train_data[col].corr(train_data[target_col])\n",
    "            if not np.isnan(corr) and abs(corr) > 0.8:\n",
    "                target_corr.append((col, corr))\n",
    "    \n",
    "    target_corr.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    print(f\"\\nVariables muy correlacionadas con target (>0.8):\")\n",
    "    for col, corr in target_corr:\n",
    "        print(f\"  {col}: corr={corr:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'potential_leaks': potential_leaks,\n",
    "        'extreme_vars': [x[0] for x in extreme_vars],\n",
    "        'high_corr': [x[0] for x in target_corr]\n",
    "    }\n",
    "\n",
    "def create_feature_exclusion_list(problematic_features):\n",
    "    \"\"\"\n",
    "    Crea lista de variables a excluir\n",
    "    \"\"\"\n",
    "    print(\"=== CREANDO LISTA DE EXCLUSIÓN ===\")\n",
    "    \n",
    "    # Variables definitivamente problemáticas\n",
    "    definite_excludes = [\n",
    "        'tn_total_cliente_periodo',\n",
    "        'tn_total_producto_periodo', \n",
    "        'clientes_total_producto',\n",
    "        'productos_total_cliente_periodo'\n",
    "    ]\n",
    "\n",
    "    # Variables importantes que NUNCA debemos excluir\n",
    "    keep_always = [\n",
    "        'antiguedad_cliente',\n",
    "        'antiguedad_producto',\n",
    "        #'stock_final',\n",
    "        'fill_rate',\n",
    "        'cust_request_qty',\n",
    "        'cust_request_tn'\n",
    "    ]\n",
    "    \n",
    "    auto_excludes = []\n",
    "\n",
    "    # Filtrar potential_leaks excluyendo las importantes\n",
    "    filtered_leaks = [var for var in problematic_features['potential_leaks'] \n",
    "                      if var not in keep_always]\n",
    "    auto_excludes.extend(filtered_leaks)\n",
    "    \n",
    "    # Solo las más extremas de las variables con rangos amplios (pero filtradas)\n",
    "    filtered_extreme = [var for var in problematic_features['extreme_vars'][:5] \n",
    "                        if var not in keep_always]\n",
    "    auto_excludes.extend(filtered_extreme)\n",
    "    \n",
    "    # Variables muy correlacionadas (posible leakage) - pero filtradas\n",
    "    filtered_corr = [var for var in problematic_features['high_corr'] \n",
    "                     if var not in keep_always]\n",
    "    auto_excludes.extend(filtered_corr)\n",
    "    \n",
    "    # Remover duplicados\n",
    "    all_excludes = list(set(definite_excludes + auto_excludes))\n",
    "    \n",
    "    print(f\"Variables a excluir ({len(all_excludes)}):\")\n",
    "    for var in sorted(all_excludes):\n",
    "        print(f\"  - {var}\")\n",
    "    \n",
    "    return all_excludes\n",
    "\n",
    "def robust_preprocessing(X_train, X_test, numeric_vars, extreme_threshold=1000):\n",
    "    \"\"\"\n",
    "    Preprocesamiento robusto usando RobustScaler y transformaciones\n",
    "    \"\"\"\n",
    "    print(\"=== PREPROCESAMIENTO ROBUSTO ===\")\n",
    "    \n",
    "    X_train_processed = X_train.copy()\n",
    "    X_test_processed = X_test.copy()\n",
    "    \n",
    "    # Identificar columnas numéricas válidas\n",
    "    numeric_cols_present = [col for col in numeric_vars if col in X_train.columns]\n",
    "    \n",
    "    transformers = {}\n",
    "    \n",
    "    for col in numeric_cols_present:\n",
    "        if X_train_processed[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "            \n",
    "            # 1. Manejar valores extremos\n",
    "            q99 = X_train_processed[col].quantile(0.99)\n",
    "            q01 = X_train_processed[col].quantile(0.01)\n",
    "            \n",
    "            # Winsorización (clip outliers)\n",
    "            X_train_processed[col] = X_train_processed[col].clip(q01, q99)\n",
    "            X_test_processed[col] = X_test_processed[col].clip(q01, q99)\n",
    "            \n",
    "            # 2. Verificar si necesita transformación adicional\n",
    "            col_range = q99 - q01\n",
    "            col_std = X_train_processed[col].std()\n",
    "            \n",
    "            if col_range > extreme_threshold or col_std > extreme_threshold:\n",
    "                print(f\"  Aplicando transformación robusta a: {col}\")\n",
    "                \n",
    "                # Usar RobustScaler (menos sensible a outliers)\n",
    "                scaler = RobustScaler()\n",
    "                X_train_processed[col] = scaler.fit_transform(X_train_processed[col].values.reshape(-1, 1)).flatten()\n",
    "                X_test_processed[col] = scaler.transform(X_test_processed[col].values.reshape(-1, 1)).flatten()\n",
    "                \n",
    "                transformers[col] = scaler\n",
    "            else:\n",
    "                # StandardScaler normal para variables bien comportadas\n",
    "                from sklearn.preprocessing import StandardScaler\n",
    "                scaler = StandardScaler()\n",
    "                X_train_processed[col] = scaler.fit_transform(X_train_processed[col].values.reshape(-1, 1)).flatten()\n",
    "                X_test_processed[col] = scaler.transform(X_test_processed[col].values.reshape(-1, 1)).flatten()\n",
    "                \n",
    "                transformers[col] = scaler\n",
    "    \n",
    "    print(f\"Procesadas {len(transformers)} variables numéricas\")\n",
    "    \n",
    "    return X_train_processed, X_test_processed, transformers\n",
    "\n",
    "def debug_feature_selection(train_data, feature_cols):\n",
    "    \"\"\"\n",
    "    Debuggea qué variables se están incluyendo incorrectamente\n",
    "    \"\"\"\n",
    "    print(\"=== DEBUG DE SELECCIÓN DE FEATURES ===\")\n",
    "    \n",
    "    # Variables que DEFINITIVAMENTE no deberían estar\n",
    "    forbidden_vars = ['product_id', 'customer_id', 'periodo', 'target_tn', 'tn']\n",
    "    forbidden_found = []\n",
    "    \n",
    "    for var in forbidden_vars:\n",
    "        if var in feature_cols:\n",
    "            forbidden_found.append(var)\n",
    "    \n",
    "    if forbidden_found:\n",
    "        print(f\"🚨 ERROR: Variables prohibidas encontradas en features:\")\n",
    "        for var in forbidden_found:\n",
    "            print(f\"  - {var}\")\n",
    "    \n",
    "    # Variables con nombres sospechosos\n",
    "    suspicious_vars = []\n",
    "    for var in feature_cols:\n",
    "        if any(keyword in var.lower() for keyword in ['total', 'id', 'key_']):\n",
    "            suspicious_vars.append(var)\n",
    "    \n",
    "    if suspicious_vars:\n",
    "        print(f\"⚠️  Variables sospechosas en features:\")\n",
    "        for var in suspicious_vars:\n",
    "            print(f\"  - {var}\")\n",
    "    \n",
    "    print(f\"\\nTotal features seleccionadas: {len(feature_cols)}\")\n",
    "    print(f\"Primeras 10 features: {feature_cols[:10]}\")\n",
    "    \n",
    "    return forbidden_found, suspicious_vars\n",
    "def prepare_features_fixed(df):\n",
    "    \"\"\"\n",
    "    Función de preparación de features con exclusión FORZADA\n",
    "    \"\"\"\n",
    "    print(\"Preparando features (versión corregida)...\")\n",
    "    \n",
    "    # 1. EXCLUSIÓN FORZADA DE IDENTIFICADORES\n",
    "    forced_excludes = [\n",
    "        'product_id', 'customer_id', 'periodo',  # IDs\n",
    "        'target_tn', 'tn',  # Target y variable original\n",
    "        'key_customer_producto_periodo',  # Keys\n",
    "        'key_periodo_customer_producto',\n",
    "        'key_periodo_producto',\n",
    "        'PVC', 'PVP', 'UVC', 'UVP', 'periodo_fecha'  # Fechas\n",
    "    ]\n",
    "    \n",
    "    # 2. EXCLUSIÓN DE VARIABLES PROBLEMÁTICAS\n",
    "    problematic_excludes = [\n",
    "        'tn_total_cliente_periodo',\n",
    "        'tn_total_producto_periodo', \n",
    "        'clientes_total_producto',\n",
    "        'productos_total_cliente_periodo'\n",
    "    ]\n",
    "    \n",
    "    # 3. Variables categóricas\n",
    "    categorical_vars = ['cat1', 'cat2', 'cat3', 'brand']\n",
    "    \n",
    "    # 4. Combinar todas las exclusiones\n",
    "    all_excludes = forced_excludes + problematic_excludes\n",
    "    \n",
    "    # 5. Variables numéricas = todo lo demás\n",
    "    all_vars = set(df.columns)\n",
    "    excluded_set = set(all_excludes)\n",
    "    categorical_set = set(categorical_vars)\n",
    "    \n",
    "    numeric_vars = list(all_vars - excluded_set - categorical_set)\n",
    "    \n",
    "    # 6. Verificar que no hay contaminación\n",
    "    contaminated = []\n",
    "    for var in numeric_vars:\n",
    "        if any(forbidden in var.lower() for forbidden in ['product_id', 'customer_id', 'periodo']):\n",
    "            contaminated.append(var)\n",
    "    \n",
    "    if contaminated:\n",
    "        print(f\"🚨 ELIMINANDO variables contaminadas: {contaminated}\")\n",
    "        numeric_vars = [var for var in numeric_vars if var not in contaminated]\n",
    "    \n",
    "    print(f\"Variables categóricas ({len(categorical_vars)}): {categorical_vars}\")\n",
    "    print(f\"Variables numéricas ({len(numeric_vars)})\")\n",
    "    print(f\"Variables excluidas ({len(all_excludes)})\")\n",
    "    print(f\"VERIFICACIÓN: product_id excluido = {'product_id' not in numeric_vars}\")\n",
    "    print(f\"VERIFICACIÓN: customer_id excluido = {'customer_id' not in numeric_vars}\")\n",
    "    print(f\"VERIFICACIÓN: periodo excluido = {'periodo' not in numeric_vars}\")\n",
    "    \n",
    "    return numeric_vars, categorical_vars, all_excludes\n",
    "\n",
    "\n",
    "\n",
    "def prepare_features_improved(df):\n",
    "    \"\"\"\n",
    "    Versión mejorada de prepare_features que excluye variables problemáticas\n",
    "    \"\"\"\n",
    "    print(\"Preparando features (versión mejorada)...\")\n",
    "    \n",
    "    # 1. Identificar variables problemáticas\n",
    "    problematic = identify_problematic_features(df)\n",
    "    \n",
    "    # 2. Crear lista de exclusión\n",
    "    auto_excludes = create_feature_exclusion_list(problematic)\n",
    "    \n",
    "    # 3. Variables a excluir (originales + automáticas)\n",
    "    exclude_vars = [col for col in df.columns if col.startswith('key_')]\n",
    "    exclude_vars.extend(['target_tn', 'tn'])\n",
    "    \n",
    "    # Excluir variables de fecha\n",
    "    date_vars = ['PVC', 'PVP', 'UVC', 'UVP', 'periodo_fecha']\n",
    "    exclude_vars.extend([col for col in date_vars if col in df.columns])\n",
    "\n",
    "    id_vars = ['product_id', 'customer_id', 'periodo']\n",
    "    exclude_vars.extend([col for col in id_vars if col in df.columns])\n",
    "    \n",
    "    # Agregar exclusiones automáticas\n",
    "    exclude_vars.extend(auto_excludes)\n",
    "    \n",
    "    # Remover duplicados\n",
    "    exclude_vars = list(set(exclude_vars))\n",
    "    \n",
    "    # Variables categóricas\n",
    "    categorical_vars = ['cat1', 'cat2', 'cat3', 'brand']\n",
    "    \n",
    "    # Variables numéricas\n",
    "    all_vars = set(df.columns)\n",
    "    numeric_vars = list(all_vars - set(exclude_vars) - set(categorical_vars))\n",
    "    \n",
    "    print(f\"Variables categóricas ({len(categorical_vars)}): {categorical_vars}\")\n",
    "    print(f\"Variables numéricas ({len(numeric_vars)}): {numeric_vars[:5]}...\")\n",
    "    print(f\"Variables excluidas ({len(exclude_vars)}): {auto_excludes[:5]}...\")\n",
    "    \n",
    "    return numeric_vars, categorical_vars, exclude_vars\n",
    "\n",
    "def apply_same_preprocessing(X_new, transformers, numeric_vars):\n",
    "    \"\"\"\n",
    "    Aplica las mismas transformaciones del entrenamiento a nuevos datos\n",
    "    \"\"\"\n",
    "    X_processed = X_new.copy()\n",
    "    \n",
    "    for col in numeric_vars:\n",
    "        if col in transformers and col in X_processed.columns:\n",
    "            if X_processed[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "                # Aplicar la misma transformación que se usó en entrenamiento\n",
    "                scaler = transformers[col]\n",
    "                X_processed[col] = scaler.transform(X_processed[col].values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    return X_processed\n",
    "\n",
    "def apply_post_prediction_constraints(predictions):\n",
    "    \"\"\"\n",
    "    Aplica restricciones post-predicción para asegurar valores válidos\n",
    "    \"\"\"\n",
    "    print(\"Aplicando restricciones post-predicción...\")\n",
    "    \n",
    "    original_min = predictions.min()\n",
    "    original_max = predictions.max()\n",
    "    negative_count = (predictions < 0).sum()\n",
    "    \n",
    "    print(f\"Predicciones originales - Min: {original_min:.4f}, Max: {original_max:.4f}\")\n",
    "    print(f\"Predicciones negativas: {negative_count} ({negative_count/len(predictions)*100:.1f}%)\")\n",
    "    \n",
    "    # Clip a 0 (evita valores negativos)\n",
    "    predictions_clipped = np.maximum(predictions, 0)\n",
    "    \n",
    "    print(f\"Después del clip - Min: {predictions_clipped.min():.4f}, Max: {predictions_clipped.max():.4f}\")\n",
    "    \n",
    "    return predictions_clipped\n",
    "\n",
    "\n",
    "\n",
    "def create_target_variable(df):\n",
    "    \"\"\"\n",
    "    Crea la variable objetivo (tn en periodo +2) agrupada por product_id\n",
    "    \"\"\"\n",
    "    print(\"Creando variable objetivo...\")\n",
    "    \n",
    "    # Agregar tn por product_id y periodo\n",
    "    df_agg = df.groupby(['product_id', 'periodo']).agg({\n",
    "        'tn': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Ordenar por product_id y periodo\n",
    "    df_agg = df_agg.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    # Crear target variable (tn en periodo +2)\n",
    "    df_agg['target_tn'] = df_agg.groupby('product_id')['tn'].shift(-TARGET_LAG)\n",
    "    \n",
    "    # Merge de vuelta con el dataset original\n",
    "    df = df.merge(df_agg[['product_id', 'periodo', 'target_tn']], \n",
    "                  on=['product_id', 'periodo'], how='left')\n",
    "    \n",
    "    print(f\"Registros con target válido: {df['target_tn'].notna().sum()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_features(df):\n",
    "    \"\"\"\n",
    "    Prepara las features excluyendo variables key_*, fechas y separando categóricas\n",
    "    \"\"\"\n",
    "    print(\"Preparando features...\")\n",
    "    \n",
    "    # Variables a excluir\n",
    "    exclude_vars = [col for col in df.columns if col.startswith('key_')]\n",
    "    exclude_vars.extend(['target_tn', 'tn'])  # Excluir también el target y la variable original\n",
    "    \n",
    "    # Excluir variables de fecha que no son útiles como features numéricas\n",
    "    date_vars = ['PVC', 'PVP', 'UVC', 'UVP', 'periodo_fecha']\n",
    "    exclude_vars.extend([col for col in date_vars if col in df.columns])\n",
    "    \n",
    "    # Variables categóricas\n",
    "    categorical_vars = ['cat1', 'cat2', 'cat3', 'brand']\n",
    "    \n",
    "    # Variables numéricas (todas las demás excepto las excluidas)\n",
    "    all_vars = set(df.columns)\n",
    "    numeric_vars = list(all_vars - set(exclude_vars) - set(categorical_vars))\n",
    "    \n",
    "    print(f\"Variables categóricas ({len(categorical_vars)}): {categorical_vars}\")\n",
    "    print(f\"Variables numéricas ({len(numeric_vars)}): {numeric_vars[:10]}...\")  # Mostrar solo las primeras 10\n",
    "    print(f\"Variables excluidas ({len(exclude_vars)}): {exclude_vars}\")\n",
    "    \n",
    "    return numeric_vars, categorical_vars, exclude_vars\n",
    "\n",
    "def encode_categorical_variables(df, categorical_vars):\n",
    "    \"\"\"\n",
    "    Codifica variables categóricas usando LabelEncoder\n",
    "    \"\"\"\n",
    "    print(\"Codificando variables categóricas...\")\n",
    "    \n",
    "    label_encoders = {}\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    for col in categorical_vars:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            # Manejar valores nulos\n",
    "            df_encoded[col] = df_encoded[col].fillna('UNKNOWN')\n",
    "            df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "            print(f\"  {col}: {len(le.classes_)} categorías únicas\")\n",
    "    \n",
    "    return df_encoded, label_encoders\n",
    "\n",
    "def clean_numeric_data(df):\n",
    "    \"\"\"\n",
    "    Limpia y convierte datos numéricos para evitar problemas\n",
    "    \"\"\"\n",
    "    print(\"Limpiando datos numéricos...\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convertir plan_precios_cuidados de bool a int\n",
    "    if 'plan_precios_cuidados' in df_clean.columns:\n",
    "        df_clean['plan_precios_cuidados'] = df_clean['plan_precios_cuidados'].astype(int)\n",
    "        print(\"  Convertido plan_precios_cuidados de bool a int\")\n",
    "    \n",
    "    # Identificar columnas numéricas\n",
    "    numeric_columns = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        # Reemplazar infinitos por NaN\n",
    "        df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Convertir float64 a float32 para ahorrar memoria y evitar problemas de precisión\n",
    "        if df_clean[col].dtype == 'float64':\n",
    "            df_clean[col] = df_clean[col].astype('float32')\n",
    "    \n",
    "    print(f\"  Limpiadas {len(numeric_columns)} columnas numéricas\")\n",
    "    print(\"  Convertidos float64 -> float32\")\n",
    "    print(\"  Reemplazados valores infinitos por NaN\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def standardize_numeric_features(X_train, X_test, numeric_vars):\n",
    "    \"\"\"\n",
    "    Estandariza variables numéricas\n",
    "    \"\"\"\n",
    "    print(\"Estandarizando variables numéricas...\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Copiar los datasets\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    \n",
    "    # Manejar valores nulos antes de estandarizar\n",
    "    X_train_scaled = X_train_scaled.fillna(0)\n",
    "    X_test_scaled = X_test_scaled.fillna(0)\n",
    "    \n",
    "    # Identificar columnas numéricas que existen en los datos\n",
    "    numeric_cols_present = [col for col in numeric_vars if col in X_train.columns]\n",
    "    \n",
    "    # Verificar que las columnas sean realmente numéricas y no tengan problemas\n",
    "    numeric_cols_valid = []\n",
    "    for col in numeric_cols_present:\n",
    "        if X_train_scaled[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "            # Verificar que no haya infinitos o valores muy grandes\n",
    "            if not (np.isinf(X_train_scaled[col]).any() or np.isinf(X_test_scaled[col]).any()):\n",
    "                # Verificar que no haya valores extremadamente grandes\n",
    "                max_val = max(X_train_scaled[col].abs().max(), X_test_scaled[col].abs().max())\n",
    "                if max_val < 1e10:  # Límite razonable\n",
    "                    numeric_cols_valid.append(col)\n",
    "                else:\n",
    "                    print(f\"  Saltando {col}: valores muy grandes (max: {max_val:.2e})\")\n",
    "            else:\n",
    "                print(f\"  Saltando {col}: contiene valores infinitos\")\n",
    "        else:\n",
    "            print(f\"  Saltando columna no numérica: {col} (tipo: {X_train_scaled[col].dtype})\")\n",
    "    \n",
    "    if numeric_cols_valid:\n",
    "        # Estandarizar solo las columnas numéricas válidas\n",
    "        X_train_scaled[numeric_cols_valid] = scaler.fit_transform(X_train_scaled[numeric_cols_valid])\n",
    "        X_test_scaled[numeric_cols_valid] = scaler.transform(X_test_scaled[numeric_cols_valid])\n",
    "        print(f\"Estandarizadas {len(numeric_cols_valid)} variables numéricas\")\n",
    "    else:\n",
    "        print(\"No se encontraron variables numéricas válidas para estandarizar\")\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "def train_lgbm_model(X_train, y_train, X_val, y_val, categorical_features):\n",
    "    \"\"\"\n",
    "    Entrena modelo LightGBM con parámetros mejorados\n",
    "    \"\"\"\n",
    "    print(\"Entrenando modelo LightGBM mejorado...\")\n",
    "    \n",
    "    # Parámetros mejorados\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 50,  # Aumentado\n",
    "        'learning_rate': 0.03,  # Reducido\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'min_data_in_leaf': 100,  # NUEVO - evita overfitting\n",
    "        'lambda_l1': 0.1,  # NUEVO - regularización\n",
    "        'lambda_l2': 0.1,  # NUEVO - regularización\n",
    "        'verbose': -1,\n",
    "        'random_state': 42,\n",
    "        'force_col_wise': True\n",
    "    }\n",
    "    \n",
    "    # Crear datasets de LightGBM\n",
    "    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=categorical_features, reference=train_data)\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'val'],\n",
    "        num_boost_round=2000,  # Aumentado\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=200), lgb.log_evaluation(period=100)]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo con mejoras\n",
    "    \"\"\"\n",
    "    print(\"Evaluando modelo...\")\n",
    "    \n",
    "    # Predicciones base\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Aplicar restricciones (clip negativo a 0)\n",
    "    y_pred_final = apply_post_prediction_constraints(y_pred)\n",
    "    \n",
    "    # Métricas\n",
    "    mae = mean_absolute_error(y_test, y_pred_final)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_final))\n",
    "    r2 = r2_score(y_test, y_pred_final)\n",
    "    \n",
    "    print(f\"\\nMétricas del modelo:\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    \n",
    "    return {'mae': mae, 'rmse': rmse, 'r2': r2, 'predictions': y_pred_final}\n",
    "\n",
    "def get_feature_importance(model, feature_names):\n",
    "    \"\"\"\n",
    "    Obtiene importancia de features\n",
    "    \"\"\"\n",
    "    importance = model.feature_importance(importance_type='gain')\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 features más importantes:\")\n",
    "    print(feature_imp.head(15))\n",
    "    \n",
    "    return feature_imp\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal\n",
    "    \"\"\"\n",
    "    print(\"=== MODELO LGBM PARA PREDICCIÓN DE TONELADAS ===\\n\")\n",
    "    \n",
    "    # 1. Cargar datos\n",
    "    df = load_and_prepare_data()\n",
    "    \n",
    "    # 2. Crear variable objetivo\n",
    "    df = create_target_variable(df)\n",
    "    \n",
    "    # 2.5. Limpiar datos numéricos (nueva función)\n",
    "    df = clean_numeric_data(df)\n",
    "    \n",
    "    # 3. Filtrar datos para entrenamiento (hasta PERIODO_CORTE)\n",
    "    train_data = df[df['periodo'] <= PERIODO_CORTE].copy()\n",
    "    print(f\"\\nDatos de entrenamiento: {train_data.shape}\")\n",
    "    \n",
    "    # 4. Remover registros sin target\n",
    "    train_data = train_data.dropna(subset=['target_tn'])\n",
    "    print(f\"Datos después de remover NaN en target: {train_data.shape}\")\n",
    "    \n",
    "    # 4.5. Analizar distribución del target ← NUEVO\n",
    "    target_analysis = analyze_target_distribution(train_data)\n",
    "\n",
    "    # 5. Preparar features\n",
    "    numeric_vars, categorical_vars, exclude_vars = prepare_features_fixed(train_data)\n",
    "    #numeric_vars, categorical_vars, exclude_vars = prepare_features(train_data)\n",
    "    #numeric_vars, categorical_vars, exclude_vars = prepare_features_improved(train_data)\n",
    "    \n",
    "    # 6. Codificar variables categóricas\n",
    "    train_data_encoded, label_encoders = encode_categorical_variables(train_data, categorical_vars)\n",
    "    \n",
    "    # 7. Seleccionar features finales\n",
    "    feature_cols = numeric_vars + categorical_vars\n",
    "    feature_cols = [col for col in feature_cols if col in train_data_encoded.columns]\n",
    "    \n",
    "    print(f\"\\nFeatures finales: {len(feature_cols)}\")\n",
    "    \n",
    "    # 8. Preparar X y y\n",
    "    forbidden_found, suspicious_vars = debug_feature_selection(train_data, feature_cols)\n",
    "\n",
    "       \n",
    "    if forbidden_found:\n",
    "        print(\"🛑 DETENIENDO: Variables prohibidas detectadas\")\n",
    "        return None\n",
    "\n",
    "    X = train_data_encoded[feature_cols].copy()\n",
    "    y = train_data_encoded['target_tn'].copy()\n",
    "    \n",
    "    # Manejar valores nulos en X\n",
    "    X = X.fillna(0)\n",
    "    \n",
    "    print(f\"Shape final - X: {X.shape}, y: {y.shape}\")\n",
    "    \n",
    "    # 9. Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=None\n",
    "    )\n",
    "    \n",
    "    # 10. Split train/validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    # 11. Estandarizar variables numéricas\n",
    "    X_train_scaled, X_val_scaled, scaler = standardize_numeric_features(X_train, X_val, numeric_vars)\n",
    "    #X_train_scaled, X_val_scaled, transformers = robust_preprocessing(X_train, X_val, numeric_vars)\n",
    "    X_test_scaled, _, _ = standardize_numeric_features(X_test, X_test, numeric_vars)\n",
    "    #X_test_scaled = apply_same_preprocessing(X_test, transformers, numeric_vars)\n",
    "    \n",
    "    # 12. Identificar features categóricas para LightGBM\n",
    "    categorical_indices = [i for i, col in enumerate(feature_cols) if col in categorical_vars]\n",
    "    \n",
    "    # 13. Entrenar modelo\n",
    "    model = train_lgbm_model(\n",
    "        X_train_scaled, y_train, \n",
    "        X_val_scaled, y_val, \n",
    "        categorical_indices\n",
    "    )\n",
    "    \n",
    "    # 14. Evaluar modelo\n",
    "    results = evaluate_model(model, X_test_scaled, y_test)\n",
    "    \n",
    "    # 15. Feature importance\n",
    "    feature_importance = get_feature_importance(model, feature_cols)\n",
    "    \n",
    "    # 16. Guardar modelo y objetos necesarios\n",
    "    print(\"\\nGuardando modelo...\")\n",
    "    model.save_model('../output/lgbm/02_lgbm_model_v2.txt')\n",
    "    \n",
    "    # Guardar escalador y encoders\n",
    "    import pickle\n",
    "    with open('../output/lgbm/02_lgbm_model_v2.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            #'transformers': transformers,\n",
    "            'scaler': scaler,\n",
    "            'label_encoders': label_encoders,\n",
    "            'feature_cols': feature_cols,\n",
    "            'categorical_vars': categorical_vars,\n",
    "            'numeric_vars': numeric_vars\n",
    "        }, f)\n",
    "    \n",
    "    print(\"Modelo guardado como '02_lgbm_model_v2.txt'\")\n",
    "    print(\"Objetos auxiliares guardados como '02_lgbm_model_v2.pkl'\")\n",
    "    \n",
    "    return model, results, feature_importance\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, results, feature_importance = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14e3983e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREDICCIÓN PARA PRODUCTOS ESPECÍFICOS ===\n",
      "\n",
      "Cargando productos a predecir...\n",
      "Productos a predecir: 780\n",
      "Product_ids únicos: 780\n",
      "Cargando modelo y objetos auxiliares...\n",
      "Cargando datos más recientes para predicción...\n",
      "Datos del periodo 201912: (553419, 98)\n",
      "Preparando datos para predicción...\n",
      "Datos para predicción (todos los productos): (553419, 98)\n",
      "Limpiando datos numéricos para predicción...\n",
      "  Limpiadas 85 columnas numéricas\n",
      "Generando predicciones mejoradas para todos los productos...\n",
      "Aplicando restricciones post-predicción...\n",
      "Predicciones originales - Min: -363.2131, Max: 1435.9539\n",
      "Predicciones negativas: 59310 (10.7%)\n",
      "Después del clip - Min: 0.0000, Max: 1435.9539\n",
      "Agrupando predicciones por product_id...\n",
      "Predicciones generadas para 927 productos en total\n",
      "\n",
      "Resultados finales (filtrados):\n",
      "Productos en tb_productos_a_predecir: 780\n",
      "Productos con predicción disponible: 780\n",
      "Total tn predichas (solo productos solicitados): 17512233.59\n",
      "Promedio tn por producto: 22451.58\n",
      "\n",
      "Top 10 productos solicitados con mayor predicción:\n",
      "    product_id             tn\n",
      "13       20014  543049.798533\n",
      "0        20001  501716.162067\n",
      "3        20004  444297.116065\n",
      "14       20015  414491.139371\n",
      "2        20003  412461.722533\n",
      "4        20005  315909.280767\n",
      "1        20002  313192.421999\n",
      "8        20009  241061.887738\n",
      "6        20007  234385.376849\n",
      "12       20013  217223.857142\n",
      "\n",
      "Predicciones finales enviadas: 780 productos\n",
      "Predicciones guardadas en: ../output/lgbm/02_lgbm_model_v2.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CHUNK DE PREDICCIÓN PARA PRODUCTOS ESPECÍFICOS\n",
    "# =============================================================================\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_productos_a_predecir():\n",
    "    \"\"\"\n",
    "    Carga la lista de productos a predecir desde DuckDB\n",
    "    \"\"\"\n",
    "    print(\"Cargando productos a predecir...\")\n",
    "    \n",
    "    # Conectar a la base de datos DuckDB\n",
    "    con = duckdb.connect(database='../input/db/labo3.duckdb')\n",
    "    \n",
    "    # Cargar productos a predecir\n",
    "    query = \"SELECT * FROM tb_productos_a_predecir\"\n",
    "    productos_df = con.execute(query).df()\n",
    "    \n",
    "    con.close()\n",
    "    \n",
    "    print(f\"Productos a predecir: {len(productos_df)}\")\n",
    "    print(f\"Product_ids únicos: {productos_df['product_id'].nunique()}\")\n",
    "    \n",
    "    return productos_df\n",
    "\n",
    "def load_latest_data_for_prediction():\n",
    "    \"\"\"\n",
    "    Carga los datos más recientes para hacer predicciones\n",
    "    \"\"\"\n",
    "    print(\"Cargando datos más recientes para predicción...\")\n",
    "    \n",
    "    # Conectar a la base de datos DuckDB\n",
    "    con = duckdb.connect(database='../input/db/labo3.duckdb')\n",
    "    \n",
    "    # Cargar datos del periodo más reciente (201912 para predecir 202002)\n",
    "    # Necesitamos datos del periodo 201912 para predecir periodo +2 (202002)\n",
    "    query = \"\"\"\n",
    "    SELECT * \n",
    "    FROM ventas_features_final \n",
    "    WHERE periodo = 201912\n",
    "    \"\"\"\n",
    "    df = con.execute(query).df()\n",
    "    \n",
    "    con.close()\n",
    "    \n",
    "    print(f\"Datos del periodo 201912: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_model_and_objects():\n",
    "    \"\"\"\n",
    "    Carga el modelo entrenado y objetos auxiliares\n",
    "    \"\"\"\n",
    "    print(\"Cargando modelo y objetos auxiliares...\")\n",
    "    \n",
    "    # Cargar modelo LightGBM\n",
    "    model = lgb.Booster(model_file='../output/lgbm/02_lgbm_model_v2.txt')\n",
    "    \n",
    "    # Cargar objetos auxiliares\n",
    "    with open('../output/lgbm/02_lgbm_model_v2.pkl', 'rb') as f:\n",
    "        objects = pickle.load(f)\n",
    "    \n",
    "    return model, objects\n",
    "\n",
    "def clean_numeric_data_prediction(df):\n",
    "    \"\"\"\n",
    "    Aplica la misma limpieza de datos numéricos que en entrenamiento\n",
    "    \"\"\"\n",
    "    print(\"Limpiando datos numéricos para predicción...\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convertir plan_precios_cuidados de bool a int\n",
    "    if 'plan_precios_cuidados' in df_clean.columns:\n",
    "        df_clean['plan_precios_cuidados'] = df_clean['plan_precios_cuidados'].astype(int)\n",
    "    \n",
    "    # Identificar columnas numéricas\n",
    "    numeric_columns = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        # Reemplazar infinitos por NaN\n",
    "        df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Convertir float64 a float32\n",
    "        if df_clean[col].dtype == 'float64':\n",
    "            df_clean[col] = df_clean[col].astype('float32')\n",
    "    \n",
    "    print(f\"  Limpiadas {len(numeric_columns)} columnas numéricas\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def prepare_prediction_data(df, objects):\n",
    "    \"\"\"\n",
    "    Prepara datos para predicción usando los mismos pasos del entrenamiento\n",
    "    PREDICE PARA TODOS LOS PRODUCTOS (no filtra)\n",
    "    \"\"\"\n",
    "    print(\"Preparando datos para predicción...\")\n",
    "    \n",
    "    # NO filtrar - usar todos los productos para predicción\n",
    "    df_filtered = df.copy()\n",
    "    \n",
    "    print(f\"Datos para predicción (todos los productos): {df_filtered.shape}\")\n",
    "    \n",
    "    if df_filtered.empty:\n",
    "        print(\"¡ADVERTENCIA! No se encontraron datos para predicción.\")\n",
    "        return None\n",
    "    \n",
    "    # Aplicar limpieza de datos\n",
    "    df_filtered = clean_numeric_data_prediction(df_filtered)\n",
    "    \n",
    "    scaler = objects['scaler']\n",
    "    label_encoders = objects['label_encoders']\n",
    "    feature_cols = objects['feature_cols']\n",
    "    categorical_vars = objects['categorical_vars']\n",
    "    numeric_vars = objects['numeric_vars']\n",
    "    \n",
    "    # Codificar variables categóricas\n",
    "    for col in categorical_vars:\n",
    "        if col in df_filtered.columns and col in label_encoders:\n",
    "            le = label_encoders[col]\n",
    "            df_filtered[col] = df_filtered[col].fillna('UNKNOWN')\n",
    "            \n",
    "            # Manejar categorías no vistas durante el entrenamiento\n",
    "            unknown_mask = ~df_filtered[col].astype(str).isin(le.classes_)\n",
    "            df_filtered[col] = df_filtered[col].astype(str)\n",
    "            df_filtered.loc[unknown_mask, col] = 'UNKNOWN'\n",
    "            \n",
    "            # Si 'UNKNOWN' no está en las clases, usar la primera clase\n",
    "            if 'UNKNOWN' not in le.classes_:\n",
    "                df_filtered.loc[unknown_mask, col] = le.classes_[0]\n",
    "            \n",
    "            df_filtered[col] = le.transform(df_filtered[col])\n",
    "    \n",
    "    # Seleccionar features\n",
    "    missing_features = [col for col in feature_cols if col not in df_filtered.columns]\n",
    "    if missing_features:\n",
    "        print(f\"¡ADVERTENCIA! Features faltantes: {missing_features}\")\n",
    "        # Crear columnas faltantes con valor 0\n",
    "        for col in missing_features:\n",
    "            df_filtered[col] = 0\n",
    "    \n",
    "    X_pred = df_filtered[feature_cols].copy()\n",
    "    \n",
    "    # Manejar valores nulos\n",
    "    X_pred = X_pred.fillna(0)\n",
    "    \n",
    "    # Estandarizar variables numéricas usando los mismos pasos del entrenamiento\n",
    "    numeric_cols_present = [col for col in numeric_vars if col in X_pred.columns]\n",
    "    numeric_cols_valid = []\n",
    "    \n",
    "    for col in numeric_cols_present:\n",
    "        if X_pred[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "            if not np.isinf(X_pred[col]).any():\n",
    "                max_val = X_pred[col].abs().max()\n",
    "                if max_val < 1e10:\n",
    "                    numeric_cols_valid.append(col)\n",
    "    \n",
    "    if numeric_cols_valid:\n",
    "        X_pred[numeric_cols_valid] = scaler.transform(X_pred[numeric_cols_valid])\n",
    "    \n",
    "    # Conservar información para el resultado final\n",
    "    result_info = df_filtered[['customer_id', 'product_id']].copy()\n",
    "    \n",
    "    return X_pred, result_info\n",
    "\n",
    "def make_predictions_for_products():\n",
    "    \"\"\"\n",
    "    Función principal para hacer predicciones para productos específicos\n",
    "    \"\"\"\n",
    "    print(\"=== PREDICCIÓN PARA PRODUCTOS ESPECÍFICOS ===\\n\")\n",
    "    \n",
    "    # 1. Cargar productos a predecir\n",
    "    productos_a_predecir = load_productos_a_predecir()\n",
    "    \n",
    "    # 2. Cargar modelo y objetos\n",
    "    model, objects = load_model_and_objects()\n",
    "    \n",
    "    # 3. Cargar datos más recientes\n",
    "    df_latest = load_latest_data_for_prediction()\n",
    "    \n",
    "    # 4. Preparar datos para predicción (TODOS los productos)\n",
    "    prediction_result = prepare_prediction_data(df_latest, objects)\n",
    "    \n",
    "    if prediction_result is None:\n",
    "        print(\"No se pudieron preparar los datos para predicción.\")\n",
    "        return None\n",
    "    \n",
    "    X_pred, result_info = prediction_result\n",
    "    \n",
    "    # 5. Hacer predicciones PARA TODOS LOS PRODUCTOS\n",
    "    print(\"Generando predicciones mejoradas para todos los productos...\")\n",
    "    predictions_raw = model.predict(X_pred)\n",
    "    predictions = apply_post_prediction_constraints(predictions_raw)  # ← NUEVO\n",
    "    \n",
    "    # 6. Agregar predicciones al resultado\n",
    "    result_info['predicted_tn'] = predictions\n",
    "    \n",
    "    # 7. Agrupar por product_id y sumar las predicciones (suma por todos los customer_id)\n",
    "    print(\"Agrupando predicciones por product_id...\")\n",
    "    all_predictions = result_info.groupby('product_id').agg({\n",
    "        'predicted_tn': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Renombrar columna para claridad\n",
    "    all_predictions = all_predictions.rename(columns={'predicted_tn': 'tn'})\n",
    "    \n",
    "    print(f\"Predicciones generadas para {len(all_predictions)} productos en total\")\n",
    "    \n",
    "    # 8. FILTRAR RESULTADOS: Solo enviar productos que están en tb_productos_a_predecir\n",
    "    product_ids_to_send = productos_a_predecir['product_id'].unique()\n",
    "    final_predictions = all_predictions[\n",
    "        all_predictions['product_id'].isin(product_ids_to_send)\n",
    "    ].copy()\n",
    "    \n",
    "    # Ordenar por tn descendente\n",
    "    final_predictions = final_predictions.sort_values('tn', ascending=False)\n",
    "    \n",
    "    print(f\"\\nResultados finales (filtrados):\")\n",
    "    print(f\"Productos en tb_productos_a_predecir: {len(product_ids_to_send)}\")\n",
    "    print(f\"Productos con predicción disponible: {len(final_predictions)}\")\n",
    "    print(f\"Total tn predichas (solo productos solicitados): {final_predictions['tn'].sum():.2f}\")\n",
    "    print(f\"Promedio tn por producto: {final_predictions['tn'].mean():.2f}\")\n",
    "    \n",
    "    # Mostrar top 10\n",
    "    print(\"\\nTop 10 productos solicitados con mayor predicción:\")\n",
    "    print(final_predictions.head(10))\n",
    "    \n",
    "    # 9. Verificar que tenemos todos los productos solicitados\n",
    "    productos_solicitados = set(product_ids_to_send)\n",
    "    productos_predichos = set(final_predictions['product_id'].unique())\n",
    "    productos_faltantes = productos_solicitados - productos_predichos\n",
    "    \n",
    "    if productos_faltantes:\n",
    "        print(f\"\\n¡ADVERTENCIA! Productos solicitados sin datos en 201912: {len(productos_faltantes)}\")\n",
    "        print(f\"Product_ids faltantes: {list(productos_faltantes)[:10]}...\")  # Mostrar solo los primeros 10\n",
    "        \n",
    "        # Agregar productos faltantes con tn = 0\n",
    "        for product_id in productos_faltantes:\n",
    "            final_predictions = pd.concat([\n",
    "                final_predictions,\n",
    "                pd.DataFrame({'product_id': [product_id], 'tn': [0.0]})\n",
    "            ], ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nPredicciones finales enviadas: {len(final_predictions)} productos\")\n",
    "    \n",
    "    # 10. Guardar resultados\n",
    "    output_file = '../output/lgbm/02_lgbm_model_v2.csv'\n",
    "    final_predictions.to_csv(output_file, index=False)\n",
    "    print(f\"Predicciones guardadas en: {output_file}\")\n",
    "    \n",
    "    # Opcional: Guardar también TODAS las predicciones para análisis\n",
    "    #all_output_file = 'predicciones_todos_los_productos.csv'\n",
    "    #all_predictions.sort_values('tn', ascending=False).to_csv(all_output_file, index=False)\n",
    "    #print(f\"TODAS las predicciones guardadas en: {all_output_file}\")\n",
    "    \n",
    "    return final_predictions\n",
    "\n",
    "# Ejecutar predicciones\n",
    "if __name__ == \"__main__\":\n",
    "    predicciones = make_predictions_for_products()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311 (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
